[ { "title": "Unity Shadows(翻译七)", "url": "/posts/Unity-Shadows/", "categories": "翻译, Shader", "tags": "Unity3D, Shader", "date": "2018-01-05 20:00:00 +0800", "snippet": "本篇摘要: 探索Unity中的阴影渲染 投射一个方向光阴影 接收一个方向光阴影 支持对聚光源和点光源阴影方向光阴影-Direction前面写的光照shader产生了相当真实的效果，可它假设着来自每个光源的光线最终都会击中它的片元，但是这只有在那些光线没有被遮挡才成立。 方向光投射阴影的草图. 当一个物体位于光源和另一个物体之间时，它可能会阻止部分或全部光线到达另一个物体。这些光线照亮了第一个物体就不再可能照亮第二个物体。因此，第二个物体有一部不发光，而不发光的区域位于第一个物体的阴影下。我们通常是这样描述：第一个物体投射了一个阴影到第二个物体。实际上，在全光照和全阴影的存在过渡区，被称为半阴影。这是因为所有光源都有一个体积，因此，这些区域只有部分光源是可见的，意味着它是部分阴影。光源远大，表面距离阴影投射器越远，半影区域也就越大。但是Unity不支持半影，只支持软阴影soft shadow，但它是阴影过滤算法。 半阴影或是soft shadow. 启用阴影-Enable Shadow先关闭环境光，这样会更容易看见阴影。 没有投射阴影. 没有阴影，物体间的空间视觉感不太强。在QualitySetting可以打开或关闭阴影。 阴影参数. 同时确保光源开启投射阴影，分辨率依赖于上面的quality设置 阴影类型. 阴影投射. 阴影贴图-Shadow mapUnity是如何把阴影添加到屏幕？上面所有物体使用的standard着色器，有一些方法确定光线是否被阻挡。要搞清楚一个点是否在阴影中，可以通过在场景中从光线到表面片元投射光线，如果光线在到达表面之前击中某些东西，说明它就被阻挡了。这些事是物理引擎做的，但是要计算每个片元与每个光是不实际的，而且还要把结果传递给GPU。现在有许多支持实时阴影的技术，它们各有优劣。而Unity采用了最常用的技术：Shadow Mapping。这意味着unity把阴影数据存储至纹理中。现在来看看它是如何工作的。打开frame Debugger，Window/Frame Debugger。点击Enable，按顺序查看面板信息。注意看看每帧在gameScene视图中的不同，以及阴影的开启。 frame debugger调试. 当启用阴影绘制时，这个绘制过程变得非常复杂：有更多的渲染阶段，和更多的draw call。阴影绘制非常昂贵！渲染深度纹理-Rendering to the Depth Texture当方向阴影激活后，Unity在渲染过程开启一个depth pass通道计算。结果存储在与屏幕分辨率相匹配的纹理，这个pass通道会渲染整个屏幕，但是只收集每个片元的深度信息。这些信息与GPU用于确定一个片段渲染结束时在先前渲染的片段之上(前)还是之下(后)的信息相同。这个数据对应在裁剪空间(clip space)坐标的z分量值。而裁剪空间是定义摄像机能看见的区域，深度信息最终存储为0-1范围内的值。在debugger查看该纹理时，近裁切面附近的纹理显示趋近为(白)浅色，远裁切面附近的纹素texel，颜色趋近黑(暗)色。 depth texture, 摄像机近裁切面为5. 与屏幕分辨率一致. 这些信息实际上与阴影没有太多直接关系，但Unity在后面的pass通道使用了它。渲染阴影贴图-Rendering to Shadow Maps这步主要工作：先渲染第一个光源的阴影贴图，然后就会渲染第二个光源的阴影贴图。再一次渲染整个屏幕，并再次把深度信息存储在纹理中。但是，这此的屏幕渲染是从光源位置角度进行的，实际上是把光源作为摄像机。这意味着用深度值告诉了我们光线击中物体之前走了多远距离，这可以用来确定什么东西被遮蔽了! 阴影贴图记录了实际的几何图形的深度信息。而法线贴图是为了添加粗糙表面的一种错觉，阴影贴图会忽略它们。因此，阴影不受法线贴图的影响。由于我们使用方向光，这些光模拟的摄像机是正交投影，没有透视投影。因此它们模拟的相机的位置精确性就不那么重要。Unity将定位常规相机使其能够看见视野内所有物体。 左第一个光源，右第二个光源. 事实上，原来Unity渲染整个场景不是每个光只渲染一次，而是每个光要渲染四次！ 这个阴影纹理被分成四个象限，每个象限从不同的角度呈现。这是因为我们选择使用Four Cascades(QualitySetting)。如果我们设置为Two Cascades，就是每个光渲染两次；如果设置没有，只会渲染一次。我们接下来要探查阴影质量与该项设置的关系。Unity为什么渲染这么多次。收集阴影-Collecting Shadows我们已经从摄像机的角度得到场景的深度信息，也有了从每个光模拟的相机视角得到的深度信息，这些数据存在不同的裁剪空间。但是我们知道这些空间的相对位置和方向，因此能从一个空间转换到另一个空间。这允许我们从两个视角比较深度测量。理论上讲，我们有两个向量应该在同一点交会结束，这样相机和光源都能看见该点，说明它被点亮了。如果光的向量在到达该点之前结束，则光被挡住，这意味着该点被阴影化。 当摄像机看不到一个点时？看不到的这些点被隐藏在距离相机更近的其他点后面。场景深度纹理仅包含最接近的点。 因此没有时间浪费在评估隐藏点。 每个光的屏幕空间阴影. Unity通过渲染一个单独的覆盖整个视野的面片来创建这些纹理，它使用了Hidden/Internal-ScreenSpaceShadows shader的通道，每个片元从场景和光源的深度纹理采样，进行比较，渲染最终阴影值到屏幕空间的阴影纹理。亮的纹素值设为1，阴影纹素值设为0。此时Unity能执行过滤，创建柔和的阴影。 shader 通道0. 为什么Unity在渲染和收集间交替？ 每个光需要它自己的屏幕空间阴影贴图，然而从光源位置视野渲染的阴影贴图能被重复使用。采样阴影贴图-Sampling the Shadow Maps最后，Unity完成了阴影渲染。现在屏幕是常规渲染，只有一个更改：光照颜色与它的阴影贴图的值相乘。这就消除了被遮挡的光线。渲染的每个片元都要采样阴影贴图，每个最终隐藏在其他对象之后的片元会最后绘制。因此这些片元最后能接收到最终能遮挡它们的对象的阴影。当在frame debugger步进调试观察时，您还可以看到阴影在实际投射它的对象之前出现。当然这些错误只在渲染帧时很明显，一旦完成渲染就是正确的了 部分渲染帧. 阴影质量-Shadow Quality虽然场景是从光源的方向进行渲染，但是该方向与场景内摄像机视野方向不匹配。因此阴影贴图的纹素与最终呈现图像的纹素是没有对齐的，会出现锯齿。阴影贴图的分辨率也会不同，最终图像的分辨率是由显示设置决定的，而阴影贴图的分辨率由阴影质量设置决定。当阴影贴图的纹素最后渲染的比最终图像大时，将很明显：阴影的边缘出现叠加，在使用硬阴影时非常明显。 硬阴影 vs 软阴影. 在质量设置面板修改使用hard shadow、lowest resolution、no cascades。就会看见满屏的锯齿。 低质量阴影. “阴影是一张纹理” 现在就非常明显了。但是上图有些阴影出现在了不该出现的地方。距离摄像机越近的阴影，它们的纹素变得越大。这是由于阴影贴图当前覆盖了场景相机的整个可视区域。在QualitySetting面板通过降低阴影覆盖的区域，来提升靠近相机区域的阴影质量。 Shadow Distance降至25，其他参数一致. 通过限制靠近屏幕相机的阴影区域，我们能使用相同的阴影纹理去覆盖更多小区域。结果是能得到更好的阴影。但是会丢失更远区域的阴影细节，因为当阴影接近最大距离时会逐渐消失。理想情况是，既要获得近距离高质量阴影，同时也要保留远处的阴影。因为远处的阴影最后渲染在较小的屏幕区域，就可以用作低分辨率阴影纹理。这就是Shadow Cascades的工作。当启用该选项，多个阴影贴图渲染进同一张纹理，每张贴图对应某些距离来使用。 fourCascades,100Distance,hardShodw,LowResolution. 当使用FourCascades，上图结果看起来比之前的要好，尽管我们使用了同一张纹理分辨率，我们更有效的使用了纹理。不过缺点就是我们现在至少要渲染场景3次以上。当渲染屏幕空间阴影纹理时，Unity关注从正确Cascade采样，如下图CascadeSplits：一个cascade结束是下一个的开始。 Cascade Splits. 可以控制cascade的范围，作为阴影距离的一部分。也能通过改变_Shading Mode/Miscellaneous/Shadow Cascades_观察scene视图的变化。 Cascade范围：StableFit. 上图显示的cascade形状(覆盖区域)是可以通过_Shadow Projection_调整，默认是_Stable Fit_：这个模式cascade条带选择的区域基于距离摄像机位置的远近。其他模式是_Close Fit_：使用相机的深度信息替代，在相机可视方向产生一条规则的条带。 Close Fit. Close Fit模式可以更高效的利用阴影纹理，绘制更高质量的阴影。然而，该阴影投射模式(ShadowProjection)取决于阴影产生后位置和方向以及相机参数。结果是，当移动或旋转相机，阴影贴图也会跟着移动。这就是著名的阴影抖动。所以Stale Fit是引擎默认的选项。 Close Fit: swimming. Stable Fit模式下，在相机位置改变时Unity能够对齐纹理，纹素看起来好像没动。实际上cascade移动了，只是在cascade相互过渡时阴影会发生改变。如果没有注意到cascade改变，就不容易察觉到。 Stable Fit: edge transition. 阴影“痤疮”(0!什么鬼)当我们使用低质量的硬阴影时，我们看见一些阴影出现在不正确的地方。不幸的是，不管如何设置_Quality Setting_都会发生。 Shadow Acne 阴影贴图中每个纹素表示光线击中表面的点。然而，这些纹素不是单独点。它们最后要覆盖很大的区域并且与光的方向对齐，而不是与表面一致。结果时，它们会像黑色瓦片最终黏在、穿过、伸出表面；当阴影纹理的一部分从投射出阴影的表面伸出时，表面看起来也会产生阴影。 凸起. 阴影凸起的另一个来源是数字精度的限制，当使用非常小的距离时这些限制会导致不正确的结果。默认是0.05. light组件中设置没有biases. 避免该问题的一个方法是：当渲染阴影纹理时增加深度偏移。这个_偏差系数_目的是增加‘光投射到表面距离’，把阴影‘推进’表面内。 Biases系数控制粉刺. 较低的Bias系数会产生粉刺，而较高的偏差系数就会有另一个问题：当投射阴影的对象逐渐远离光源时，阴影也会逐渐飘离原对象。使用较小的值问题还可接受，但太大的值会导致物体与该物体的阴影不再相连接了，好像飞起来了。 太大的Bias导致阴影飘移. 除了距离bias偏差，还有法线偏差。该系数辅助调整阴影投射：沿着法线，将投射的阴影顶点向内‘推’。该值也会改善“阴影粉刺”，但是越大的值越会使阴影变得更小并且有可能使阴影中间出现洞。best bias settings？没有最优的默认值，必须不停的实验调整 。抗锯齿Anti-Aliasing:图形边缘锯齿缓和。在Unity开启了4倍抗锯齿，感觉并没有达到想要的抗锯齿效果。Unity采用的多重采样抗锯齿方案：MSAA，通过沿三角形边缘执行超级采样以消除边缘锯齿，更重要的是Unity渲染屏幕空间阴影时，它使用了一个单独四方面片覆盖整个可视区域。结果是，这就没有了三角形边缘，因此MSAA对屏幕空间阴影纹理采样就没有效果了。MSAA对最终图像有效，但阴影值是取之屏幕空间阴影纹理，当亮表面紧挨着暗表面被阴影覆盖时就非常明显。明暗之间的边缘是反锯齿的，而阴影边缘则不是。 no AA. 4倍MSAA. 当然也有FXAA，是屏幕后处理抗锯齿，效果挺好！投射阴影通过上面我们知道了Unity如何创建方向光阴影，是时候写自己的Shader来支持阴影了。当前光照shader既不支持投射阴影也不支持接收阴影。首先来处理投射阴影：我们知道对于方向光阴影Unity会渲染多次屏幕。对每个阴影纹理一次是深度pass渲染，一次是每个光源渲染。而屏幕空间阴影纹理是屏幕效果暂时与我们无关。阴影渲染Pass标签是_ShadowCaster。_因为我们只对深度值感兴趣，它与别的Pass相比应该会简单。增加一个passPass{ Tags{\"LightMode\" = \"ShadowCaster\"} CGPROGRAM #pragma target 3.0 #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #include \"MyShadow.cginc\" ENDCG}创建一个_MyShadow.cginc_文件#if !defined(MY_SHADOW_INCLUDE)#define MY_SHADOW_INCLUDE#include “UnityCG.cginc”struct InputData { float4 position : POSITION;};float4 MyVertexProgram(InputData i) : SV_POSITION{ return UnityObjectToClipPos(i.position);}half4 MyFragmentProgram() : SV_TARGET{ return 0;}#endif上面写完就嫩产生方向光阴影了。下面开始用代码调优阴影质量。偏差-Bias我们要支持阴影的偏移。在渲染深度Pass时该值是0，但当渲染阴影纹理时，偏差值取光照组件设置。我们要做的就是：在顶点函数中在裁切空间下，对顶点坐标应用深度偏差。UnityCG函数_UnityApplyLinerShadowBias：_float4 MyVertexProgram(InputData i) : SV_POSITION{ float4 position = UnityObjectToClipPos(i.position); return\tUnityApplyLinearShadowBias(position);}在裁剪空间增加Z分量，复杂的是在其次坐标空间下，必须补偿透视投影，这样偏移不会随着与相机距离改变而改变，也必须确保结果不会越界。float4 UnityApplyLinearShadowBias(float4 clipPos){#if defined(UNITY_REVERSED_Z) // We use max/min instead of clamp to ensure proper handling of the rare case // where both numerator and denominator are zero and the fraction becomes NaN. clipPos.z += max(-1, min(unity_LightShadowBias.x / clipPos.w, 0)); float clamped = min(clipPos.z, clipPos.w*UNITY_NEAR_CLIP_VALUE);#else clipPos.z += saturate(unity_LightShadowBias.x / clipPos.w); float clamped = max(clipPos.z, clipPos.w*UNITY_NEAR_CLIP_VALUE);#endif clipPos.z = lerp(clipPos.z, clamped, unity_LightShadowBias.y); return clipPos;}同时支持Normal Bias，必须根据法向量移动顶点坐标。因此，添加一个normal变量。然后可以使用UnityCG定义的UnityClipSpaceShadowCasterPos函数float4 MyVertexProgram(InputData i) : SV_POSITION{ //float4 position = UnityObjectToClipPos(i.position); float4 position = UnityClipSpaceShadowCasterPos(i.position, i.normal); return UnityApplyLinearShadowBias(position);}先将顶点坐标转换到世界空间，然后转换到裁剪空间。计算光的方向，计算法线和光的角度，取正弦值，最后转与观察投影矩阵相乘转到裁剪空间。float4 UnityClipSpaceShadowCasterPos(float4 vertex, float3 normal){ float4 wPos = mul(unity_ObjectToWorld, vertex); if (unity_LightShadowBias.z != 0.0) { float3 wNormal = UnityObjectToWorldNormal(normal); float3 wLight = normalize(UnityWorldSpaceLightDir(wPos.xyz)); // apply normal offset bias (inset position along the normal) // bias needs to be scaled by sine between normal and light direction // (http://the-witness.net/news/2013/09/shadow-mapping-summary-part-1/) // // unity_LightShadowBias.z contains user-specified normal offset amount // scaled by world space texel size. float shadowCos = dot(wNormal, wLight); float shadowSine = sqrt(1-shadowCos*shadowCos); float normalBias = unity_LightShadowBias.z * shadowSine; wPos.xyz -= wNormal * normalBias; } return mul(UNITY_MATRIX_VP, wPos);}写完就具备了完全的阴影投射接收阴影First,我们先关注主方向光的阴影，因为该光源属于BasePass，必须要先适配。当主方向光投射阴影，Unity会找一个启用了SHADOWS_SCREEN关键字的shader变体。所以我们要在Base Pass创建两个变体，同之前使用顶点光关键字类似：一个无，一个是该关键字。#pragma multi_compile _ VERTEXLIGHT_ON#pragma multi_compile _ SHADOWS_SCREEN该basePass有两个multi_compile指令，每个都是单关键字。因此编译后这里会有4个变体：// Total snippets: 3// -----------------------------------------// Snippet #0 platforms ffffffff:SHADOWS_SCREEN VERTEXLIGHT_ON4 keyword variants used in scene:&lt;no keywords defined&gt;SHADOWS_SCREENVERTEXLIGHT_ONSHADOWS_SCREEN VERTEXLIGHT_ON(老版本Unity有可能出现)当增加了multi_compile指令后，shader编译器会提示关于__ShadowCoord_不存在。这是因为_UNITY_LIGHT_ATTENUATION_宏在使用阴影时的行为不同导致。在MyLighting_shadow.cginc顶点函数快速修复#if defined(UNITY_SCREEN) float attenuation = 1;#else UNITY_LIGHT_ATTENUATION(attenuation, 0, i.worldPos);#endif采样阴影Secend,采样屏幕空间阴影纹理。Third,需要获取屏幕空间纹理坐标，从顶点函数传递给片元函数。在插值器Interpolator添加一个float4 变量以支持传递阴影纹理坐标。从裁剪空间开始(裁剪空间顶点坐标)。struct Interpolator{ #if defined(SHADOWS_SCREEN) float4 shadowCoordinate : TEXCOORD6; #endif}Interpolators MyVertexProgram(VertexData v) { //。。。 #if defined(SHADOWS_SCREE) i.shadowCoordinate = i.position; #endif //。。。} 错误的纹理坐标映射. AutoLignt.cginc定义了Sampler2D _ShadowMapTexture，可以通过它访问屏幕阴影纹理。但是要覆盖整个屏幕，就需要屏幕空间坐标。在裁剪空间，XY坐标范围是[-1, 1]，而屏幕空间下是[0,1]；然后偏移坐标与屏幕左小脚等于0对齐。因为我们处理的使透视变换，偏移坐标值取决于距离，这里的偏移值等于加上齐次坐标的w分量之后的一半。#if defined(SHADOWS_SCREEN) i.shadowCoordinate.xy = (i.position.xy + i.position.w) * 0.5; i.shadowCoordinate.zw = i.position.zw;#endif 错误的左下角映射. 上图的投影错误，还需要通过x和y除以齐次坐标进一步转换 错误投影. 上图结果仍然是错误的，影子被拉伸了。这是由于在顶点函数计算导致，不应该在传递给片元函数时提前修改原始数据，需要保持它们的独立性。在片元函数再次除以w. 颠倒的投影. 此时，影子是上下颠倒的。如果它们被翻转，这意味着你的图形Direct3D屏幕空间Y坐标从0向下到1，而不是向上。要与此同步，翻转顶点的Y坐标。#if defined(SHADOWS_SCREEN) i.shadowCoordinate.xy = (float2(i.pos.x, -i.pos.y) + i.pos.w);// (i.pos.xy + i.pos.w) * 0.5; i.shadowCoordinate.zw = i.pos.zw;#endif 继续错误. 内置函数使用SHADOW_COORDS宏定义纹理坐标TRANSFRE_SHADOW宏获取阴影纹理坐标(转换)#define TRANSFER_SHADOW(a) a._ShadowCoord = ComputeScreenPos(a.pos);SHADOW_ATTENUATION宏阴影纹理明暗衰减#define SHADOW_COORDS(idx1) unityShadowCoord4 _ShadowCoord : TEXCOORD##idx1; #define SHADOW_ATTENUATION(a) unitySampleShadow(a._ShadowCoord)UNITY_LIGHT_ATTENUATION宏包含了SHADOW_ATTENUATION宏使用，可替换之当启用SHADOWS_SCREEN指令时，会自动计算，不启用不计算，没有任何损失。struct Interpolators { //... // #if defined(SHADOWS_SCREEN) // float4 shadowCoordinates : TEXCOORD5; // #endif SHADOW_COORDS(5) //...};Interpolators MyVertexProgram (VertexData v) { //... // #if defined(SHADOWS_SCREEN) // i.shadowCoordinates = i.position; // #endif TRANSFER_SHADOW(i); //...}UnityLight CreateLight (Interpolators i) { //... #if defined(SHADOWS_SCREEN) float attenuation = SHADOW_ATTENUATION(i); #else UNITY_LIGHT_ATTENUATION(attenuation, 0, i.worldPos); #endif UNITY_LIGHT_ATTENUATION(attenuation, i, i.worldPos); //...} 正确了. ComputeScreenPos函数inline float4 ComputeNonStereoScreenPos(float4 pos) { float4 o = pos * 0.5f; o.xy = float2(o.x, o.y*_ProjectionParams.x) + o.w; o.zw = pos.zw; return o;}inline float4 ComputeScreenPos(float4 pos) { float4 o = ComputeNonStereoScreenPos(pos);#if defined(UNITY_SINGLE_PASS_STEREO) o.xy = TransformStereoScreenSpaceTex(o.xy, pos.w);#endif return o;}聚光灯阴影关闭方向光，增加聚光灯后，竟然直接有阴影了。这是Unity宏带来的便利。 点光源阴影. 再看帧调试器 SpotLight Debugger. 上图对于聚光灯源阴影的渲染工作量很少不同之处： 没有方向光独立的深度pass和屏幕空间阴影pass，而是直接渲染阴影纹理； 与方向光渲染阴影还有很大的差别之处：聚光灯光线不是平行的，因此用光的位置模拟相机视角会得到一个透视视角，结果就是不支持阴影分段渲染(cascades)； normal bias(法线偏差)只支持方向光阴影，对于其他光源类型简单的置为0； 采样代码不同。相同之处： 投射阴影的这段代码通用。采样阴影纹理由于聚光灯不使用屏幕空间的阴影，这段采样纹理代码就有点不一样。因此，如果我们想要使用软阴影，我们必须在fragment程序中进行过滤。而Unity宏已经做了过滤计算UnitySampleShadowmap。//阴影坐标把顶点坐标从模型空间转到世界空间再转到光的阴影空间得到。// ---- Spot light shadows#if defined (SHADOWS_DEPTH) &amp;&amp; defined (SPOT)#define SHADOW_COORDS(idx1) unityShadowCoord4 _ShadowCoord : TEXCOORD##idx1;#define TRANSFER_SHADOW(a) a._ShadowCoord = mul(unity_WorldToShadow[0], mul(unity_ObjectToWorld,v.vertex));#define SHADOW_ATTENUATION(a) UnitySampleShadowmap(a._ShadowCoord)#endif然后SHADOW_ATTENUATION宏使用UnitySampleShadowmap函数采样阴影映射。这个函数定义在_UnityShadowLibrary_，_AutoLight_文件引用了它。当使用硬阴影时，该函数对阴影纹理采样一次。当使用软阴影时，它对纹理采样四次并对结果取平均值。这个结果没有用于屏幕空间阴影的过滤效果好，但是速度快得多。 hard vs. soft SpotLight Shadow. // Spot light shadowsinline fixed UnitySampleShadowmap (float4 shadowCoord){ // DX11 feature level 9.x shader compiler (d3dcompiler_47 at least) // has a bug where trying to do more than one shadowmap sample fails compilation // with \"inconsistent sampler usage\". Until that is fixed, just never compile // multi-tap shadow variant on d3d11_9x. #if defined (SHADOWS_SOFT) &amp;&amp; !defined (SHADER_API_D3D11_9X) // 4-tap shadows #if defined (SHADOWS_NATIVE) #if defined (SHADER_API_D3D9) // HLSL for D3D9, when modifying the shadow UV coordinate, really wants to do // some funky swizzles, assuming that Z coordinate is unused in texture sampling. // So force it to do projective texture reads here, with .w being one. float4 coord = shadowCoord / shadowCoord.w; half4 shadows; shadows.x = UNITY_SAMPLE_SHADOW_PROJ(_ShadowMapTexture, coord + _ShadowOffsets[0]); shadows.y = UNITY_SAMPLE_SHADOW_PROJ(_ShadowMapTexture, coord + _ShadowOffsets[1]); shadows.z = UNITY_SAMPLE_SHADOW_PROJ(_ShadowMapTexture, coord + _ShadowOffsets[2]); shadows.w = UNITY_SAMPLE_SHADOW_PROJ(_ShadowMapTexture, coord + _ShadowOffsets[3]); shadows = _LightShadowData.rrrr + shadows * (1-_LightShadowData.rrrr); #else // On other platforms, no need to do projective texture reads. float3 coord = shadowCoord.xyz / shadowCoord.w; half4 shadows; shadows.x = UNITY_SAMPLE_SHADOW(_ShadowMapTexture, coord + _ShadowOffsets[0]); shadows.y = UNITY_SAMPLE_SHADOW(_ShadowMapTexture, coord + _ShadowOffsets[1]); shadows.z = UNITY_SAMPLE_SHADOW(_ShadowMapTexture, coord + _ShadowOffsets[2]); shadows.w = UNITY_SAMPLE_SHADOW(_ShadowMapTexture, coord + _ShadowOffsets[3]); shadows = _LightShadowData.rrrr + shadows * (1-_LightShadowData.rrrr); #endif #else float3 coord = shadowCoord.xyz / shadowCoord.w; float4 shadowVals; shadowVals.x = SAMPLE_DEPTH_TEXTURE (_ShadowMapTexture, coord + _ShadowOffsets[0].xy); shadowVals.y = SAMPLE_DEPTH_TEXTURE (_ShadowMapTexture, coord + _ShadowOffsets[1].xy); shadowVals.z = SAMPLE_DEPTH_TEXTURE (_ShadowMapTexture, coord + _ShadowOffsets[2].xy); shadowVals.w = SAMPLE_DEPTH_TEXTURE (_ShadowMapTexture, coord + _ShadowOffsets[3].xy); half4 shadows = (shadowVals &lt; coord.zzzz) ? _LightShadowData.rrrr : 1.0f; #endif // average-4 PCF half shadow = dot (shadows, 0.25f); #else // 1-tap shadows #if defined (SHADOWS_NATIVE) half shadow = UNITY_SAMPLE_SHADOW_PROJ(_ShadowMapTexture, shadowCoord); shadow = _LightShadowData.r + shadow * (1-_LightShadowData.r); #else half shadow = SAMPLE_DEPTH_TEXTURE_PROJ(_ShadowMapTexture, UNITY_PROJ_COORD(shadowCoord)) &lt; (shadowCoord.z / shadowCoord.w) ? _LightShadowData.r : 1.0; #endif #endif return shadow;}点光源阴影如果直接使用点光源，会有编译报错：undeclared identifier ‘UnityDecodeCubeShadowDepth’。该函数在_UnityCG.cginc_文件。 UnityPBSLighting文件引用；AutoLight文件引用. 所以根据引用结构，需要把UnityPBSLighing文件放在第一位引用。就不会报错了。 左：render six times per light. 投射阴影从帧调试器查看，左边一个光要渲染6次，两盏光就是12次了。有很多个RenderJobPoint渲染了。结果是，点光源的阴影纹理是一个立方体贴图，而立方体贴图是通过相机在6个不同方向观察场景，每个方向渲染一面组成六面体，前面1.4讲过把光源模拟相机对屏幕渲染。所以点光源阴影计算很费，尤其是实时点光源阴影。 错误的阴影纹理. 当渲染点光源阴影纹理时，Unity引擎会找shader变体关键字SHADOWS_CUBE，而SHADOWS_DEPTH关键字只适用于方向光和聚光灯。为了支持点光源阴影，Unity提供了一个特殊编译指令#pragma multi_compile_shadowcaster// -----------------------------------------// Snippet #2 platforms ffffffff:SHADOWS_CUBE SHADOWS_DEPTH2 keyword variants used in scene:SHADOWS_DEPTHSHADOWS_CUBE所以，需要创建一个独立的处理程序。这里首先要计算光到表面的距离，但得知道光到表面的方向。在顶点函数先转换顶点坐标所在世界空间，再计算光的方向。然后在片元函数计算该方向向量长度再与bias偏差相加。然后再除以点光源的范围映射到[0.1]再与长度相乘，最后解码。而_LightPositionRange.w = 1/range已经计算好了隐射范围，直接用。#if defined(SHADOWS_CUBE) struct Interplotars { float4 position : SV_POSITION; float3 lightVec : TEXCOORD0; }; Interplotars MyVertexProgram(InputData v){ Interplotars i; i.position = UnityObjectToClipPos(v.position); i.lightVec = mul(unity_ObjectToWorld, v.position).xyz - _LightPositionRange.xyz; //float4 position = UnityClipSpaceShadowCasterPos(i.position, i.normal);//方向光源：简单的裁剪空间顶点坐标 return\ti; } half4 MyFragmentProgram(Interplotars i) : SV_TARGET{ float depth = length(i.lightVec) + unity_LightShadowBias.x; depth *= _LightPositionRange.w; return UnityEncodeCubeShadowDepth(depth); }#else float4 MyVertexProgram(InputData i) : SV_POSITION{ //float4 position = UnityObjectToClipPos(i.position); float4 position = UnityClipSpaceShadowCasterPos(i.position, i.normal); return\tUnityApplyLinearShadowBias(position); } half4 MyFragmentProgram() : SV_TARGET{ return 0; }#endif 正确的阴影纹理. UnityEncodeCubeShadowDepth函数：// Shadow caster pass helpersfloat4 UnityEncodeCubeShadowDepth (float z){ #ifdef UNITY_USE_RGBA_FOR_POINT_SHADOWS return EncodeFloatRGBA (min(z, 0.999)); #else return z; #endif}// 使用浮点类型cube——map,存储再8位RGBA纹理inline float4 EncodeFloatRGBA( float v ){ float4 kEncodeMul = float4(1.0, 255.0, 65025.0, 16581375.0); float kEncodeBit = 1.0/255.0; float4 enc = kEncodeMul * v; enc = frac (enc);//返回小数部分 enc -= enc.yzww * kEncodeBit; return enc;}采样阴影纹理在additional pass的编译指令，Unity宏已经做了。//同样计算光的方向，然后采样cubeMap。区别是float3类型而不是float4，不需要齐次坐标。// ---- Point light shadows#if defined (SHADOWS_CUBE)#define SHADOW_COORDS(idx1) unityShadowCoord3 _ShadowCoord : TEXCOORD##idx1;#define TRANSFER_SHADOW(a) a._ShadowCoord = mul(unity_ObjectToWorld, v.vertex).xyz - _LightPositionRange.xyz;#define SHADOW_ATTENUATION(a) UnitySampleShadowmap(a._ShadowCoord)#endif// ------------------------------------------------------------------// Point light shadows//在这种情况下，UnitySampleShadowmap采样一个立方体地图，而不是2D纹理。#if defined (SHADOWS_CUBE)samplerCUBE_float _ShadowMapTexture;inline float SampleCubeDistance (float3 vec){ #ifdef UNITY_FAST_COHERENT_DYNAMIC_BRANCHING return UnityDecodeCubeShadowDepth(texCUBElod(_ShadowMapTexture, float4(vec, 0))); #else return UnityDecodeCubeShadowDepth(texCUBE(_ShadowMapTexture, vec)); #endif}inline half UnitySampleShadowmap (float3 vec){ float mydist = length(vec) * _LightPositionRange.w; mydist *= 0.97; // bias #if defined (SHADOWS_SOFT) float z = 1.0/128.0; float4 shadowVals; shadowVals.x = SampleCubeDistance (vec+float3( z, z, z)); shadowVals.y = SampleCubeDistance (vec+float3(-z,-z, z)); shadowVals.z = SampleCubeDistance (vec+float3(-z, z,-z)); shadowVals.w = SampleCubeDistance (vec+float3( z,-z,-z)); half4 shadows = (shadowVals &lt; mydist.xxxx) ? _LightShadowData.rrrr : 1.0f; return dot(shadows,0.25); #else float dist = SampleCubeDistance (vec); return dist &lt; mydist ? _LightShadowData.r : 1.0; #endif}#endif // #if defined (SHADOWS_CUBE)同样，如果使用软阴影会采样四次并取平均值，硬阴影采样一次。同时没有进行过滤计算，计算昂贵且效果很粗糙！ hard vs soft pointLight Shadows. 对于点光源阴影实在不能用于手机平台，替代方式就是用无阴影点光+cookie投射，模拟阴影。或者用较少的聚光灯阴影代替。" }, { "title": "Unity纹理高级用法(翻译六)", "url": "/posts/Unity-Advance-Texture/", "categories": "翻译, Shader", "tags": "Unity3D, Shader, Texture", "date": "2018-01-04 20:00:00 +0800", "snippet": "本篇摘要: 扰动法线以模拟凹凸视觉 从高度场计算法线 采样和混合法线贴图 从切线空间转换到世界空间Bump Map(凹凸贴图): Bump Map Type Describe NormalMap 法线图，映射公式：normal=pixel*2-1,反映射：pixel=(normal+1)/2. 法线存储既可以在模型空间，也可以在切线空间。//unity顶点输入结构带切线变量，一般存在切线空间更佳 HeightMap 灰度图(黑白纹理-强度值)，颜色越浅该表面越向外凸起，颜色深越凹。视差映射技术，与Occlusion Map搭配使用体验更佳，计算昂贵 Occlusion Map 灰度图，表面细节更丰富。颜色值白色表示应该接收完全间接照明的区域，黑色表示没有间接照明。如裂缝或褶皱，实际上不会接收到太多的间接光，可与高度图一起使用 Detail Maps 参考StandardShader:第二细节纹理，应用第二反照率图和第二法线图，在近距离观察时有清晰的细节，比如毛孔、细小的裂缝等。计算昂贵 常用纹理NormalMap 法线纹理：比较常用HeightMap 高度纹理(视差映射)：手机平台不常用，使用法线纹理替代Occlusion Map：细节纹理Secondary Maps (Detail Maps) &amp; Detail Mask：细节纹理高度图-HeightMap高度图为了模拟平面的凹凸程度，将高度(黑白色)数据存储在纹理中，由于纹理数据是二维的，即u轴和v轴，那为了得到这些数据为每个片段生成法向量，可分别在u轴和v轴上采样。先从U轴计算：f(u)=h ,如果知道了斜率就能求得u轴上所有点的法向量，但斜率由h的变化程度高低决定。为了近似得到从一个点到下一个点的高度差： 斜率采样示意图，从$f(0) \\rightarrow f(1)$. 这是对切向量的一个粗略的估算，它把整张纹理作为线性的斜率。那为了避免这种粗略计算，可以采样两个靠的更近点的。例如，从$0 \\rightarrow {1\\over2}$，那这两点的斜率$f=f({1 \\over 2})-f(0)$，同时f因子被缩小，需要乘以相应的倍数。$2f({1 \\over 2})-f(0)$。扩展开来，可以得到如下的函数：δ值越小越精确，必须大于0小于1.差分函数:\\[f^`(u) = {f(u+δ) - f(u) \\over δ}\\]有限差分函数:\\[f^`(u)= lim_{(δ\\rightarrow 0)} {f(u+δ) - f(u) \\over δ}\\]那么切向量就是\\(\\begin{bmatrix} 1&amp;f`(u)&amp;0\\end{bmatrix}^\\mathrm{T}\\)，从切向量计算法向量\\(\\begin{bmatrix} f’(u)&amp;1&amp;0\\end{bmatrix}^\\mathrm{T}\\)sampler2D _HeightMap;float4 _HeightMap_TexelSize;//xy是纹素坐标(uv)，zw是整张纹理宽高float2 delta = float2(_HeightMap_TexelSize.x, 0);//u轴float h1 = tex2D(_HeightMap, i.uv);//模型uv在高度图采样float h2 = tex2D(_HeightMap, i.uv + delta);//二次采样//第一步套用公式//i.normal = float3(1, (h2 - h1) / delta.x, 0); //第二步优化，缩放向量并不改变方向，消除了除法操作//i.normal = float3( delta.x, (h2 - h1), 0);//第三步改变垂直方向，需要得到法向量正向垂直于表面，那么逆时针旋转90度以翻转x分量符号.//Y是扰动法向量的高低变化因子i.normal = float3( h1 - h2, 1 , 0);i.normal = normalize(i.normal);有限差分只在一个方向近似求值，为了更好近似可以在两个方向线性逼近.中心差分:\\[f^`(u)= lim_{(δ\\rightarrow 0)} {f(u+{δ\\over 2}) - f(u - {δ\\over 2}) \\over δ}\\]float2 delta = float2(_HeightMap_TexelSize.x * 0.5, 0);float h1 = tex2D(_HeightMap, i.uv - delta);float h2 = tex2D(_HeightMap, i.uv + delta);i.normal = float3(h1 - h2, 1, 0);那么$f’(u, v)$计算$f’(v)$同理,切向量\\(\\begin{bmatrix} 0&amp;f’(v)&amp;1\\end{bmatrix}^\\mathrm{T}\\)法向量是\\(\\begin{bmatrix} 0&amp;1&amp;f’(v)\\end{bmatrix}^\\mathrm{T}\\)float2 du = float2(_HeightMap_TexelSize.x * 0.5, 0);float u1 = tex2D(_HeightMap, i.uv - du);float u2 = tex2D(_HeightMap, i.uv + du);//float3 tu = float3(1, u2 - u1, 0);float2 dv = float2(0, _HeightMap_TexelSize.y * 0.5);float v1 = tex2D(_HeightMap, i.uv - dv);float v2 = tex2D(_HeightMap, i.uv + dv);//float3 tv = float3(0, v2 - v1, 1);//i.normal = cross(tv, tu);//直接使用叉积求出垂直于u和v轴的法向量=&gt;(0*(v2-v1)-(u2-u1)*1, 1*1-0*0, (u2-u1)*0-1*(v2-v1))=(u1-u2, 1, v1-v2)i.normal = float3(u1 - u2, 1, v1 - v2);i.normal = normalize(i.normal);\\(\\begin{bmatrix} 0\\\\f’(v)\\\\1 \\end{bmatrix}\\)x\\(\\begin{bmatrix} 1\\\\f’(u)\\\\0 \\end{bmatrix}\\)=\\(\\begin{bmatrix} -f’(u)\\\\1\\\\-f’(v) \\end{bmatrix}\\)法线-Normal Map高度图是每帧采样实时计算法线，为了避免高额计算量，采用预制法线纹理代替。 Unity中使用高度图. 导入高度图作为法线贴图预先计算法线纹理必须勾选Create from Grayscale，白色表示相对更高，黑色表示相对更低。像素分量范围是[0,1]，而法线分量范围[-1,1]。相互映射转换公式为：$pixel = {(normal+1)\\over 2}$$normal = pixel \\cdot 2 – 1$法线纹理呈现淡蓝色，这是因为法向映射最常见的约定是将向上的方向存储在Z分量中(垂直于表面外侧)，又由于DXT5nm纹理压缩格式的原因，只存储了X与Y分量舍弃了Z分量(Y分量存储在G通道，X分量存储在A通道，RB通道被舍弃)。通过推导法向量的单位向量可得Z分量：$|N| = |N|^2 = N_x{^2} + N_y{^2} + N_z{^2} = 1$$Nz = \\sqrt{1 - N_x{^2} - N_y{^2}}$//第一种方法// Unpack normal as DXT5nm (1, y, 1, x) or BC5 (x, y, 0, 1)//dxt5压缩对应的位置取wyi.normal.xy = tex2D(_NormalMap, i.uv).wy * 2 - 1;i.normal.xy *= _BumpScale;//计算Z之前缩放才有效，平坦凹凸程度i.normal.z = sqrt(1 - saturate(dot(i.normal.xy, i.normal.xy)));//dot模拟平方计算-((x,y)*(x,y))=-x方-y方i.normal = i.normal.xzy;i.normal = normalize(i.normal);//第二种方法//UnityStandardUtils.cginc包含了解码法线函数，替代上面的方法i.normal = UnpackScaleNormal(tex2D(_NormalMap, i.uv), _BumpScale);i.normal = i.normal.xzy;i.normal = normalize(i.normal);细节纹理与细节法线-Detail Maps(Second Texture) 与 Detail Normals第二细节纹理与MainTexture合并，简要代码如下：//顶点uv坐标映射到纹理uvi.uv.xy = TRANSFORM_TEX(v.uv, _MainTex);i.uv.zw = TRANSFORM_TEX(v.uv, _DetailTex);//计算第二纹理的影响float3 albedo = tex2D(_MainTex, i.uv.xy).rgb * _Tint.rgb;albedo *= tex2D(_DetailTex, i.uv.zw) * unity_ColorSpaceDouble;//颜色空间转换第二细节纹理的法线映射i.normal = UnpackScaleNormal(tex2D(_NormalMap, i.uv.xy), _BumpScale);i.normal = UnpackScaleNormal(tex2D(_DetailNormalMap, i.uv.zw), _DetailBumpScale);i.normal = i.normal.xzy;i.normal = normalize(i.normal);法线融合-Blending Normals方式一：(main.normal + details.normal) * 0.5; 简单容易，但结果不是很好。主纹理和细节纹理都变得平坦。理想情况下，当其中一个是平的，期望它不会影响到另一个。float3 mainNormal = UnpackScaleNormal(tex2D(_NormalMap, i.uv.xy), _BumpScale);float3 detailNormal = UnpackScaleNormal(tex2D(_DetailNormalMap, i.uv.zw), _DetailBumpScale);i.normal = (mainNormal + detailNormal) * 0.5;i.normal = i.normal.xzy;i.normal = normalize(i.normal);方式二：用z分量做缩放因子求偏导函数，然后相加。[Mx, My, Mz]T = [Mx/Mz, My/Mz, 1]T 同理求得detail偏导函数，然后相加：[Mx/Mz + Dx/Dz, My/Mz + Dy/Dz, 1]T .效果很好，但是在合并陡峭时仍将失去细节。float3 mainNormal = UnpackScaleNormal(tex2D(_NormalMap, i.uv.xy), _BumpScale);float3 detailNormal = UnpackScaleNormal(tex2D(_DetailNormalMap, i.uv.zw), _DetailBumpScale);i.normal = float3(mainNormal.xy / mainNormal.z + detailNormal.xy / detailNormal.z, 1);i.normal = i.normal.xzy;i.normal = normalize(i.normal);方式三：白色调和，对上一步合并法线分别乘以MzDz,然后再去掉x和y的缩放因子夸大缩放，使陡峭更加明显，同时平坦的法线，它不会影响到另一个了。float3 mainNormal = UnpackScaleNormal(tex2D(_NormalMap, i.uv.xy), _BumpScale);float3 detailNormal = UnpackScaleNormal(tex2D(_DetailNormalMap, i.uv.zw), _DetailBumpScale);i.normal = float3(mainNormal.xy + detailNormal.xy, mainNormal.z * detailNormal.z);//UnityStandardUtils包含了混合函数//i.normal = BlendNormals(mainNormal, detailNormal);i.normal = i.normal.xzy;i.normal = normalize(i.normal);View Code切线空间-Tangent Space切线空间的法线纹理：顶点为原点，z轴为法线方向，x轴为切线方向，y轴为垂直于xz的副切线方向。Unity导入模型计算切线默认使用了mikktspace(在顶点着色器计算)，也可以在片元着色器计算cross得到副切线向量。顶点下计算：struct VertexData {\tfloat4 tangent : TANGENT;};struct Interpolators {\tfloat4 tangent : TEXCOORD2;};使用UnityCG中的UnityObjectToWorldDir在顶点程序中将切线转换为世界空间。 当然，这仅适用于切线的XYZ部分。 它的W分量需要不加修改地传递。Interpolators MyVertexProgram (VertexData v) {\tInterpolators i;\ti.position = mul(UNITY_MATRIX_MVP, v.position);\ti.worldPos = mul(unity_ObjectToWorld, v.position);\ti.normal = UnityObjectToWorldNormal(v.normal);\ti.tangent = float4(UnityObjectToWorldDir(v.tangent.xyz), v.tangent.w);\ti.uv.xy = TRANSFORM_TEX(v.uv, _MainTex);\ti.uv.zw = TRANSFORM_TEX(v.uv, _DetailTex);\tComputeVertexLightColor(i);\treturn i;}现在我们可以将法线从切线空间转换为世界空间。float3 binormal = cross(i.normal, i.tangent.xyz) * i.tangent.w;i.normal = normalize(\ttangentSpaceNormal.x * i.tangent +\ttangentSpaceNormal.y * i.normal +\ttangentSpaceNormal.z * binormal);去掉显式YZ交换，将其与空间转换结合在一起。//tangentSpaceNormal = tangentSpaceNormal.xzy;\tfloat3 binormal = cross(i.normal, i.tangent.xyz) * i.tangent.w;i.normal = normalize(\ttangentSpaceNormal.x * i.tangent +\ttangentSpaceNormal.y * binormal +\ttangentSpaceNormal.z * i.normal）;在构造副法线时，还有一个额外的细节。假设一个对象的scale设置为(- 1,1,1)，这意味着它是镜像的。在这种情况下，我们必须翻转副法线，来正确地镜像切线空间。事实上，当奇数维数为负时，我们必须这样做。_UnityShaderVariables_通过定义float4 unity_WorldTransformParams变量来帮助我们完成这个任务。当需要翻转副法线时，它的第四个分量为- 1，否则为1。float3 binormal = cross(i.normal, i.tangent.xyz) *(i.tangent.w * unity_WorldTransformParams.w);转换空间在世界空间下计算fixed4 MyFrag(v2f v) : SV_TARGET{\t//...\tfloat3 tangentSpaceNormal= UnpackScaleNormal(tex2D(_NormalMap, i.uv.xy), _BumpScale);\t#if defined(BINORMAL_PER_FRAGMENT)\t float3 binormal = cross(v.normal, v.tangent.xyz) * v.tangent.w;\t#else\t float3 binormal = v.binormal;\t#endif\t//把切线空间转到世界空间\t//tangentSpaceNormal * [v.tangent,binromal, v.normal]T\tv.normal = normalize(\t tangentSpaceNormal.x * v.tangent +\t tangentSpaceNormal.y * binormal +\t tangentSpaceNormal.z * v.normal\t);\t//...}在切线空间计算//计算副切线float3 binormal = cross(normalize(i.normal), normalize(i.tangent.xyz)) * i.tangent.w;//切线空间矩阵//行优先的填充float3x3 t_matrix = float3x3(i.tangent.xyz, binormal, i.normal);//把各种信息转到切线空间下参与计算副切线在哪算合适在顶点计算不必计算叉乘函数，通过宏定义开启。struct Interpolators {\tfloat4 position : SV_POSITION;\tfloat4 uv : TEXCOORD0;\tfloat3 normal : TEXCOORD1;\t#if defined(BINORMAL_PER_FRAGMENT)\t\tfloat4 tangent : TEXCOORD2;\t#else\t\tfloat3 tangent : TEXCOORD2;\t\tfloat3 binormal : TEXCOORD3;\t#endif\tfloat3 worldPos : TEXCOORD4;\t#if defined(VERTEXLIGHT_ON)\t\tfloat3 vertexLightColor : TEXCOORD5;\t#endif};如果不确定在哪里计算比较好，可以同时支持这两种方法。假设定义了BINORMAL_PER_FRAGMENT，我们逐像素计算每个片段的副法线。否则，逐顶点计算。在前一种情况下，我们保持我们的float4 tangent变量 。在后者中，我们需要两个float3变量。float3 CreateBinormal (float3 normal, float3 tangent, float binormalSign) {\treturn cross(normal, tangent.xyz) *\t\t(binormalSign * unity_WorldTransformParams.w);}Interpolators MyVertexProgram (VertexData v) {\tInterpolators i;\ti.position = mul(UNITY_MATRIX_MVP, v.position);\ti.worldPos = mul(unity_ObjectToWorld, v.position);\ti.normal = UnityObjectToWorldNormal(v.normal);\t#if defined(BINORMAL_PER_FRAGMENT)\t\ti.tangent = float4(UnityObjectToWorldDir(v.tangent.xyz), v.tangent.w);\t#else\t\ti.tangent = UnityObjectToWorldDir(v.tangent.xyz);\t\ti.binormal = CreateBinormal(i.normal, i.tangent, v.tangent.w);\t#endif\ti.uv.xy = TRANSFORM_TEX(v.uv, _MainTex);\ti.uv.zw = TRANSFORM_TEX(v.uv, _DetailTex);\tComputeVertexLightColor(i);\treturn i;}void InitializeFragmentNormal(inout Interpolators i) {\tfloat3 mainNormal =\t\tUnpackScaleNormal(tex2D(_NormalMap, i.uv.xy), _BumpScale);\tfloat3 detailNormal =\t\tUnpackScaleNormal(tex2D(_DetailNormalMap, i.uv.zw), _DetailBumpScale);\tfloat3 tangentSpaceNormal = BlendNormals(mainNormal, detailNormal);\t#if defined(BINORMAL_PER_FRAGMENT)\t\tfloat3 binormal = CreateBinormal(i.normal, i.tangent.xyz, i.tangent.w);\t#else\t\tfloat3 binormal = i.binormal;\t#endif\ti.normal = normalize(\t\ttangentSpaceNormal.x * i.tangent +\t\ttangentSpaceNormal.y * binormal +\t\ttangentSpaceNormal.z * i.normal\t);}要对所有Pass块生效，需要使用CGINCLUDE … ENDCG包含" }, { "title": "Unity基础光照多光源采样(翻译五)", "url": "/posts/Unity-Multi-Light/", "categories": "翻译, Shader", "tags": "Unity3D, Shader, Light", "date": "2018-01-03 09:00:00 +0800", "snippet": "本篇摘要： 使用多个光源渲染 支持多光源类型 使用光照信息 计算顶点光照 了解球谐函数Include Files为了给Shader增加支持多个光源，我们需要增加更多Pass通道。但是这些Pass最终包含了几乎完全相似的代码，为了避免代码的重复性，我们可以通过把着色器代码移动到一个CG文件，然后在Shader代码中引用该文件在文件目录中手动创建一个MyLighting.cginc文件，再把FirstLighting.shader内从#pragma以下到ENDCG以上区间内代码拷贝进MyLighting.cginc文件。这样我们不直接在shader中写这些重复的代码，而是通过include引用。注意，.cginc文件也提供了类似的避免重复定义，#define XXX_INCLUDED，再把整个文件内容放置在预处理文件块中。#if !defined(MY_LIGHTING_INCLUDED)#define MY_LIGHTING_INCLUDED//...#endif第二光源-Direction新建两个方向光对象，参数设置如下图： 两个光源参数. 现在场景中有两个光，但是每个物体看起来没有什么区别。现在我们一次只激活一个光源，看看有什么变化。 左main光源，右minor光源. 增加第二个Pass当前场景内只能看见一个光源效果，这是由于MyMultiLightShader只有一个Pass且只计算了一个光源。Pass光照标签ForwardBase只计算主光源， 为了渲染额外的光源，需要增加一个Pass且指定光照标签为ForwardAdd方可计算额外的光源。SubShader{ Pass{ Tags { \"LightMode\" = \"ForwardBase\" } CGPROGRAM #pragma target 3.0 #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #include \"MyLighting.cginc\" ENDCG } Pass { Tags { \"LightMode\" = \"ForwardAdd\" } CGPROGRAM #pragma target 3.0 #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #include \"MyLighting.cginc\" ENDCG }}现在虽然计算了两个光源，但是ForwardAdd计算结果会直接覆盖ForwardBase的结果。我们需要把这两个光照效果结合起来，需要在ForwardAdd Pass内使用混合。UnityShader的Blend函数：如何通过定义两个因子来合并新旧数据？ 新旧数据分别与Blend函数的因子相乘然后相加，得到最终结果。如果Pass内没有Blend默认不混合，Blend One Zero。每个Pass计算后的数据会写入帧缓冲区中，也就会替换之前任何写入该缓冲区的内容。为了把新旧数据都能加到帧缓冲区，我们可以需要指示GPU使用Blend one one模式。Pass{ Tags { \"LightMode\" = \"ForwardAdd\" } Blend One One //...} 左无混合， 右one one混合. Z-buffer GPU`s depth buffer： 一个物体第一次被渲染，GPU就会检查该片元是否会渲染在其他已经渲染过的像素的前面，这些距离信息就存储在该缓冲区中。因此每个像素都有颜色和深度信息**，该深度表示从相机到最近表面的每个像素的距离。ForwardBase中，如果要渲染的片元前面没有任何内容(深度值最小)，它就是最靠近摄像机的表面。GPU也会继续运行fragment程序，生成新的颜色和记录新的深度。如果要渲染的片元的深度值最终比已经存在的大，说明它前面有东西，它就不会被渲染也不能看见。在forward add中重复计算minor光时，要添加到已经存在的灯光，再次运行fragment程序时，因为针对的是同一个对象，最终记录了完全相同的深度值。因此两次写入相同的深度信息是没必要的，用ZWrite off关闭它。Blend One OneZWrite Off合批-Draw Call Batches在Game视图右上角打开Stats窗口，可以更好地了解运行时发生的事情。查看Batches、Saved by batching数据。先只激活main光源。 Batches数据6，总共7. 场景内有5个对象，应该是5个Batches。见下图图 实际Batches. 通过FrameDebugger分析，实际是5个draw mesh加上3个内置阴影render函数，一共8个Batches。但是由于启用了动态批处理dynamic batching，所以有一个Saved by batching统计。那现在来消除这3个阴影渲染函数调用，打开Edit/Project Settings/Quality。Shadows选择Disable Shadows. 先无视它这个系统清屏Clear函数。 去掉了阴影渲染函数. 激活minor光源，如下图： **10** + 1 = 11. 10个Batches？ 因为这5个对象被渲染了两次，最终为10个批次，而不是上面的4个。 动态批处理失效了！Unity规定动态批处理最多只支持一个方向光作用的物体对象。帧调试-Frame Debugger通过Window/Frame Debugger打开可以清楚了解屏幕画面是如何被渲染出来的 Frame Debugger调试. 通过选择滑动条可单步调试渲染，窗口会自动显示每一步的细节。按照上面的顺序，优先画出了靠近相机的不透明物体，同时开启depth-buffer深度缓冲，这个front-to-back从前到后的渲染顺序是有效的，被遮挡的片元就会被跳过不渲染。如果使用back-to-front从后到前的顺序，同时关闭zwrite，就会覆写远处的像素，发生overdraw。Unity渲染顺序是front-to-back，同时Unity喜欢把相似的物体分组。例如，sphere和cube分开，可避免在不同mesh网格间切换；或者把使用相同的material分组。点光源Point Lights先关闭两个方向光，再创建一个Point Light光。然后打开Frame Debugger调试查看。单步调试发现，第一次渲染的的纯黑色，然后才有怪异的光。什么奇怪现象？即使没有激活方向光第一个base Pass始终都会渲染，因此渲染得到一个黑色轮廓。而第二个Pass会额外渲染一次，这次使用了point light代替了方向光，而代码任然是假设使用了方向光。光照函数-Light Function光越来越复杂了，现在把UnityLight的计算单独剥离为一个函数：UnityLight CreateLight(Interpolators i){ UnityLight light; light.color = _LightColor0.rgb; light.dir \t= _WorldSpaceLightPos0.xyz; light.ndotl = DotClamped(i.normal, lightDir); return light;}修改后的Fragment代码如下：float4 MyFragmentProgram (Interpolators i) : SV_TARGET { i.normal = normalize(i.normal); //float3 lightDir = _WorldSpaceLightPos0.xyz; //float3 lightColor = _LightColor0.rgb; float3 viewDir = normalize(_WorldSpaceCameraPos - i.worldPos); float3 albedo = tex2D(_MainTex, i.uv).rgb * _Tint.rgb; float3 specularTint; float oneMinusReflectivity; albedo = DiffuseAndSpecularFromMetallic( albedo, _Metallic, specularTint, oneMinusReflectivity ); // UnityLight light; // light.color = lightColor; // light.dir = lightDir; // light.ndotl = DotClamped(i.normal, lightDir); UnityLight light = CreateLight(i); UnityIndirect indirectLight; indirectLight.diffuse \t= 0; indirectLight.specular\t= 0; return UNITY_BRDF_PBS( albedo, specularTint, oneMinusReflectivity, _Smoothness, i.normal, viewDir, light, indirectLight );}光照位置-Light Position_WorldSpaceLightPos0变量包含的是当前光的位置，但是在方向光的情况下，它实际上保存的是光方向的朝向。而我们使用了Point Light，这个变量就只是光的位置了(如其名)。因此必须要我们自己计算光的方向：减去片元的世界位置再归一化得到。//light.dir = _WorldSpaceLightPos0.xyz;light.dir = normalize(_WorldSpaceLightPos0.xyz - i.worldPos);光的衰减-Light Attenuation在使用方向光的情况下，只需知道光的方向即可，因为它被认为是无限远的。 但是Point Light有明确的位置，这意味它到物体表面的距离也会产生影响，距离物体越远，物体表面越暗。也就是光的衰减。方向光的衰减是被假设为非常缓慢的以至于可以作为常亮，不需担心。那Point Light的衰减是什么样的？球形衰减：想象一下，从一个点向四面八方发射一束光子，随着时间推移，这些光子会以相同的移动速度逐渐远离这个点，就像组成了一个球体表面，而这个点就是球体中心。球的半径随着光子移动增长，光子的密度随着移动就会逐渐降低。这就决定了可见光的亮度。 球形衰减. 衰减公式：球的表面积计算公式 $s= 4πr^2$。我们可以通过除以该公式得到光子的密度。把4π作为影响光的强度因子，先忽略掉这个常数。这就得到了衰减因子为$1\\over d^2$，其中d是光的距离.UnityLight CreateLight(Interpolators i){ UnityLight light; //light.dir = _WorldSpaceLightPos0.xyz; float3 lightVec = _WorldSpaceLightPos0.xyz - i.worldPos; light.dir = normalize(lightVec); float attenuation = 1 / dot(lightVec, lightVec); light.color = _LightColor0.rgb * attenuation; light.ndotl = DotClamped(i.normal, light.dir); return light;} 过曝. 靠近光源时非常明亮，这是因为越靠近球体的中心点，距离就越小，直到趋近于0。修改公式$1 \\over 1 + d^2$。float attenuation = 1 / (1 + dot(lightVec, lightVec)); 正常光强. 光源范围-Light Range现实中，光子持续移动直到击中某个物体停止。光变的非常微弱直到肉眼不可见，这意味着光的范围是可能无限远的。而实际上我们不会浪费时间去渲染不可见光，所以我们必须在某个时候停止渲染。Point Light 和 Spot Light都有范围，位于范围内的物体将会使用此光源参与绘制，否则不会参与。它们的默认范围都是10，随着范围缩小，调用额外draw Call的物体会更少，这也会提高帧率。把范围缩小到1，当拖动光源时会清楚看见物体何时进出这个范围，物体会突然变亮或不亮，要修复它需要确保衰减和范围是同步的。为了确保物体移出光源范围不会突然出现光线过渡，这就要求衰减系数在最大范围时为0Unity把片元从世界空间转换到光源空间来计算点光源的衰减，光源空间是灯光对象本地空间坐标，按比例衰减。在该空间，点光源位于原点，任何超过一个单位的都不在该范围内，所以点到原点的距离的平方定义了衰减系数。Unity更进一步，使用距离的平方采样衰减图。这样做确保了衰减早一点下降到0。没有这步，移动光源进出范围时我们仍将看见物体突然变亮或不亮。这个算法函数在AutoLight.cginc文件中。 AutoLight 引用结构. 我们可以使用_UNITY_LIGHT_ATTENUATION_指令，注意其中有if预处理块，包含三个参数：第一个参数是attenuation；第二个参数是计算阴影；第三个参数是世界坐标。//UNITY_LIGHT_ATTENUATION#ifdef POINTuniform sampler2D _LightTexture0;uniform unityShadowCoord4x4 unity_WorldToLight;#define UNITY_LIGHT_ATTENUATION(destName, input, worldPos) \\ unityShadowCoord3 lightCoord = \\ mul(unity_WorldToLight, unityShadowCoord4(worldPos, 1)).xyz; \\ fixed destName = \\ (tex2D(_LightTexture0, dot(lightCoord, lightCoord).rr). \\ UNITY_ATTEN_CHANNEL * SHADOW_ATTENUATION(input));#endifunityShadowCoord4在其他地方定义的；点击产生一个单精度值，.rr是重复取值组成float2.然后用来采样衰减纹理，而纹理是1维数据，第二个分量也无关紧要； UNITY_ATTEN_CHANNEL可能是r或a,取决于目标平台。UnityLight CreateLight (Interpolators i) { UnityLight light; light.dir = normalize(_WorldSpaceLightPos0.xyz - i.worldPos); //\tfloat3 lightVec = _WorldSpaceLightPos0.xyz - i.worldPos; //\tfloat attenuation = 1 / (dot(lightVec, lightVec)); UNITY_LIGHT_ATTENUATION(attenuation, 0, i.worldPos); light.color = _LightColor0.rgb * attenuation; light.ndotl = DotClamped(i.normal, light.dir); return light;}需要在引用AutoLight文件之间宏定义POINT，才能呈现最终正确的画面.混合光源-Mixing Light关闭Point Light再次打开两个Directional light，这里又出现了错误的addition pass计算，把minor 方向光作为点光源计算。为了解决它，我们引入Shader variant 变体。变体-Shader Variants选中Shader文件，在Inspector点击Compileed code查看// Total snippets: 2// -----------------------------------------// Snippet #0 platforms ffffffff:Just one shader variant.// -----------------------------------------// Snippet #1 platforms ffffffff:Just one shader variant.打开文件看到2个snippets代码片段，这是shader的passes。分别是base pass 和 additive pass。我们想要在additive pass中创建既支持directional 光又支持point 光的变体，需要使用Unity提供的multi_compile声明关键字，Unity将自动为每个关键字生成独立的shader。变体数量多少会影响编译效率！Pass{ Tags { \"LightMode\" = \"ForwardAdd\" } Blend One One ZWrite Off CGPROGRAM #pragma target 3.0 #pragma multi_compile DIRECTION POINT #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram //#define POINT #include \"MyLighting.cginc\" ENDCG}编译后能看见2个关键字：// Total snippets: 2// -----------------------------------------// Snippet #0 platforms ffffffff:Just one shader variant.// -----------------------------------------// Snippet #1 platforms ffffffff:DIRECTION POINT2 keyword variants used in scene:DIRECTIONPOINT使用关键字-KeyWordsUnity决定使用那个变体，是基于当前光源类型和shader中定义的变体关键字。当渲染方向光它就使用_DIRECTIONAL_变体，当渲染点光源它就使用_POINT_变体。如果都不匹配，它就选着变体关键字列表中第一个变体。UnityLight CreateLight(Interpolators i){\tUnityLight light; #ifdef POINT float3 lightVec = _WorldSpaceLightPos0.xyz - i.worldPos; #else float3 lightVec = _WorldSpaceLightPos0.xyz; #endif\tUNITY_LIGHT_ATTENUATION(attenuation, 0, i.worldPos);\tlight.color \t\t= _LightColor0.rgb * attenuation;\tlight.dir \t\t\t= normalize(lightVec);\tlight.ndotl \t\t= DotClamped(i.normal, light.dir);\treturn light;} 变体-两次渲染. 聚光源-Spotlights上面说了方向光和点光源，Unity还提供了聚光灯。聚光灯与点光源类似，不过它发射的是呈圆锥形光束。同样，为了支持聚光灯，需再加一个变体支持。#pragma multi_compile DIRECTIONAL POINT 查看增加了SPOT光源编译后的变体文件// -----------------------------------------// Snippet #1 platforms ffffffff:DIRECTION POINT SPOT3 keyword variants used in scene:DIRECTIONPOINTSPOT聚光灯同样有一个（原点）发射点，朝锥形方向发射光子，所以也需要手动计算光的方向#if defined(POINT) || defined(SPOT)\tfloat3 lightVec = _WorldSpaceLightPos0.xyz - i.worldPos;#else\tfloat3 lightVec = _WorldSpaceLightPos0.xyz;#endif//...聚光源衰减聚光灯衰减方式开始时与点光源相同，转换到光源空间然后计算衰减因子。然后把原点后面所有点强制衰减为0，将光线限制在聚光灯前面的物体上。然后把光空间中X和Y坐标作为UV坐标采样纹理，用于遮罩光线，该纹理是一个边缘模糊的圆，就像圆锥体。同时变换到光空间实际上是透视变换并使用了其次坐标。 //UNITY_LIGHT_ATTENUATION#ifdef SPOTsampler2D _LightTexture0;unityShadowCoord4x4 unity_WorldToLight;sampler2D _LightTextureB0;inline fixed UnitySpotCookie(unityShadowCoord4 LightCoord){ return tex2D(_LightTexture0, LightCoord.xy / LightCoord.w + 0.5).w;}inline fixed UnitySpotAttenuate(unityShadowCoord3 LightCoord){ return tex2D(_LightTextureB0, dot(LightCoord, LightCoord).xx).UNITY_ATTEN_CHANNEL;}#defineUNITY_LIGHT_ATTENUATION(destName, input, worldPos) \\\\ unityShadowCoord4 lightCoord = mul(unity_WorldToLight, unityShadowCoord4(worldPos, 1)); \\\\ fixed shadow = UNITY_SHADOW_ATTENUATION(input, worldPos); \\\\ fixed destName = (lightCoord.z &gt; 0) * UnitySpotCookie(lightCoord) * UnitySpotAttenuate(lightCoord.xyz) * shadow;#endif光斑阴影-Light CookiesCookies名字来源于剪影[cucoloris]，是指在电影、戏剧、摄影中为光线添加阴影。Unity支持3种light Cookies：DirectionLight、spotLight、pointLight。Cookie需要采样到纹理中。聚光灯光斑阴影-Spotlight cookie默认的聚光灯遮罩纹理是一个模糊的圆，但它也可以是任意的正方形纹理且它的边缘alpha降到0即可。使用cookies的alpha通道遮罩光线，其他rgb通道无关紧要。 spot-light cookies设置. 方向光斑阴影-Directon light Cookiedirection lights的cookie是无限平铺，因此边缘必须无缝衔接，边缘不必过渡到0. direction cookie. cookie size大小决定了可视面积，反过来又影响平铺速度。默认为10。带有cookie的Direction light必须转换到光照空间，它也有自己的_UNITY_LIGHT_ATTENUTION_指令。Unity把它作为不同的方向光对待，放置到addive pass渲染，使用_DIRECTIONAL_COOKIE_启用。#pragma multi_compile DIRECTIONAL DIRECTIONAL_COOKIE POINT SPOT电光源光斑阴影-Point Light cookie点光源的cookie是一个围绕球性的cube map映射纹理，同时必须指定_Mapping_映射模式，使Unity知道如何解释图像，最好的方法是自己提供一张cube map，这里先指定自动映射模式。 Mapping模式. 必须增加POINT_COOKIE关键字编译，Unity提供了一个简短的的关键字语义。#pragma multi_compile_fwdadd//#pragma multi_compile DIRECTIONAL DIRECTIONAL_COOKIE POINT SPOT打开编译后的文件// Snippet #1 platforms ffffffff:DIRECTIONAL DIRECTIONAL_COOKIE POINT POINT_COOKIE SPOT5 keyword variants used in scene:POINTDIRECTIONALSPOTPOINT_COOKIEDIRECTIONAL_COOKIE同时带有cookie的点光源方向也需要自己计算#if defined(POINT) || defined(POINT_COOKIE) || defined(SPOT)\tlight.dir = normalize(_WorldSpaceLightPos0.xyz - i.worldPos);#else\tlight.dir = _WorldSpaceLightPos0.xyz;#endif 同时渲染三个光. 顶点光照计算-Vertex Lights在Forward前向渲染路径，每个可见的物体都必须在BasePass中渲染一次。这个Pass只关心主方向光，而有cookie的方向光会忽略。在此之上其他多余的光会自动增加additive pass。因此有多少光就会产生多少Draw Call。举例，场景中增加四个点光源，让所有物体处于光源范围内。1个base加上4个additive pass，一共25个draw call。即使再增加一个方向光，也不会增加draw call。在Unity/Edit/Quality中可以设置 Pixel Light Count，定义最大逐像素光照数量，这个决定了有多少个光会在片元函数被作为逐像素光照计算。默认为4个。每个物体渲染的灯光是不同的，Unity根据光的强度和距离从高到低排序，贡献最少的光首先被丢弃不参与计算。由于不同的光影响不同的物体，有可能出现矛盾的光照效果。当物体移动时可能变得更糟，移动会导致光线突然变化。这个问题很麻烦，因为光完全关闭了，幸运的是我们可以使用逐顶点渲染这一更节省性能的方式。这意味着光照计算都在顶点函数进行，然后得到的插值结果并传递给片元函数。可以使用定义_VERTEXLIGHT_ON_关键字激活计算。 Unity自带顶点光只支持Point Light。 这和逐顶点光照有区别(完全可以在顶点函数计算法线、光线方向、视野方向、反射反向，再用着色模型计算颜色传递给片元函数，优点性能好，缺点着色粗糙) 物体受光数量差异. Pass{ Tags { \"LightMode\" = \"ForwardBase\" } CGPROGRAM #pragma target 3.0 #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #pragma multi_compile _ VERTEXLIGHT_ON #include \"MyLighting.cginc\" ENDCG}一个顶点光-One Vertex Light把顶点光传到片元函数，需要在Interpolators结构体使用VERTEXLIGHT_ON关键字。然后定义一个生成顶点颜色的函数以解耦，由于是从Interpolators读写成员变量，需要inout修饰符。struct Interpolators { float4 position : SV_POSITION; float2 uv : TEXCOORD0; float3 normal : TEXCOORD1; float3 worldPos : TEXCOORD2; #if defined(VERTEXLIGHT_ON) float3 vertexLightColor : TEXCOORD3; #endif};创建一个单独的函数来计算这种颜色，使用了inout修饰符，同时读取和写入插值器。void ComputeVertexLightColor (inout Interpolators i) {}Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.worldPos = mul(unity_ObjectToWorld, v.position); i.normal = UnityObjectToWorldNormal(v.normal); i.uv = TRANSFORM_TEX(v.uv, _MainTex); ComputeVertexLightColor(i); return i;}UnityShaderVariables定义了一个顶点光颜色数组：unity_LightColor[0].rgbvoid ComputeVertexLightColor (inout Interpolators i) { #if defined(VERTEXLIGHT_ON) i.vertexLightColor = unity_LightColor[0].rgb; #endif}然后，在片元函数把顶点光照色增加到所有其他光照色。这可以把顶点光照色作为间接光对待。再把生成间接光的代码剥离解耦，把顶点光照色传递给间接光的漫反射。UnityIndirect CreateIndirectLight (Interpolators i) { UnityIndirect indirectLight; indirectLight.diffuse = 0; indirectLight.specular = 0; #if defined(VERTEXLIGHT_ON) indirectLight.diffuse = i.vertexLightColor; #endif return indirectLight;}float4 MyFragmentProgram (Interpolators i) : SV_TARGET { i.normal = normalize(i.normal); float3 viewDir = normalize(_WorldSpaceCameraPos - i.worldPos); float3 albedo = tex2D(_MainTex, i.uv).rgb * _Tint.rgb; float3 specularTint; float oneMinusReflectivity; albedo = DiffuseAndSpecularFromMetallic( albedo, _Metallic, specularTint, oneMinusReflectivity ); //\tUnityIndirect indirectLight; //\tindirectLight.diffuse = 0; //\tindirectLight.specular = 0; return UNITY_BRDF_PBS( albedo, specularTint, oneMinusReflectivity, _Smoothness, i.normal, viewDir, CreateLight(i), CreateIndirectLight(i) );}当把 Pixel Light Count 数量调为0时，每个物体被渲染为对应光照色的剪影。 纯色轮廓，0 Pixel Light Count. Unity支持多达四个顶点光，这些光的坐标存储在float4变量： unity_4LightPosX0 unity_4LightPosY0 unity_4LightPosZ0void ComputeVertexLightColor (inout Interpolators i) { #if defined(VERTEXLIGHT_ON) float3 lightPos = float3( unity_4LightPosX0.x, unity_4LightPosY0.x, unity_4LightPosZ0.x ); i.vertexLightColor = unity_LightColor[0].rgb; #endif}这几个变量定义在 UnityShaderVariables.cginc 文件，这些变量的x y z w表示依次表示每个光的(x,y,z)。 接下来计算光的方向、反射方向、衰减$1\\over 1+d^2$，得到最终的颜色。void ComputeVertexLightColor (inout Interpolators i) { #if defined(VERTEXLIGHT_ON) float3 lightPos = float3( unity_4LightPosX0.x, unity_4LightPosY0.x, unity_4LightPosZ0.x ); float3 lightVec = lightPos - i.worldPos; float3 lightDir = normalize(lightVec); float ndotl = DotClamped(i.normal, lightDir); float attenuation = 1 / (1 + dot(lightVec, lightVec)); i.vertexLightColor = unity_LightColor[0].rgb * ndotl * attenuation; #endif}用这计算大三角形插值的镜面反射会很糟，所幸Unity提供了衰减因子unity_4LightAtten0，可帮助近似计算像素光的衰减，$1\\over 1+d^2a$ 一个像素光呈现的轮廓色. 四个顶点光-Four Vertex Light为了支持4个顶点光，就需要手写四次类似的代码，然后把结果加在一起。Unity提供了Shade4PointLights函数，参数需要：3个光的位置，4个顶点光的颜色，4个顶点光的衰减色，顶点世界坐标，顶点法线。void CreateVertexLightColor(inout Interpolators i){\t#if defined(VERTEXLIGHT_ON)\ti.vertexLightCoolr = Shade4PointLights(\t\tunity_4LightPosX0, unity_4LightPosY0, unity_4LightPosZ0,\t\tunity_LightColor[0].rgb, unity_LightColor[1].rgb,\t\tunity_LightColor[4].rgb, unity_LightColor[3].rgb,\t\tunity_4LightAtten0, i.worldPos, i.normal\t);\t#endif}Shade4PointLights源码。不同的是计算光的方向 和 方向的摸，rsqrt平方根的倒数// Used in ForwardBase pass: Calculates diffuse lighting from 4 point lights, with data packed in a special way.float3 Shade4PointLights ( float4 lightPosX, float4 lightPosY, float4 lightPosZ, float3 lightColor0, float3 lightColor1, float3 lightColor2, float3 lightColor3, float4 lightAttenSq, float3 pos, float3 normal){ // to light vectors float4 toLightX = lightPosX - pos.x; float4 toLightY = lightPosY - pos.y; float4 toLightZ = lightPosZ - pos.z; // squared lengths float4 lengthSq = 0; lengthSq += toLightX * toLightX; lengthSq += toLightY * toLightY; lengthSq += toLightZ * toLightZ; // don't produce NaNs if some vertex position overlaps with the light lengthSq = max(lengthSq, 0.000001); // NdotL float4 ndotl = 0; ndotl += toLightX * normal.x; ndotl += toLightY * normal.y; ndotl += toLightZ * normal.z; // correct NdotL float4 corr = rsqrt(lengthSq);//平方根倒数 ndotl = max (float4(0,0,0,0), ndotl * corr); // attenuation float4 atten = 1.0 / (1.0 + lengthSq * lightAttenSq); float4 diff = ndotl * atten; // final color float3 col = 0; col += lightColor0 * diff.x; col += lightColor1 * diff.y; col += lightColor2 * diff.z; col += lightColor3 * diff.w; return col;} 4个顶点光. 像素光与顶点光：在Light组件RenderMode有两个重要选项，Important将指示该light被渲染为像素光，not Important指示该light被渲染为顶点光。下图是2个顶点和2个像素光对比。不管物体有没有处于四个顶点光范围内，其计算量不变。 两个顶点两个像素。**注意右下角差别**. 顶点和像素光对比. 球谐函数-Spherical Harmonics除了用像素光和顶点光以外，还可以用球谐函数计算支持所有光源类型。球谐函数思想是用一个函数描述入射光在球体表面某一点的情况。通常该函数用球坐标描述，但也可以用3D坐标。这允许使用物体的法向量来采样该函数。要创建该函数，需要采样所有方向上的光照强度，然后想办法转为一个连续函数。理想情况是在物体表面每个点都采样，但这是不现实的，这需要噪声理论算法近似模拟来完成。 首先，只能从物体本地原点角度定义该函数，这对沿物体表面变化不大的光照条件来说很好。尤其是对于小物体来说，光照要么弱要么距离远。这也意味着，这种计算方式与像素光或顶点光的情况不同。 其次，我们还需要近似模拟函数本身。这就要把任何连续函数拆解为多个不同频率的连续函数，这可能有无限多个频率函数来组合。从基本正弦函数开始 Sine wave, $Sin 2πx$. 增大一倍震动频率，降低振幅 双频率半振幅，$Sin 4πx\\over 2$$. 把以上两种频率振幅函数加在一起 $Sin 2πx + {Sin 4πx\\over 2}$. 基于上面，我们可以无限加大频率降低振幅 3倍 和 4倍. 把各频率加在一起，又可组成新的更复杂的函数 4倍振幅$\\sum{_{i=1} ^4} {Sin2\\pi ix \\over i^2}$. 本示例使用具有固定模式的规则正弦波函数。为了用正弦波函数描述任意函数，必须调整每个频段的频率，振幅和偏移，直到获得完美的结果匹配为止。使用的频带越少，近似值的准确性就越低。 该技术用于压缩其他很多东西，例如声音和图像数据。在我们的案例中，我们将使用它来近似计算3D照明。该函数的最大特征体现在最低频率处，为此需要丢弃最高频率处，这也意味着会丢失一些光照的细节变化。但是当光线变化不快时问题不大，所以我们需要再次限制漫反射光照。球谐函数频率-Spherical Harmonics Bands最简单的方式，假设各个方向的照明都是一样的，照明颜色近似为均匀色。 第一个波段标识为$Y_0^0$，它由单个子函数定义，该子函数只是一个常数值。 第二个波段引入线性方向光，所有光线方向一致。有三个函数分别用$Y_1^{-1}$、$Y_1^0$、$Y_1^1$标识。 每个函数都包含一个法线坐标，乘以一个常数。 第三个波段更加复杂。由五个函数，$Y_2^{−2}$、$Y_2^{-1}$、$Y_2^0$、$Y_2^1$、$Y_2^2$。这些函数是二次函数，这意味着它们包含两个法线坐标的乘积。先Unity只使用了三个波段描述球谐函数，定义在一张表内：   -2 -1 0 1 2 0     1     1   $-y\\sqrt3$ $z\\sqrt3$ $−x\\sqrt3$   2 $xy\\sqrt{15}$ $−yz\\sqrt{15}$ $(3z^2−1){\\sqrt{5}\\over2}$ $−xz\\sqrt{15}$ $(x^2−y^2){\\sqrt{15}\\over 2}$ 什么决定了这个函数的形状？ 表的索引用Y表示，$Y_i^j(i∈(0,2), j∈(-2,2))$. Sine代表Y轴。 $Y_i^j$从何而来? 球面谐波是拉普拉斯方程在球面上的一个解。数学是相当复杂的。函数的定义是$Y_m^l=K_l^me^{imφ}P_l^{|m|}cosθ,l∈N, –l ≤ m ≤ l$而P_l^m项是勒让德多项式和Klm项是标准化常数。这是复杂形式的定义，使用复数i和球坐标，φ和θ。你也可以使用它的一个真实的版本，用三维坐标计算这就引出了我们使用的函数。最终的结果是把所有九项计算后加在一起，简化后$a + by + cz + dx + exy + fyz + gz^2 + hxz + i(x^2−y^2)$，其中a到i是常数因子。float t = i.normal.x;return t &gt; 0 ? t : float4(1, 0, 0, 1) * -t; t=1. t=x,y,z. t=xy,yz,zz,xz,xx-yy. 实际运用球谐函数-Using Spherical HarmonicsUnity提供了现成的27数字可供使用，定义在_UnityShadervariables.cginc_文件中的7个half4变量。// SH lighting environment half4 unity_SHAr; half4 unity_SHAg; half4 unity_SHAb; half4 unity_SHBr; half4 unity_SHBg; half4 unity_SHBb; half4 unity_SHC;View Code同时_UnityCG.cginc_也提供了ShadeSH9球谐函数。// ShadeSH9源码实现// normal should be normalized, w=1.0half3 SHEvalLinearL0L1 (half4 normal){ half3 x; // Linear (L1) + constant (L0) polynomial terms x.r = dot(unity_SHAr,normal); x.g = dot(unity_SHAg,normal); x.b = dot(unity_SHAb,normal); return x;}// normal should be normalized, w=1.0half3 SHEvalLinearL2 (half4 normal){ half3 x1, x2; // 4 of the quadratic (L2) polynomials half4 vB = normal.xyzz * normal.yzzx; x1.r = dot(unity_SHBr,vB); x1.g = dot(unity_SHBg,vB); x1.b = dot(unity_SHBb,vB); // Final (5th) quadratic (L2) polynomial half vC = normal.x*normal.x - normal.y*normal.y; x2 = unity_SHC.rgb * vC; return x1 + x2;}// normal should be normalized, w=1.0// output in active color spacehalf3 ShadeSH9 (half4 normal){ // Linear + constant polynomial terms half3 res = SHEvalLinearL0L1 (normal); // Quadratic polynomials res += SHEvalLinearL2 (normal); #ifdef UNITY_COLORSPACE_GAMMA res = LinearToGammaSpace (res); #endif return res;}在片元函数直接返回球谐光照，并关闭所有light组件。float3 shColor = ShadeSH9(float4(i.normal, 1));return float4(shColor, 1);物体不再是纯黑色了，选取了环境光颜色 关闭环境光的球谐着色. 当场景里有大于light pixel count数量的光，多余的光会被计算为球谐光。如果不够，就会只采样环境光着色 多余的光用球谐着色. 像计算顶点光一样，把球谐光照数据加到漫反射间接光之上，同时确保不要提供负数值。UnityIndirect CreateIndirectLight (Interpolators i) { UnityIndirect indirectLight; indirectLight.diffuse = 0; indirectLight.specular = 0; #if defined(VERTEXLIGHT_ON) indirectLight.diffuse = i.vertexLightColor; #endif indirectLight.diffuse += max(0, ShadeSH9(float4(i.normal, 1))); return indirectLight;}float4 MyFragmentProgram (Interpolators i) : SV_TARGET {//\tfloat3 shColor = ShadeSH9(float4(i.normal, 1));//\treturn float4(shColor, 1); return UNITY_BRDF_PBS( albedo, specularTint, oneMinusReflectivity, _Smoothness, i.normal, viewDir, CreateLight(i), CreateIndirectLight(i) );} 2个important 6个not important. 由于球谐光是不重要的光，我们也像计算顶点光一样在base pass通道计算，但是不能跟顶点光使用同一个关键字，需要独立定义FORWARD_BASE_PASS关键字。#if defined(FORWARD_BASE_PASS) indirectLight.diffuse += max(0, ShadeSH9(float4(i.normal, 1)));#endif 三种着色：像素、顶点、球谐. 球谐采样Skybox关闭所有的光，使用默认天空盒。这时开始渲染天空盒，它是基于主方向光程序化生成的天空盒。由于没有激活light，光就像在地平线附近徘徊，同时物体选取了天空盒颜色着色，有那么点微妙变化。这是球谐函数作用的结果。物体突然变得更亮了!因为环境因素的影响非常大。程序skybox代表的是一个完美的晴天。在这种情况下，白色的表面会显得非常明亮。这种效果在伽马空间渲染时是最强的。在现实生活中并没有很多完全白色的表面，它们通常要暗得多。 左有球谐函数，右无. " }, { "title": "Unity基础光照(翻译四)", "url": "/posts/Unity-First-light/", "categories": "翻译, Shader", "tags": "Unity3D, Shader, Light", "date": "2018-01-03 08:00:00 +0800", "snippet": "本篇摘要 将法线从对象空间转换为世界空间。 使用方向光。 计算光的漫反射和镜面反射。 调整光的能耗强度。 应用金属工作流程。 学习使用Unity的PBS算法。 光照投射到物体. 法线 Normals可见光是电磁波谱中人眼可以感知的部分，可见光谱没有精确的范围。我们可以看到电磁波谱的一部分，也就是我们所知的可见光，因为人眼睛可以检测到电磁辐射，而光谱的其余部分对我们来说是不可见的。光的单个光量子被称为光子 整个电磁频谱是多少？ 光谱被分成光谱带。从低频到高频，这些被称为无线电波、微波、红外线、可见光、紫外线、X 射线和伽马射线。光源发出一束光，一些光会照射到物体上，一些光会从物体反射回来。如果那道反射到光最终照射到我们的眼睛或相机镜头上，那么我们就会看到这个物体。为了能看见游戏里的物体，我们已知道模型表面各个顶点和顶点坐标，但是不知道顶点方向。为此，我们需要知道顶点法线以计算其方向。计算网格法线创建一个材质球和shader，创建一些cube、sphere物体并使用创建的材质。Shader \"Custom/My First Lighting Shader\" {} 未着色物体. Unity内置的cube、sphere网格包含了顶点法线，可直接获取它们并传递给片元函数。struct VertexData { float4 position : POSITION; float3 normal : NORMAL; float2 uv : TEXCOORD0;};struct Interpolators { float4 position : SV_POSITION; float2 uv : TEXCOORD0; float3 normal : TEXCOORD1;};Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.uv = TRANSFORM_TEX(v.uv, _MainTex); i.position = mul(UNITY_MATRIX_MVP, v.position); i.normal = v.normal; return i;}现在能看见法线向量作为颜色输出的显示float4 MyFragmentProgram (Interpolators i) : SV_TARGET { return float4(i.normal = 0.5 + 0.5, 1);} 法线向量作为颜色渲染. 这些是直接来自网格的原始法线。立方体的面看起来是平的，因为每个面都只有四个顶点组成了两个三角面，这些顶点的法线都指向同一个方向。相比之下，球体的顶点法线都指向了不同的方向，从而产生了平滑的颜色过渡。动态批处理立方体法线发生了一些奇怪显示。我们希望每个立方体显示相同的颜色，不同视角下的立方体却改变了颜色。 变色立方体. 这是由动态批处理引起的。Unity规定将一定数量的网格顶点动态合并在一起，以减少绘制调用。而球体的网格数量顶点太多了导致不能合批，所以它们不会受到影响。要合并网格，必须将它们从本地空间转换为世界空间。对象是否被批处理以及如何被合批有一方面取决于它们如何排序以进行渲染。由于这种转换也会影响法线，这就是我们看到颜色变化的原因。这里关闭动态合批处理，先专注法线知识。 关闭动态合批. 除了动态批处理，Unity还支持进行静态批处理。这对于静态几何体的工作方式不同，但也涉及到世界空间的转换。静态批处理需要预先构建，这里先跳过。 没有批处理影响的法线颜色. 转换法线到世界空间除了动态批处理之外的物体, 其他所有物体法线都是在对象空间, 但是又需要知道它们在世界空间下的方向. 所以必须将法线从物体空间转换到世界空间, 为此需要进行矩阵计算.Unity将gamebject的多次的矩阵转换结构合并成一个变换矩阵,类似这样:$O = T_1\\cdot T_2\\cdot T_3\\cdot …$， 其中T是单独变换矩阵, O是所有T的组合.Unity内置了float4 unity_objectToWolrd矩阵变量, 该矩阵定义在UnityShaderVariables.cginc文件, 将该矩阵与顶点函数内的法线相乘, 可以将其转换到世界空间. 虽然是不同空间转换但是也要保持空间内的方向一致, 所以其次坐标第四个分量设为0.Interpolators MyVertexProgram (VertexData v){ Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.normal = mul(unity_ObjectToWorld, float4(v.normal, 0)); i.uv = TRANSFORM_TEX(v.uv, _MainTex); return i;}或者，也可以仅与矩阵的3x3部分相乘, 编译后的代码最终是一样的，因为编译器将消除所有乘以常数零的东西。i.normal = mul((float3x3)unity_ObjectToWorld, v.normal); 对象到世界空间. 法线现在位于世界空间中，但有些法线颜色看起来比其他法线更亮。那是因为它们当中有被缩放过。所以我们必须在转换后对它们进行归一化。i.normal = mul(unity_ObjectToWorld, float4(v.normal, 0));i.normal = normalize(i.normal); 归一化法线. 虽然对向量进行了归一化，但对于没有统一比例的法线，颜色看起来也有些不一致。这是因为当一个表面在某一维上被拉伸时，它的法线不会以同样的方式拉伸。 缩放x轴，顶点和法线变为$1 \\over 2$. 当缩放比例不均匀时，应将其反转为法线。法线将在归一标准化后与变形表面的形状相匹配。它对统一的比例没有影响。 缩放x轴，顶点变为$1 \\over 2$法线变为2. 所以我们必须反转比例，但旋转应该保持不变。那如何去翻转缩放？根据$O = [T_1]\\cdot[T_2]\\cdot[T_3]\\cdot…$矩阵描述了S缩放、R旋转、P位移，而每个T也可以拆解为SRP.即$O=[S1R1P1]\\cdot[S_2R_2P_2]\\cdot[S_3R_3P_3]\\cdot[…]$成立，但是法线不需要改变位移，去掉T3；同时每个T也不需要位移，去掉P。最后简化为：$O=[S_1R_1]\\cdot[S_2R_2]$目标是翻转S，所以object-to-world矩阵:\\(N = S^{-1}_1R_1 \\cdot S^{-1}_2R_2\\)而Unity提供了world-to-object矩阵，它是object-to-world的逆矩阵:\\(N^{-1} = O^{-1} =S_2^{-1}R_2^{-1}\\cdot S_1^{-1}R_1^{-1}\\)这个$O^{-1}$同时把旋转和变换顺序也翻转了，需要对$O^{-1}$进行转置消除对旋转的影响:\\((O^{-1})^T =(S_2^{-1}R_2^{-1}\\cdot S_1^{-1}R_1^{-1})^T = N\\)推导：$R^T = R^{-1}。 sin(-z) = –sin z , cos(-z) = cosz$\\({\\begin{bmatrix} Cosz&amp;-Sinz&amp;0\\\\ Sinz&amp;Cosz&amp;0\\\\ 0&amp;0&amp;0\\\\\\end{bmatrix}}^T\\)=\\(\\begin{bmatrix} Cosz&amp;SinZ&amp;0\\\\ -Sinz&amp;Cosz&amp;0\\\\ 0&amp;0&amp;0\\\\\\end{bmatrix}\\)=\\({\\begin{bmatrix} Cosz&amp;-Sinz&amp;0\\\\ Sinz&amp;Cosz&amp;0\\\\ 0&amp;0&amp;0\\\\\\end{bmatrix}}^{-1}\\)$O^{−1}=R_2^{-1}S_2^{−1}\\cdot R_1^{−1}S_1^{−1}=R_2^TS_2^{−1}\\cdot R_1^TS_1^{−1}.$$(O^{−1})^T=(S_1^{−1})^T(R_1^T)^T\\cdot (S_2^1)^T(R_2^T)^T=(S_1^{−1})^TR_1\\cdot (S_2^{−1})^TR_2.$缩放矩阵具有单位矩阵特性$(S^T=S)$:$(O^{−1})^T=S_1^{−1}R_1\\cdot S_2^{−1}R_2=N$因此，再对unity_WorldToObject矩阵转置后并与顶点法线相乘。i.normal = mul( transpose((float3x3)unity_WorldToObject), v.normal);i.normal = normalize(i.normal); 正确的世界空间法线. i.normal = mul(transpose((float3x3)unity_WorldToObject, v.normal);//这个写法行的通，只是学习一下上面的知识.汇编也很难看//汇编-----------9: dp3 r0.x, cb1[4].xyzx, v1.xyzx10: dp3 r0.y, cb1[5].xyzx, v1.xyzx11: dp3 r0.z, cb1[6].xyzx, v1.xyzx12: dp3 r0.w, r0.xyzx, r0.xyzx13: rsq r0.w, r0.w14: mul o2.xyz, r0.wwww, r0.xyzx//汇编-----------i.normal = mul(unity_ObjectToWorld, v.normal);//简化写法Unity也提供了更好的函数：unityObjectToWorldNormalInterpolators MyVertexProgram (VertexData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.normal = UnityObjectToWorldNormal(v.normal); i.uv = TRANSFORM_TEX(v.uv, _MainTex); return i;} UnityObjectToWorldNormal内部实现： // Transforms normal from object to world spaceinline float3 UnityObjectToWorldNormal( in float3 norm ) {\t// Multiply by transposed inverse matrix,\t// actually using transpose() generates badly optimized code\treturn normalize(\t\tunity_WorldToObject[0].xyz * norm.x +\t\tunity_WorldToObject[1].xyz * norm.y +\t\tunity_WorldToObject[2].xyz * norm.z\t);} 再次归一化法线 ReNormalsed在顶点函数中计算出正确的法线后，传递给片元函数。然而，在不同的单位长度向量之间进行值传递，并不能确保法线的归一化。所以我们必须在片元函数中再次对法线进行归一化。float4 MyFragmentProgram (Interpolators i) : SV_TARGET { i.normal = normalize(i.normal); return float4(i.normal = 0.5 + 0.5, 1);} 再次归一化. 虽然再次归一化会产生更好的结果，但是值的误差其实通常非常小。如果在移动设备上，这一步可以省略。 夸张的错误. 漫反射 Diffuse Shading我们看到的物体本身因为它们反射了光。反射有不同方式:漫反射、镜面反射、基于物理着色，先讨论下漫反射。漫反射发生的原因是光线不会直接从物体表面反弹，而是一部分光到达粗糙的表面后弥射开，另一部光穿透物体，在物体内部游走然后再次离开表面。这里我们先不用完全遵循现实世界的物理细节。从表面漫反射多少光取决于光线照射它的角度。大多数光在正面撞击表面时以0°角反射。随着这个角度的增加，反射的光量随之会减少。在90°时，不再有光线照射到表面，因此是黑暗的。漫射光的量与光的方向和表面法线之间的夹角的余弦成正比。这被称为兰伯特余弦定律。\\(Diffuse = Albedo \\cdot lightColor \\cdot DotClamped(lightDir, normal)\\) 漫反射. 我们可以通过计算表面法线和光方向的点积来确定这个朗伯反射系数。我们已经知道法线，但还不知道光的方向。在代码内给定一个直接从物体头顶上方照射的方向。float4 MyFragmentProgram (Interpolators i) : SV_TARGET { i.normal = normalize(i.normal); return dot(float3(0, 1, 0), i.normal);} 从上方照亮，左伽马 vs. 右线性。. 什么是点积？。 1 在几何上定义为\\(A \\cdot B = |A| \\cdot |B| \\cdot cos \\theta\\)点积表示两向量间角度的余弦值乘以两向量的长度，所以在两个单位位向量的情况下\\(A \\cdot B = cos \\theta = A \\cdot B = |A| \\cdot |B| \\cdot cos \\theta\\) 2 在代数上定义为\\(A \\cdot B = {\\sum_{i=1}^n} A_i B_i = A_1 B_1 + A_2 B_2 + ... + A_n B_n\\)这意味着可以通过将所有组件相乘后并将它们相加来计算它。 float dotProduct = v1.x * v2.x + v1.y * v2.y + v1.z * v2.z; 从视觉上看，这个操作将一个向量直接投射到另一个向量。仿佛在上面投下了阴影, 最终得到一个直角三角形，其底边的长度是点积的结果。如果两个向量都是单位长度，那就是它们角度的余弦值。 点积. 约束负数光照 Clamped Lighting当表面朝向光时计算点积有效，但当它远离光时则无效。这样做是防止物体表面被从后面来的光源照亮。当表面朝向光计算点积才是有意义的；当物体表面处于自己的阴影面是不需要接受光照的；当光的方向与法线的方向大于90°时点积的结果是负数。使用max函数return max(0, dot(float3(0, 1, 0), i.normal));Unity着色器更推荐使用saturate, 这个标准函数限制值在0-1之间。return saturate(dot(float3(0, 1, 0), i.normal));但是UnityStandardBRDF.cginc文件为程序定义了更方便方便的DotClamped函数。此函数执行点积结果并确保它永远不会是负数。它还包含许多其他照明函数，也包含其他有用的文件！#include \"UnityCG.cginc\"#include \"UnityStandardBRDF.cginc\"float4 MyFragmentProgram (Interpolators i) : SV_TARGET { i.normal = normalize(i.normal); return DotClamped(float3(0, 1, 0), i.normal);} DotClamped 内部实现 unity决定saturate在针对低能力着色器硬件和针对S3时使用它会更好 inline half DotClamped (half3 a, half3 b) { #if (SHADER_TARGET &lt; 30 || defined(SHADER_API_PS3)) return saturate(dot(a, b)); #else return max(0.0h, dot(a, b)); #endif} 因为UnityStandardBRDF已经包括UnityCG和其他一些文件，我们不必再明确地包含UnityCG//#include \"UnityCG.cginc\"#include \"UnityStandardBRDF.cginc\" UnityStandardBRDF引用结构. 光源 Light Source现在在场景中新建一个的方向光，而不是硬编码的光方向。默认情况下，每个Unity场景都需要有一个方向光。方向光被认为是无限远的，它的所有光线都来自完全相同的方向。当然，这在现实生活中是不正确的，但是太阳离地球太远了，光的方向近似平行的。 方向光. UnityShaderVariables定义float4 _WorldSpaceLightPos0，其中包含当前灯光的位置和光线来自的方向，在定向光的情况下，它有四个分量。因为它们是齐次坐标，所以我们的定向光的第四个分量是不需要的。float3 lightDir = _WorldSpaceLightPos0.xyz;return DotClamped(lightDir, i.normal);灯光模式 Light Mode在产生正确的结果之前，我们必须告诉Unity我们想要使用哪些光照数据。我们通过添加一个LightMode变量标记到我们的着色器通道。需要哪种光照模式取决于我们如何渲染场景。我们可以使用正向或延迟渲染路径，还有两种较旧的渲染模式。可以通过渲染设置选择渲染路径。它位于色彩空间选择的正上方。我们正在使用前向渲染，这是默认设置。 渲染通道. Pass { Tags { \"LightMode\" = \"ForwardBase\" } CGPROGRAM ENDCG} 漫反射光. 光照颜色 Light Color光并不总是白色的。每个光源都有自己的颜色，我们可以通过fixed4 _LightColor0获取, 它定义在UnityLightingCommon.cginc文件 是什么fixed4？ 这些是低精度数字关键字，在移动设备上以精度换取速度。在台式机上，fixed只是float. 精度优化是以后的主题。_LightColor0变量表示光的颜色乘以光的强度：首先它是有rgba颜色值，同时光组件有一个Intensity强度属性，会改变颜色值的大小。float4 MyFragmentProgram (Interpolators i) : SV_TARGET { i.normal = normalize(i.normal); float3 lightDir = _WorldSpaceLightPos0.xyz; float3 lightColor = _LightColor0.rgb; float3 diffuse = lightColor = DotClamped(lightDir, i.normal); return float4(diffuse, 1);} 光的颜色. 反照率 Albedo大多数物体材料会吸收一部分光，这成了它们的颜色。物体的漫反射率的颜色被称为反照率，它描述了多少红色、绿色、蓝色通道被漫反射，其余的颜色被吸收不反射。我们使用材料的纹理和色调定义它。而Albedo带有白化whiteness的含义，它作为因子控制物体由暗到亮。 无法逃脱的光会发生什么？ 光的能量被储存在物体中，通常以热量的形式存在。这就是为什么黑色的东西往往比白色的东西更温暖。材料漫反射率的颜色称为其反照率。Albedo在拉丁语中是白色的意思。所以它描述了有多少红色、绿色和蓝色通道被漫反射。其余的被吸收。我们可以使用材质的纹理和色调来定义它。Properties { _Tint (\"Tint\", Color) = (1, 1, 1, 1) _MainTex (\"Albedo\", 2D) = \"white\" {}}float4 MyFragmentProgram (Interpolators i) : SV_TARGET { i.normal = normalize(i.normal); float3 lightDir = _WorldSpaceLightPos0.xyz; float3 lightColor = _LightColor0.rgb; float3 albedo = tex2D(_MainTex, i.uv).rgb = _Tint.rgb; float3 diffuse = albedo = lightColor = DotClamped(lightDir, i.normal); return float4(diffuse, 1);} 在伽马和线性空间中使用反照率进行漫反射着色. 高光(镜面)反射 Specular Shading除了漫反射，还有镜面反射。当光线在撞击表面后没有扩散而是直接反射，光线从表面反射回来的角度等于它撞击表面的角度。这就是导致您在镜子中看到的反射的原因。表面需要极其光滑。观察者的位置对镜面反射很重要：仅当最终反射出的光朝向观察者是可见的，其他都看不见。所以，我们要知道从表面一点到观察者的方向，这就要求表面点和摄像机的世界位置。struct Interpolators { float4 position : SV_POSITION; float2 uv : TEXCOORD0; float3 normal : TEXCOORD1; float3 worldPos : TEXCOORD2;//add}Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.worldPos = mul(unity_ObjectToWorld, v.position);//add i.normal = UnityObjectToWorldNormal(v.normal); i.uv = TRANSFORM_TEX(v.uv, _MainTex); return i;}可以通过访问摄像机的位置float3 _WorldSpaceCameraPos定义在UnityShaderVariables.cginc文件，我们发现视图方向从中减去表面位置并进行归一化。float4 MyFragmentProgram (Interpolators i) : SV_TARGET { i.normal = normalize(i.normal); float3 lightDir = _WorldSpaceLightPos0.xyz; float3 viewDir = normalize(_WorldSpaceCameraPos - i.worldPos);//add float3 lightColor = _LightColor0.rgb; float3 albedo = tex2D(_MainTex, i.uv).rgb = _Tint.rgb; float3 diffuse = albedo = lightColor = DotClamped(lightDir, i.normal); return float4(diffuse, 1);}反射光照颜色 Reflecting Light要知道反射光的去向，我们可以使用标准reflect函数。它接受入射光线的方向和基于表面法线反射光线，我们要反向调整光的方向。float3 reflectionDir = reflect(-lightDir, i.normal);//addreturn float4(reflectionDir = 0.5 + 0.5, 1); 反射方向. reflect内部实现$D\\cdot N\\cdot D − 2\\cdot N\\cdot (N\\cdot D)$假设物体表面极其光滑，我们将只能在表面角度合适的地方看见反射光，在其他地方反射光对观察者不可见并呈现黑色。但实际上物体表面是不平整的，有太多细微的凹凸，这也意味着表面法线差别很大。尽管我们的观察方向不完全匹配反射方向，我们仍能看见一些反射光。当我们偏离反射方向越多，我们能看见的反射光就越少，所以我们继续约束dot点积值return DotClamped(viewDir, reflectionDir); 镜面反射. 光滑度 Smoothness这种效果产生的高光大小取决于材料的粗糙度。光滑的材料更好地聚焦光线，因此它们的高光较小。我们可以通过将其设为材质属性来控制这种平滑度。它通常定义为 0 到 1 之间的值，所以让我们将其设为滑块。Properties { _Tint (\"Tint\", Color) = (1, 1, 1, 1) _MainTex (\"Texture\", 2D) = \"white\" {} _Smoothness (\"Smoothness\", Range(0, 1)) = 0.5//add}float _Smoothness;//add我们使用smoothness作为因子，通过提高点积的幂次来缩小光点。但是必须要比1大得多才具有更好的效果。再给因子提高100倍。return pow(DotClamped(viewDir, reflectionDir), _Smoothness * 100); 看着像玉. Blinn-Phong光照公式上面使用的是Blinn reflection计算公式，但业界更常用Blinn-Phong reflection公式。通过使用一个光照方向和视野方向的半角方向，然后再取法向量和半角向量的点积结果来决定镜面反射。float3 reflectionDir = reflect(-lightDir, i.normal);float3 halfVector = normalize(lightDir + viewDir);return pow( DotClamped(halfVector, i.normal), _Smoothness = 100); Blinn-Phong反射. 这种方法会产生更大范围的高光，但这可以通过使用更高的平滑度值来抵消。尽管这两种方法仍然是近似值。一个很大的限制是它可以为从后面照亮的对象产生无效的高光。 不正确的反射，平滑度为0.01. 当使用低平滑度值时，这些伪影会变得很明显。它们可以通过使用阴影来隐藏，或者通过基于光照角度淡出高光来隐藏。Unity 的传统着色器也有这个问题，所以我们也不用担心。无论如何，我们很快就会转向另一种照明方法。高光颜色 Specular Color镜面反射的颜色与光源的颜色相匹配,所以增加颜色渲染float3 halfVector = normalize(lightDir + viewDir);float3 specular = lightColor = pow(//add DotClamped(halfVector, i.normal), _Smoothness = 100);return float4(specular, 1);//add反射的颜色也取决于材料，这与反照率不同。金属的反照率往往很小，同时具有很强的且通常是彩色的镜面反射率。相比之下，非金属往往具有明显的反照率，而它们的镜面反射率较弱且未着色。我们可以添加纹理和色调来定义镜面反射颜色，我们不要打扰另一种纹理，只需使用色​​调即可。Properties { _Tint (\"Tint\", Color) = (1, 1, 1, 1) _MainTex (\"Albedo\", 2D) = \"white\" {} _SpecularTint (\"Specular\", Color) = (0.5, 0.5, 0.5) _Smoothness (\"Smoothness\", Range(0, 1)) = 0.1}float4 _SpecularTint;float _Smoothness;float4 MyFragmentProgram (Interpolators i) : SV_TARGET { float3 halfVector = normalize(lightDir + viewDir); float3 specular = _SpecularTint.rgb = lightColor = pow( DotClamped(halfVector, i.normal), _Smoothness = 100 ); return float4(specular, 1);}我们可以使用颜色属性来控制镜面反射的着色和强度 有色镜面反射. 漫反射和镜面反射 Diffuse and Specular漫反射和镜面反射是照明中的两个难点部分。我们将它们加在一起，使我们的画面更加完整。return float4(diffuse + specular, 1); 漫反射加镜面反射 gama vs. linear. 能量守恒 Energy Conservation将漫反射和镜面反射加在一起是有问题的，结果可能比光源更亮。当使用低平滑度的全白高光时，这一点非常明显。 很亮的镜面，0.1平滑度. 当光照射到表面时，它的一部分光会作为镜面光反射回来，其余部分要么会穿透表面，要么会作为漫射光返回，要么被吸收。但我们目前没有考虑到这一点。相反，我们的光既反射又漫射。我们必须确保材质的漫反射和镜面反射部分的总和不能超过1。这保证了我们不会凭空创造光。如果总数小于1就可以了。那意味着部分光被吸收了。由于我们使用的是恒定的镜面反射色调，我们可以通过将其乘以1减去镜面反射来简单地调整反照率色调。但是手动执行此操作很不方便，特别是如果我们想使用特定的反照率色调。所以让我们在着色器中执行此操作。float3 albedo = tex2D(_MainTex, i.uv).rgb * _Tint.rgb;albedo == 1 - _SpecularTint.rgb;//add 不再那么亮. 漫反射和镜面反射的贡献现在是相关联的。高光越强，漫反射部分越暗。黑色镜面色调产生零反射，只会看到全强度的反照率。白色镜面色调会形成完美的镜子，因此完全消除了反照率。 能量守恒. 非金属单色 Monochrome在这种色调模型下，当Specular色调是灰度图时这中方法工作良好。其他颜色就会有奇怪的结果，例如红色Specular色调只会减少漫反射的红色部分(其他颜色被吸收了)。 红色镜面反射，青色反照率. 为了防止这种着色，我们可以使用单色能量守恒。这只是意味着我们使用镜面反射颜色的最强分量来降低反照率。To prevent this coloration, we can use monochrome energy conservation. This just means that we use the strongest component of the specular color to reduce the albedo.albedo *= (1 - max(_SpecularTint.r, max(_SpecularTint.g, _SpecularTint.b))); 单色能量守恒. 辅助函数Unity有一个实用函数来处理能量守恒EnergyConservationBetweenDiffuseAndSpecular，定义在UnityStandardUtils.cginc#include \"UnityStandardBRDF.cginc\"#include \"UnityStandardUtils.cginc\" 引用层次结构. 此函数将反照率和镜面反射颜色作为输入，并输出调整后的反照率和输出$1-reflectivity$。第三个输出参数是是一个减去镜面反射强度，我们拿来乘以反照率的因子。float3 albedo = tex2D(_MainTex, i.uv).rgb = _Tint.rgb;albedo *= (1 - max(_SpecularTint.r, max(_SpecularTint.g, _SpecularTint.b))) float oneMinusReflectivity;albedo = EnergyConservationBetweenDiffuseAndSpecular( albedo, _SpecularTint.rgb, oneMinusReflectivity); EnergyConservationBetweenDiffuseAndSpecular内部实现 它具有三种模式：无保护、单色或彩色。这些由#define语句控制。默认为单色。 half SpecularStrength(half3 specular) {\t#if (SHADER_TARGET &lt; 30)\t\t// SM2.0: instruction count limitation\t\t// SM2.0: simplified SpecularStrength\t\t// Red channel - because most metals are either monochrome\t\t// or with redish/yellowish tint\t\treturn specular.r;\t#else\t\treturn max(max(specular.r, specular.g), specular.b);\t#endif}// Diffuse/Spec Energy conservationinline half3 EnergyConservationBetweenDiffuseAndSpecular (\thalf3 albedo, half3 specColor, out half oneMinusReflectivity) {\toneMinusReflectivity = 1 - SpecularStrength(specColor);\t#if !UNITY_CONSERVE_ENERGY\t\treturn albedo;\t#elif UNITY_CONSERVE_ENERGY_MONOCHROME\t\treturn albedo * oneMinusReflectivity;\t#else\t\treturn albedo * (half3(1, 1, 1) - specColor);\t#endif} 金属工作流程 Metallic Workflow基本上有两种我们关心的材料。金属和非金属，后者也称为介电材料。目前，我们可以通过使用强烈的镜面反射色调来创建金属。我们可以通过使用弱单色镜面反射来创建电介质。这是镜面反射工作流程。如果我们可以在金属和非金属之间切换会简单得多。由于金属没有反照率，我们可以使用该颜色数据来代替它们的镜面色调。而且非金属无论如何都没有彩色镜面反射，所以我们根本不需要单独的镜面反射色调。这被称为金属工作流程。我们可以使用另一个滑块属性作为金属切换，以替换镜面反射色调。通常把它设置为0或1，因为某物要么是金属，要么不是金属。介于两者之间的值表示混合了金属和非金属成分的材料。Properties { _Tint (\"Tint\", Color) = (1, 1, 1, 1) _MainTex (\"Albedo\", 2D) = \"white\" {} //_SpecularTint (\"Specular\", Color) = (0.5, 0.5, 0.5) _Metallic (\"Metallic\", Range(0, 1)) = 0 _Smoothness (\"Smoothness\", Range(0, 1)) = 0.1}//float4 _SpecularTint;float _Metallic;float _Smoothness; 金属滑块. 现在我们可以从反照率和金属属性中推导出镜面色调。然后可以简单地将反照率乘以$1-metallic$。float3 specularTint = albedo * _Metallic;float oneMinusReflectivity = 1 - _Metallic;// albedo = EnergyConservationBetweenDiffuseAndSpecular(// albedo, _SpecularTint.rgb, oneMinusReflectivity~// );albedo *= oneMinusReflectivity;float3 diffuse = albedo * lightColor * DotClamped(lightDir, i.normal);float3 halfVector = normalize(lightDir + viewDir);float3 specular = specularTint * lightColor * pow( DotClamped(halfVector, i.normal), _Smoothness = 100);即使是纯电介质也有一些镜面反射,因此镜面反射强度和反射值与金属滑块的值不完全匹配,这也受到色彩空间的影响。幸运的是，UnityStandardUtils.cginc定义了DiffuseAndSpecularFromMetallic功能函数。float3 specularTint; // = albedo = _Metallic;float oneMinusReflectivity; // = 1 - _Metallic;//albedo *= oneMinusReflectivity;albedo = DiffuseAndSpecularFromMetallic( albedo, _Metallic, specularTint, oneMinusReflectivity); 金属工作流. DiffuseAndSpecularFromMetallic内部实现 它使用了half4 unity_ColorSpaceDielectricSpecUnity根据颜色空间设置的变量。 inline half OneMinusReflectivityFromMetallic(half metallic) {\t// We'll need oneMinusReflectivity, so\t// 1-reflectivity = 1-lerp(dielectricSpec, 1, metallic)\t// = lerp(1-dielectricSpec, 0, metallic)\t// store (1-dielectricSpec) in unity_ColorSpaceDielectricSpec.a, then\t//\t 1-reflectivity = lerp(alpha, 0, metallic)\t// = alpha + metallic=(0 - alpha)\t// = alpha - metallic = alpha\thalf oneMinusDielectricSpec = unity_ColorSpaceDielectricSpec.a;\treturn oneMinusDielectricSpec - metallic = oneMinusDielectricSpec;}inline half3 DiffuseAndSpecularFromMetallic (\thalf3 albedo, half metallic,\tout half3 specColor, out half oneMinusReflectivity) {\tspecColor = lerp(unity_ColorSpaceDielectricSpec.rgb, albedo, metallic);\toneMinusReflectivity = OneMinusReflectivityFromMetallic(metallic);\treturn albedo = oneMinusReflectivity;} 有一个细节是金属滑块本身应该在伽马空间中，但是在线性空间中渲染时，Unity不会自动校正这个值。我们可以使用该Gamma属性告诉Unity它还应该对我们的金属滑块应用伽马校正。[Gamma]_Metallic (\"Metallic\", Range(0, 1)) = 0经过一番调整之后，镜面反射对于非金属来说变得相当模糊，效果不好。为了改善这一点，我们需要一种更好的方法来计算光照。基于物理着色 Physically-Based ShadingBlinn-Phong方案长期以来一直是游戏行业的主流方案，但现在基于物理的着色（称为 PBS）风靡一时，因为它更加真实和可预测。Unity 的标准着色器也使用 PBS 方法。Unity 实际上有多种实现。它根据目标平台、硬件和 API 级别决定使用哪个。该算法可通过UNITY_BRDF_PBS宏访问，该宏定义在UnityPBSLighting.cginc,BRDF代表双向反射率分布函数。#include \"UnityStandardBRDF.cginc\"#include \"UnityStandardUtils.cginc\"#include \"UnityPBSLighting.cginc\" UnityPBSLighting的引用结构. UNITY_BRDF_PBS内部实现 UNITY_PBS_USE_BRDF1默认情况下由 Unity 设置，作为平台定义。这将选择最佳着色器，除非着色器目标低于 3.0。 // Default BRDF to use:#if !defined (UNITY_BRDF_PBS) // allow to explicitly override BRDF in custom shader // still add safe net for low shader models, // otherwise we might end up with shaders failing to compile #if SHADER_TARGET &lt; 30 #define UNITY_BRDF_PBS BRDF3_Unity_PBS #elif UNITY_PBS_USE_BRDF3 #define UNITY_BRDF_PBS BRDF3_Unity_PBS #elif UNITY_PBS_USE_BRDF2 #define UNITY_BRDF_PBS BRDF2_Unity_PBS #elif UNITY_PBS_USE_BRDF1 #define UNITY_BRDF_PBS BRDF1_Unity_PBS #elif defined(SHADER_TARGET_SURFACE_ANALYSIS) // we do preprocess pass during shader analysis and we dont // actually care about brdf as we need only inputs/outputs #define UNITY_BRDF_PBS BRDF1_Unity_PBS #else #error something broke in auto-choosing BRDF #endif#endif 这里跳过pbs详细算饭介绍，使用就好了。PBS仍然计算漫反射和镜面反射，与Blinn-Phong不同点在于PBS又一个菲涅耳反射分量计算。为了在以掠射角查看对象时获得反射，就要先确保能获取环境反射。Unity的BRDF函数返回一个RGBA颜色，alpha分量总是设置为1，所以我们可以直接让我们的片段程序返回它的结果。// float3 diffuse = albedo * lightColor * DotClamped(lightDir, i.normal);// float3 halfVector = normalize(lightDir + viewDir);// float3 specular = specularTint * lightColor * pow(// DotClamped(halfVector, i.normal),// _Smoothness * 100// );return UNITY_BRDF_PBS();UNITY_BRDF_PBS有八个参数，前两个是漫反射和镜面反射return UNITY_BRDF_PBS( albedo, specularTint);第三第四个参数必须是反射率和粗糙度。这些参数必须是减一的形式，这是一种优化。我们已经使用oneMinusReflectivity计算出来DiffuseAndSpecularFromMetallic。而平滑度是粗糙度的反义词，所以我们可以直接使用它。return UNITY_BRDF_PBS( albedo, specularTint, oneMinusReflectivity, _Smoothness);第五第六个参数是表面法线和观察方向return UNITY_BRDF_PBS( albedo, specularTint, oneMinusReflectivity, _Smoothness, i.normal, viewDir);最后两个参数是直接光和间接光光源结构 Light StructuresUnityLightingCommon定义了一个简单的UnityLight结构，Unity着色器使用它来传递光照数据。它包含光的颜色、方向和一个ndotl值：漫反射。得到这些光源信息把它放到光源结构中，并将它作为第七个参数传递。UnityLight light;light.color = lightColor;light.dir = lightDir;light.ndotl = DotClamped(i.normal, lightDir);return UNITY_BRDF_PBS( albedo, specularTint, oneMinusReflectivity, _Smoothness, i.normal, viewDir, light);最后一个参数是间接光。我们必须为此使用UnityIndirect结构，它也定义在UnityLightingCommon. 它包含两种颜色：漫反射和镜面反射。漫反射颜色代表环境光，而镜面反射颜色代表环境反射。float4 MyFragmentProgram (Interpolators i) : SV_TARGET { i.normal = normalize(i.normal); float3 lightDir = _WorldSpaceLightPos0.xyz; float3 viewDir = normalize(_WorldSpaceCameraPos - i.worldPos); float3 lightColor = _LightColor0.rgb; float3 albedo = tex2D(_MainTex, i.uv).rgb = _Tint.rgb; float3 specularTint; float oneMinusReflectivity; albedo = DiffuseAndSpecularFromMetallic( albedo, _Metallic, specularTint, oneMinusReflectivity ); UnityLight light; light.color = lightColor; light.dir = lightDir; light.ndotl = DotClamped(i.normal, lightDir); UnityIndirect indirectLight; indirectLight.diffuse = 0; indirectLight.specular = 0; return UNITY_BRDF_PBS( albedo, specularTint, oneMinusReflectivity, _Smoothness, i.normal, viewDir, light, indirectLight );} 非金属和金属，gama vs. linear. " }, { "title": "Unity 多纹理融合(翻译三)", "url": "/posts/Unity-Combine-Texture/", "categories": "翻译, Shader", "tags": "Unity3D, Shader, Texture", "date": "2018-01-02 20:00:00 +0800", "snippet": "本篇摘要： 采样多个纹理 应用细节纹理 处理线性空间中的颜色 使用 splat 地图纹理合并 融合多张纹理. 贴图在游戏应用广泛，但它们有局限性。无论以何种尺寸显示，它们都有固定数量的像素。如果需要被渲染到很小网格，可以使用mipmap来保持它们的部分细节。但是当渲染到很大的网格上，会变得模糊。我们也不能无中生有地渲染更多额外的细节。本文讨论了一些解决办法。细节纹理通常可以使用更大的纹理，意味着更多的像素和更多的细节。但是纹理的大小也是有限制的，取决于游戏包体大小和目标平台的内存，以及gpu采样能力。另一种增加像素密度的方法是平铺纹理。出一张尽可能小的贴图，设置为重复模式。近距离观察下重复感可能不会很明显。毕竟当你站着用鼻子接触墙壁时，你只会看到整面墙壁的一小部分。因此，我们能够通过拉伸与平铺纹理相结合的方式来尽可能地添加细节。为了尝试这一点，我们使用一张棱角明显的纹理。这是一个方格图，放入的工程内使用默认导入设置。 略微扭曲的网格纹理. 新建一个纹理融合shaderShader \"Custom/Textured With Detail\" { Properties { _Tint (\"Tint\", Color) = (1, 1, 1, 1) _MainTex (\"Texture\", 2D) = \"white\" {} } SubShader { }}使用此着色器创建一个新材质，然后为其指定该shader和网格纹理。 网格纹理. 将材质分配给quad并查看它。从远处看效果还行。但是靠得太近看会变得模糊不清。缺失一些细节，同时纹理压缩造成的伪影也会变得很明显。网格特写，显示低纹素密度和 DXT1 伪影。多个纹理样本 带有低像素密度和DXT1伪影. 多张纹理贴图采样现在我们只采样了单个纹理样本并将其用作片段着色器的结果，将采样的颜色存储在一个临时变量中。float4 MyFragmentProgram (Interpolators i) : SV_TARGET { float4 color = tex2D(_MainTex, i.uv) * _Tint; return color;}先假设可以通过引入平铺纹理的方式来增加像素密度。执行一次纹理采样函数，给它一个十倍采样面积，用这个结果替换临时存储的颜色原来的颜色输出到屏幕。float4 color = tex2D(_MainTex, i.uv) * _Tint;color = tex2D(_MainTex, i.uv * 10);return color;屏幕上会产生很多小网格。靠的很近再观察，结果不那么糟糕了。因为采样纹理用了平铺10次，所以很明显是一个重复的图案。硬编码平铺。 平铺纹理. 请注意，此时我们正在执行两个纹理采样，但最终却只使用其中一个。这似乎很浪费。是吗？看看编译的顶点程序。uniform sampler2D _MainTex;in vec2 vs_TEXCOORD0;layout(location = 0) out vec4 SV_TARGET0;vec2 t0;void main(){ t0.xy = vs_TEXCOORD0.xy * vec2(10.0, 10.0); SV_TARGET0 = texture(_MainTex, t0.xy); return;}SetTexture 0 [_MainTex] 2D 0 ps_4_0 dcl_sampler s0, mode_default dcl_resource_texture2d (float,float,float,float) t0 dcl_input_ps linear v0.xy dcl_output o0.xyzw dcl_temps 1 0: mul r0.xy, v0.xyxx, l(10.000000, 10.000000, 0.000000, 0.000000) 1: sample o0.xyzw, r0.xyxx, t0.xyzw, s0 2: ret是否注意到编译后的代码中只有一个纹理采样？没错，编译器为我们去掉了不必要的代码！编译器基本上会丢弃任何最终未使用的内容。我们不想丢弃原始采样到颜色，就要合并两次采样结果。让我们通过将它们相乘来做到这一点。再添加一个_Tint属性，叠加一层自定义颜色。float4 color = tex2D(_MainTex, i.uv) * _Tint;color *= tex2D(_MainTex, i.uv);return color;着色编译器会生成什么样的代码呢，对此有何影响？uniform sampler2D _MainTex;in vec2 vs_TEXCOORD0;layout(location = 0) out vec4 SV_TARGET0;mediump vec4 t16_0;lowp vec4 t10_0;void main(){ t10_0 = texture(_MainTex, vs_TEXCOORD0.xy); t16_0 = t10_0 * t10_0; SV_TARGET0 = t16_0 * _Tint; return;}SetTexture 0 [_MainTex] 2D 0ConstBuffer \"$Globals\" 144Vector 96 [_Tint]BindCB \"$Globals\" 0 ps_4_0 dcl_constantbuffer cb0[7], immediateIndexed dcl_sampler s0, mode_default dcl_resource_texture2d (float,float,float,float) t0 dcl_input_ps linear v0.xy dcl_output o0.xyzw dcl_temps 1 0: sample r0.xyzw, v0.xyxx, t0.xyzw, s0 1: mul r0.xyzw, r0.xyzw, r0.xyzw 2: mul o0.xyzw, r0.xyzw, cb0[6].xyzw 3: ret这次的纹理采样，编译器检测到重复对_MainTex采样代码。对其进行优化后纹理只采样一次，结果存储在寄存器中并重复使用。即使使用_Tint中间变量等，编译器也足够聪明，可以检测到此类代码重复。最终将所有结果汇总后输出。现在再对UV坐标平铺×10次，最终看到大网格和小网格的融合在一起color *= tex2D(_MainTex, i.uv * 10); 平铺纹理. 由于纹理采样时参数不再相同，编译器也必须保留两次采样。uniform sampler2D _MainTex;in vec2 vs_TEXCOORD0;layout(location = 0) out vec4 SV_TARGET0;vec4 t0;lowp vec4 t10_0;vec2 t1;lowp vec4 t10_1;void main(){ t10_0 = texture(_MainTex, vs_TEXCOORD0.xy); t0 = t10_0 * _Tint; t1.xy = vs_TEXCOORD0.xy * vec2(10.0, 10.0); t10_1 = texture(_MainTex, t1.xy); SV_TARGET0 = t0 * t10_1; return;}SetTexture 0 [_MainTex] 2D 0ConstBuffer \"$Globals\" 144Vector 96 [_Tint]BindCB \"$Globals\" 0 ps_4_0 dcl_constantbuffer cb0[7], immediateIndexed dcl_sampler s0, mode_default dcl_resource_texture2d (float,float,float,float) t0 dcl_input_ps linear v0.xy dcl_output o0.xyzw dcl_temps 2 0: sample r0.xyzw, v0.xyxx, t0.xyzw, s0 1: mul r0.xyzw, r0.xyzw, cb0[6].xyzw 2: mul r1.xy, v0.xyxx, l(10.000000, 10.000000, 0.000000, 0.000000) 3: sample r1.xyzw, r1.xyxx, t0.xyzw, s0 4: mul o0.xyzw, r0.xyzw, r1.xyzw 5: ret单独的细节纹理将两个纹理相乘时，结果会更暗。除非至少其中一种纹理是白色的。这是因为像素的每个颜色通道都有一个介于 0 和 1 之间的值。当向纹理添加细节时，可以通过该通道值来实现变暗或变亮。要使原始纹理变亮，给原始颜色乘2，使得每个颜色值都增大。color *= tex2D(_MainTex, i.uv * 10) * 2; 增亮颜色. 这种直接扩大倍数的做法很粗暴。我们知道任何数乘以1不变，但是对细节纹理色加倍时，但对于1/2这个分界值就有用了。颜色区间是0-1，低于1/2的值将是结果变暗，高于1/2的值将变亮。这里引入一张特殊的灰度细节纹理来处理。 细节灰度图. 灰度细节纹理？ 一般都是用灰度细节纹理来增白或加深原始颜色做二次细节调整，不是灰度图跳出的颜色不是那么直观的结果。要使用这个单独的细节纹理，我们必须在着色器中添加第二个纹理属性。使用灰色作为默认值，因为这不会改变主纹理的外观。Properties { _Tint (\"Tint\", Color) = (1, 1, 1, 1) _MainTex (\"Texture\", 2D) = \"white\" {} _DetailTex (\"Detail Texture\", 2D) = \"gray\" {}}将细节纹理分配给我们的材质并将其平铺设置为10。 两种纹理. 我们必须添加变量来访问细节纹理及其平铺和偏移数据sampler2D _MainTex, _DetailTex;float4 _MainTex_ST, _DetailTex_ST;使用两个UV我们应该使用细节纹理的平铺和偏移数据，而不是使用硬编码乘10。struct Interpolators { float4 position : SV_POSITION; float2 uv : TEXCOORD0; float2 uvDetail : TEXCOORD1;}通过使用主纹理uv对细节纹理进行采样，得到一个新的细节纹理uv。Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.uv = TRANSFORM_TEX(v.uv, _MainTex); i.uvDetail = TRANSFORM_TEX(v.uv, _DetailTex); return i;}再一次看看汇编代码uniform \tvec4 _Tint;uniform \tvec4 _MainTex_ST;uniform \tvec4 _DetailTex_ST;in vec4 in_POSITION0;in vec2 in_TEXCOORD0;out vec2 vs_TEXCOORD0;out vec2 vs_TEXCOORD1;vec4 t0;void main(){ t0 = in_POSITION0.yyyy * glstate_matrix_mvp[1]; t0 = glstate_matrix_mvp[0] * in_POSITION0.xxxx + t0; t0 = glstate_matrix_mvp[2] * in_POSITION0.zzzz + t0; gl_Position = glstate_matrix_mvp[3] * in_POSITION0.wwww + t0; vs_TEXCOORD0.xy = in_TEXCOORD0.xy * _MainTex_ST.xy + _MainTex_ST.zw; vs_TEXCOORD1.xy = in_TEXCOORD0.xy * _DetailTex_ST.xy + _DetailTex_ST.zw; return;}Vector 112 [_MainTex_ST]Vector 128 [_DetailTex_ST]ConstBuffer \"UnityPerDraw\" 352Matrix 0 [glstate_matrix_mvp]BindCB \"$Globals\" 0BindCB \"UnityPerDraw\" 1 vs_4_0 dcl_constantbuffer cb0[9], immediateIndexed dcl_constantbuffer cb1[4], immediateIndexed dcl_input v0.xyzw dcl_input v1.xy dcl_output_siv o0.xyzw, position dcl_output o1.xy dcl_output o1.zw dcl_temps 1 0: mul r0.xyzw, v0.yyyy, cb1[1].xyzw 1: mad r0.xyzw, cb1[0].xyzw, v0.xxxx, r0.xyzw 2: mad r0.xyzw, cb1[2].xyzw, v0.zzzz, r0.xyzw 3: mad o0.xyzw, cb1[3].xyzw, v0.wwww, r0.xyzw 4: mad o1.xy, v1.xyxx, cb0[7].xyxx, cb0[7].zwzz 5: mad o1.zw, v1.xxxy, cb0[8].xxxy, cb0[8].zzzw 6: ret注意两个 UV 输出是如何在两个编译器顶点程序中定义的。OpenGLCore使用vs_TEXCOORD0和vs_TEXCOORD1输出，相反Direct3D11只使用一个输出o1.// Output signature://// Name Index Mask Register SysValue Format Used// -------------------- ----- ------ -------- -------- ------- ------// SV_POSITION 0 xyzw 0 POS float xyzw// TEXCOORD 0 xy 1 NONE float xy // TEXCOORD 1 zw 1 NONE float zw上面代码意味着两个 UV 对都被打包到一个输出寄存器中。第一个在 X 和 Y 通道，第二个在 Z 和 W 通道。因为寄存器总是由四个数字组成的组。Direct3D 11 编译器利用了这一点。 试着手动打包输出?手动打包输出的常见原因是只有少数几个插值器可用。Shader Model 2硬件支持8个通用插补器，而Shader Model 3硬件支持10个。复杂着色器可能会遇到这个限制。现在我们可以在片段程序中使用额外的UV对。float4 MyFragmentProgram (Interpolators i) : SV_TARGET {\tfloat4 color = tex2D(_MainTex, i.uv) * _Tint;\tcolor *= tex2D(_DetailTex, i.uvDetail) * 2;\treturn color;}uniform \tvec4 _Tint;uniform \tvec4 _MainTex_ST;uniform \tvec4 _DetailTex_ST;uniform sampler2D _MainTex;uniform sampler2D _DetailTex;in vec2 vs_TEXCOORD0;in vec2 vs_TEXCOORD1;layout(location = 0) out vec4 SV_TARGET0;vec4 t0;lowp vec4 t10_0;lowp vec4 t10_1;void main(){ t10_0 = texture(_MainTex, vs_TEXCOORD0.xy); t0 = t10_0 * _Tint; t10_1 = texture(_DetailTex, vs_TEXCOORD1.xy); t0 = t0 * t10_1; SV_TARGET0 = t0 + t0; return;}SetTexture 0 [_MainTex] 2D 0SetTexture 1 [_DetailTex] 2D 1ConstBuffer \"$Globals\" 144Vector 96 [_Tint]BindCB \"$Globals\" 0 ps_4_0 dcl_constantbuffer cb0[7], immediateIndexed dcl_sampler s0, mode_default dcl_sampler s1, mode_default dcl_resource_texture2d (float,float,float,float) t0 dcl_resource_texture2d (float,float,float,float) t1 dcl_input_ps linear v0.xy dcl_input_ps linear v0.zw dcl_output o0.xyzw dcl_temps 2 0: sample r0.xyzw, v0.xyxx, t0.xyzw, s0 1: mul r0.xyzw, r0.xyzw, cb0[6].xyzw 2: sample r1.xyzw, v0.zwzz, t1.xyzw, s1 3: mul r0.xyzw, r0.xyzw, r1.xyzw 4: add o0.xyzw, r0.xyzw, r0.xyzw 5: ret基于细节纹理，主纹理变得更亮和更暗。 明暗两张纹理. 细节纹理渐变融合添加细节的想法是它们可以在近距离或放大时改善材质的外观。它们不应该在远处可见或缩小，因为这会使平铺变得明显。所以我们需要一种方法来随着纹理显示尺寸的减小而淡化细节。我们可以通过将细节纹理淡化为灰色来做到这一点，因为这不会导致颜色变化。需要做的就是在细节纹理的导入设置中启用Fadeout Mip Maps属性。这也会自动将过滤器模式切换为三线性，以便渐变为灰色是渐进的。 纹理过渡. 网格从详细到不详细的过渡非常明显，但通常不会注意到它。例如，这里是大理石材质的主纹理和细节纹理。 大理石纹理. 一旦我们的材质使用了这些纹理，细节纹理的过渡痕迹就不再明显了。 大理石材质. 然而，由于细节纹理的过渡加持，大理石材质在近距离看起来要好得多。 没有细节和有细节特写. 线性色彩空间当我们在 gamma 颜色空间中渲染场景时，我们的着色器工作正常，但如果我们切换到线性颜色空间，它就会出错。色彩空间在项目中设置。它配置在Other Settings播放器设置面板，可以通过Edit / Project Settings / Player. 选择颜色空间. 什么是gamma色彩空间?伽玛空间是指经过伽玛校正的色彩空间。伽玛校正是对光强度的调整。最简单的方法是将原始值提高到某个幂次，即$value^gamma$。 gamma为 1 表示没有变化， 随着gamma递增光强度递增。这种转换最初是为了适应 CRT 显示器的非线性特性而引入的。另一个好处是它也大致对应于我们的眼睛对不同光强度的敏感程度。我们注意到深色之间的差异比明亮颜色之间的差异更大。因此，将更多的数字位分配给较暗的值而不是较浅的值是有意义的。求幂运算允许将较低的值扩展到更大的范围，同时压缩较高的值。最广泛使用的图像颜色格式是sRGB。它使用比简单求幂更复杂的公式，sRGB以$1\\over 2.2$的平均gamma值存储颜色，正常情况下这是一个合理的近似值。要将此数据转换回其原始颜色，请2.2幂的伽马校正。 gamma$1\\over 2.2$encoding vs. $2.2$deconding. 假设Unity纹理的颜色存储为sRGB。在伽马空间中渲染时，着色器直接访问原始颜色和纹理数据。这是我们到目前为止所假设的。但是在线性空间中渲染时，就不再适用。 GPU会将采样的纹理颜色转换为线性空间。此外，Unity 也会将材质颜色属性转换为线性空间。然后着色器使用这些线性颜色进行操作。之后，片段程序的输出将被转换回伽马空间。使用线性颜色的优点之一是它可以实现更逼真的照明计算。这是因为光的相互作用在现实生活中是线性的，而不是指数的。不幸的是,切换到线性空间后，细节材料变得更暗。为什么会这样？ gama vs. linear 色彩空间. 因为我们将细节纹理采样后的颜色乘了2，所以$1\\over 2$的值不会导致主纹理发生任何变化。但是，转换为线性空间后，它会变为接近${1\\over 2} ^{2.2} \\approx 0.22$。加倍大约是0.44，远小于 1。这就解释了为什么变得更暗。我们可以通过在细节纹理的导入设置中启用Bypass sRGB Sampling来解决这个错误。这可以防止从伽玛到线性空间的转换，因此着色器将始终访问原始图像数据。但是，细节纹理是sRGB图像，所以结果仍然是错误的。最好的解决方案是重新对齐细节颜色，使它们再次以 1 为中心。我们可以通过${1 \\over {1\\over 2} ^{2.2}} \\approx 4.59$而不是乘以2来做到这一点。但我们只能在线性空间中渲染时才这样计算。UnityCG.cginc定义了一个统一变量，它将包含要与之相乘的正确数字。它是一个 float4，它的 rgb 分量有2或约4.59，这两个值视情况而定。由于伽马校正未应用于Alpha通道，因此始终为2。color *= tex2D(_DetailTex, i.uvDetail) * unity_ColorSpaceDouble;通过这种更改，无论我们在哪个颜色空间中渲染，我们的细节材质看起来都一样。纹理splat过渡遮罩细节纹理的一个限制是对整个表面使用相同的细节。 这适用于均匀的表面，如大理石板。但是，如果材质没有统一的外观，不希望在任何地方都使用相同的细节。考虑一个大地形。它可以有草、沙、岩石、雪等。希望这些地形类型有一定的细节。但是覆盖整个地形的纹理永远不会有足够的细节。可以通过为每种表面类型使用单独的纹理并平铺这些纹理来解决该问题。但是怎么知道在哪里使用哪种纹理呢？假设我们有一个具有两种不同表面类型的地形，什么时候决定使用哪种表面纹理呢。不是一就是二。我们可以用一个布尔值来表示这个逻辑。如果设置为true，我们使用第一个纹理，否则使用第二个。我们可以使用灰度纹理来存储这个选择。值1表示第一个纹理，而值0表示第二个纹理。事实上，我们可以使用这些值在两个纹理之间进行线性插值。然后介于 0 和 1 之间的值表示两种纹理之间的混合，这使得平滑过渡成为可能。这样的纹理被称为splat贴图。就像将多个地形特征喷溅到画布上一样。由于插值，这张地图甚至不需要高分辨率。 splat遮罩贴图. 将其添加到项目后，将其导入类型切换为高级。启用Bypass sRGB Sampling并指示其mipmap应在Linear Space中生成。因为该纹理不需要sRGB颜色。所以在线性空间中渲染时不应该进行转换。另外，将其 Wrap Mode 设置为clamp，因为我们不会平铺这张地图。 splat导入设置. 创建一个新的 Texture Splatting 着色器。Shader \"Custom/Texture Splatting\" { Properties { MainTex (\"Splat Map\", 2D) = \"white\" {} } SubShader { Pass { CGPROGRAM #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #include \"UnityCG.cginc\" sampler2D _MainTex; float4 _MainTex_ST; struct VertexData { float4 position : POSITION; float2 uv : TEXCOORD0; } struct Interpolators { float4 position : SV_POSITION; float2 uv : TEXCOORD0; } Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.uv = TRANSFORM_TEX(v.uv, _MainTex); return i; } float4 MyFragmentProgram (Interpolators i) : SV_TARGET { return tex2D(_MainTex, i.uv); } ENDCG } }}创建一个新材质并引用该shader，并将splat贴图指定为其主要纹理。 splat图渲染. 增加融合纹理为了能够在两个纹理之间进行选择，作为属性添加命名为Texture1和Texture2到我们的着色器中。Properties { _MainTex (\"Splat Map\", 2D) = \"white\" {} _Texture1 (\"Texture 1\", 2D) = \"white\" {} _Texture2 (\"Texture 2\", 2D) = \"white\" {}}可以为他们使用任何你想要的纹理,这里使用网格纹理和大理石纹理。 增加的额外纹理. 为添加到着色器的每个纹理修改平铺和偏移控制值。这需要我们将更多数据从顶点传递到片段着色器，或者在片元着色器中计算UV调整。 这很好，但通常地形的所有纹理都平铺相同。并且 splat 地图根本没有平铺。所以我们只需要一个平铺和偏移控件的实例。可以将属性控制添加到着色器属性之前，就像在C#代码中一样。NoScaleOffset将禁用纹理平铺和偏移。Properties { _MainTex (\"Splat Map\", 2D) = \"white\" {} [NoScaleOffset] _Texture1 (\"Texture 1\", 2D) = \"white\" {} [NoScaleOffset] _Texture2 (\"Texture 2\", 2D) = \"white\" {}}同时修改splat贴图tiling为4。 不需要额外的贴图纹理. 将采样器变量添加到我们的着色器代码中，但是不必添加它们对应的 _ST 变量。sampler2D _MainTex;float4 _MainTex_ST;sampler2D _Texture1, _Texture2;对两张纹理采样后叠加，颜色会得到加深，然后输出float4 MyFragmentProgram (Interpolators i) : SV_TARGET { return tex2D(_Texture1, i.uv) + tex2D(_Texture2, i.uv);} 纹理叠加. 使用splat贴图采样splat纹理需要顶点程序提供的UV坐标struct Interpolators { float4 position : SV_POSITION; float2 uv : TEXCOORD0; float2 uvSplat : TEXCOORD1;};Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.uv = TRANSFORM_TEX(v.uv, _MainTex); i.uvSplat = v.uv; return i;}然后，以在对其他纹理进行采样之前对splat贴图进行采样。float4 MyFragmentProgram (Interpolators i) : SV_TARGET { float4 splat = tex2D(_MainTex, i.uvSplat); return tex2D(_Texture1, i.uv) + tex2D(_Texture2, i.uv);}因为splat本身是单通道，可以任选一个RGB通道来来存储值，这里先决定使用第一个纹理与splat贴图R通道相乘。return tex2D(_Texture1, i.uv) * splat.r + tex2D(_Texture2, i.uv); 调制第一个纹理. 第一个纹理现在由splat贴图R通道调制。为了完成插值，我们必须将另一个纹理1-R相乘。return tex2D(_Texture1, i.uv) * splat.r + tex2D(_Texture2, i.uv) * (1 - splat.r); 调制两个纹理. RGB Splat贴图我们有一个功能性的splat材质，但它只支持两种纹理。我们可以支持更多吗？我们现在只使用了R通道，那么我们添加G和B通道怎么样？那么(1,0,0)代表第一个纹理，(0,1,0)代表第二个纹理，(0,0,1)代表第三个纹理。 为了在这三个之间获得正确的插值，我们只需要确保RGB通道的总和为1即可。但是等等，当我们只使用一个通道时，我们可以支持两个纹理。这是因为第二个纹理的权重是通过1-R得出的。同样的技巧适用于任意数量的通道。因此可以通过1-R-G-B支持另一种纹理。这导致了一个具有三种颜色和黑色的splat贴图。只要三个通道加起来不超过1，它就是一个有效的贴图。这里给出一张这样的贴图，导入Unity。 RGB splat 贴图. 当 R + G + B 超过1时会发生什么？ 那么前三个纹理的组合会太强。 同时，第四个纹理将被减去而不是被添加。 如果错误很小，那么不会注意到并且结果足够好。 示例 RGB 映射实际上并不完美，但不会注意到。 纹理压缩引入了更多错误，但同样难以察觉。 可以使用alpha通道吗？ 确实可以！ 这意味着单个 RGBA splat 贴图最多可以支持五种不同的地形类型。 但是对于本教程，四个就足够了。 如果要使用超过五个纹理，则必须使用多个splat贴图。虽然这是可能的，但最终会得到很多纹理采样此时可以使用更好的技术，例如纹理数组。为了支持 RGB splat贴图，我们必须在着色器中添加两个额外的纹理。为它们分配了大理石细节和测试纹理。Properties { _MainTex (\"Splat Map\", 2D) = \"white\" {} [NoScaleOffset] _Texture1 (\"Texture 1\", 2D) = \"white\" {} [NoScaleOffset] _Texture2 (\"Texture 2\", 2D) = \"white\" {} [NoScaleOffset] _Texture3 (\"Texture 3\", 2D) = \"white\" {} [NoScaleOffset] _Texture4 (\"Texture 4\", 2D) = \"white\" {}} Four textures. 将所需的变量添加到着色器。 再一次，没有额外的 _ST需要的变量。sampler2D _Texture1, _Texture2, _Texture3, _Texture4;在片段程序中，添加额外的纹理样本。第二个样本现在使用G通道，第三个使用B通道。最终样本用 (1 - R - G - B) 调制。retur1n tex2D(_Texture1, i.uv) * splat.r + tex2D(_Texture2, i.uv) * splat.g + tex2D(_Texture3, i.uv) * splat.b + tex2D(_Texture4, i.uv) * (1 - splat.r - splat.g - splat.b); 四个纹理飞溅. 为什么混合区域在线性色彩空间中看起来不同？ 我们的 splat 贴图绕过了 sRGB 采样，所以混合不应该取决于我们使用的颜色空间，对吧？ splat 地图确实不受影响。 但是发生混合的色彩空间确实发生了变化。 在伽马空间渲染的情况下，样本在伽马空间中混合，仅此而已。 但是在线性空间中渲染时，它们首先转换为线性空间，然后混合，然后再转换回伽马空间。 结果略有不同。 在线性空间中，混合也是线性的。 但在伽马空间中，混合偏向较深的颜色。现在知道如何应用细节纹理以及如何将多个纹理与splat贴图混合。也可以组合这些方法。可以将四个细节纹理添加到splat着色器并使用贴图在它们之间进行混合。当然，这需要四个额外的纹理采样，性能有限。还可以使用贴图来控制应用细节纹理的位置以及省略的位置。在这种情况下，需要一张单色贴图，它可以用作遮罩。当单个纹理同时包含表示多个不同材质的区域但没有地形那么大的面积时，这很有用。例如，如果我们的大理石纹理还包含金属片，则不希望在此处应用大理石细节。" }, { "title": "Unity Shader Fundamentals(翻译二)", "url": "/posts/Unity-Shader-Fundamentals/", "categories": "翻译, Shader", "tags": "Unity3D, Shader", "date": "2018-01-02 16:00:00 +0800", "snippet": "本篇摘要信息 顶点变换 Color pixels shader 属性 从顶点传数据至片元函数 查看编译后的shader代码场景初始化新建一个默认场景，新建一个圆球。这个默认场景本身进行了大量复杂的渲染，为了更容易的掌握Unity的渲染过程，我们先做一些简化设置，把默认的某些花里胡哨的东西先剥离掉。剥离天空盒打开Window-Lighting，查看光照设置选项。弹出带有3个选项卡的面板，我们先关注Scene选项卡. 默认光照. 第一选项卡Environment是跟环境光照相关，在这里可以设置天空盒。这个Default-Skybox当前正被用于场景的背景光、环境光、和反射光。设置为none就能关闭这些光。顺便把下面的Realtime Ligting和Mixed Lighting也关掉，现在还用不上，后面会陆续介绍。关闭了天空盒，环境颜色自动切换为了纯色，这个颜色默认是带着一丝蓝的黑灰色(说好的纯呢，外表很黑内心很蓝？)。而反射光会变成纯黑色。如下所示，设置后球体变暗了，背景变成了纯色。而这个背景深蓝色从哪里来的呢？ 简单光照. 这个深蓝色被定义在摄像机，它默认使用天空盒渲染，当天空盒失效后场景会默认退回到使用相机纯色模式。 默认的摄像机设置. 为了进一步简化渲染，再隐藏或删除方向光对象。这将消除场景中的直接光照，以及所有它投射的阴影。剩下纯色背景和球体的轮廓。 未着色球体. 图像渲染分两步绘制上面的场景，一是使用相机的背景色填充图像，然后再在上面画出球体的轮廓。Unity如何知道该画这个球体呢?我们有一个球体对象并且绑定了_MeshRenderer_组件，如果这个球体位于摄像机的视野内，那么它就会被渲染出来。Unity通过检测球体的边界盒是否与摄像机的视锥体相机来验证这一点。包围盒在Unity中定义为Bounds结构体Collider.bounds, Mesh.bounds, Renderer.bounds. 球体默认自带组件. Transform组件用于更改坐标、方向，以及网格和包围盒的尺寸。这里有对Transform层次结构的清晰描述。如果一个物体最终处于摄像机视野内，它就会被安排渲染。最后，GPU负责渲染物体的mesh。这些具体的渲染指令在物体的material定义好的，这个material引用了一个shader-GPU程序。 2u分工. 当前这个球体使用了Unity的默认材质，自带了一个标准shader。我们现在把它去掉替换成自己的shader，从头开始写。创建一个Shader通过点击_Assets / Create / Shader / Unlit Shader创建并命名自己的shader，双击shader文件打开，并删除里面的内容从头写。 第一个shader. Shader是通过shader关键字定义，关键字是一个字符串，在下拉界面中选择时显示的也是该关键字。它不必与文件名相同。Shader \"Unlit/MyShader\"{ //...}保存文件，回到编辑器会收到警告提示none of subshaders/fallbacks are suitable，因为它是空的，没有sub-shader或回调shader。尽管这个shader没有内容也有警告，我们仍能指定给material。点击_Assets / Create / Material_创建，然后通过下拉菜单指定。 给材质指定Shader. 给球体指定上我们新建的Material，替换掉默认的。这时的球体会立即变成紫红色。发生这个的原因是Unity切换到了错误的shader，它故意使用这个颜色来提醒开发者这是一个错误。 指定MyMaterial. shader warning中提到了没有sub-shader. 我们可以使用sub-shader操作shader变量进行分组, 这允许程序员为不同的编译平台提供不同的sub-shader.例如我们可以用一个sub-shader既支持pc又支持手机平台.定义一个SubShader块Shader \"Unlit/MyShader\"{ SubShader { //... } }sub-shader至少包含一个以上的pass块, pass代码块是物体实际被渲染的地方，我们先写一个pass，然后在写多个pass。为了呈现多种效果，pass数量可能会超过一个以上，而则代表着物体要被渲染多次。Shader \"Unlit/MyShader\"{ SubShader { Pass { //... } }}我们的球体现在应该变成了白色，因为我们使用了一个空pass渲染，这也意味着我们的Shader没有出现任何错误了。 空shader效果. Shader程序现在我们要开始编写shader代码了，我们用的Unity着色器语言是HLSL和CG着色器语言的变体。所以必须指示CGPROGRAM关键字为代码的开始，同时要用ENDCG关键字做为结束。Pass{ CGPROGRAM ENDCG}再次打开编辑器编译后有一个警告Both vertex and fragment programs must be present,表示没有顶点和片元程序。shader由这两个程序组成，vertex顶点程序负责处理网格的顶点数据，这包含了从对象空间到显示空间的转换；而fragment片元程序负责为网格的三角形内的单个像素进行着色。 顶点片元函数. 同时，我们必须通过pragma指令告诉编译器使用哪些程序CGPROGRAM#pragma vertex MyVertexProgram#pragma fragment MyFragmentProgramENDCG编译器再次发出来错误提示，这次是因为它不能找到我们指定的程序片段，因为我们光声明没实现。首先vertex和fragment被写成方法，类似C#函数。先简单地创建两个同名的void方法。CGPROGRAM#pragma vertex MyVertexProgram#pragma fragment MyFragmentProgramvoid MyVertexProgram() {}void MyFragmentProgram() {}ENDCG这次编译后没有报错，但是球体从屏幕上消失了。Shader汇编Unity的shader编译器把我们的代码根据不同target-compile成了不同程序。不同的平台需要不同的解决方案，例如Direct3D是服务于Windows平台，OpenGL针对MacOs，OpenGL ES针对手机平台。这里我们不是在处理单个编译器，而是多个编译器。最终使用哪种编译器取决于目标平台，这些编译器也是不完全相同的，每个平台可能得到不同的结果。在这个例子中，我们的空程序在OpenGL和Direct3D 11下能很好的工作，但在Direct3D 9就会报错。在编辑器下点选MyFirstShader，在inspector面板可以查看该shader的一些信息，以及编译错误。这也有一个Compiled code and show按钮和下拉菜单。 shader检视面板信息. 如果你点击该按钮，Unity将会编译该shader并打开它，接着就可以查看生成的代码。我们试着先选择OpenGL Core，然后再选择D3D11，看看底层代码是怎么回事的。Shader \"Unlit/MyShader\" {SubShader { Pass { No keywords set in this variant. -- Vertex shader for \"glcore\": Shader Disassembly: #ifdef VERTEX #version 150 #extension GL_ARB_explicit_attrib_location : require #extension GL_ARB_shader_bit_encoding : enable void main() { return; } #endif #ifdef FRAGMENT #version 150 #extension GL_ARB_explicit_attrib_location : require #extension GL_ARB_shader_bit_encoding : enable void main() { return; } #endif -- Fragment shader for \"glcore\": Shader Disassembly: // All GLSL source is contained within the vertex program }}}提炼出两个main函数，有vertex和fragment程序#ifdef VERTEXvoid main(){ return;}#endif#ifdef FRAGMENTvoid main(){ return;}#endifD3D11自行查看，因为编译后的代码实在是太长了，不方便贴上来。只选取了一个片段：Pass { No keywords set in this variant. -- Vertex shader for \"d3d11\": Shader Disassembly: vs_4_0 0: ret -- Fragment shader for \"d3d11\": Shader Disassembly: ps_4_0 0: ret}引入其他文件编写shader代码很费劲，有时需要重复写类似的函数，为了简化书写，这里有一个类似C#程序的功能，引用其他类中的通用变量、函数等。使用#include指令就能加载一个文件。先试着加载Unity内部自带的UnityCG.cgincCGPROGRAM #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #include \"UnityCG.cginc\" void MyVertexProgram() { } void MyFragmentProgram() { }ENDCG下面是_UnityCg.cginc_的引用层次结构 UnityCG.cginc结构. UnityShaderVariables.cginc定义了一大堆渲染所需的着色器变量，比如矩阵变换、相机和光照数据等等。UnityInstancing.cginc内置在引擎安装包内，这是一种减少绘制调用的特定呈现技术。虽然它不直接包含文件，但它依赖于UnityShaderVariables。HLSLSupport.cginc设置了一些无论您的目标是哪个平台都可以使用相同的代码的功能。请注意，这些文件的内容将被复制到文件中，取代include指令。这发生在预处理步骤中，该步骤执行所有预处理指令。比如#include和#pragma。产生输出(输出语义)为了渲染物体，shader必须要产生结果。Vertex顶点函数必须要返回每个顶点的最终坐标：SV_POSITION。一个顶点有几个坐标分量？4个，因为我们使用了4x4变换矩阵。现在把函数类型从void改为float4,一个float4类型是一个由4个float类型简单组成。float4 MyVertexProgram() : SV_POSITION{ return 0;}Fragment片元函数返回像素的最终颜色：SV_TARGET。也是float4。float4 MyFragmentProgram() : SV_TARGET{ return 0;}Vertex顶点函数的输出作为Fragment片元函数的输入。输入的参数需要匹配！float4 MyFragmentProgram( float4 position : SV_POSITION) : SV_TARGET{ return 0;}然后看看Unity的shader汇编//--------------D3D11------------------- Vertex shader for \"d3d11\":Shader Disassembly: vs_4_0 //顶点着色器版本 dcl_output_siv o0.xyzw, position //声明o0作为输出值，带有系统值 0: mov o0.xyzw, l(0,0,0,0) //把(0,0,0,0)移动到o0中 1: ret //返回-- Fragment shader for \"d3d11\":Shader Disassembly: ps_4_0 dcl_output o0.xyzw 0: mov o0.xyzw, l(0,0,0,0) 1: ret//---------------GL CORE-----------#ifdef VERTEXvoid main(){ gl_Position = vec4(0.0, 0.0, 0.0, 0.0); return;}#endif#ifdef FRAGMENTlayout(location = 0) out vec4 SV_TARGET0;void main(){ SV_TARGET0 = vec4(0.0, 0.0, 0.0, 0.0); return;}#endif顶点变换把球给我画出来！为了得到模型空间的顶点坐标，给ertex顶点函数增加一条语义POSITON。而模型空间的顶点坐标是其次坐标。先直接返回这个顶点坐标，贴汇编：//---D3d11------- vs_4_0 //版本 dcl_input v0.xyzw //申明v0 输入系统值 dcl_output_siv o0.xyzw, position //申明o0 输出系统值 0: mov o0.xyzw, v0.xyzw //把v0值 移动到 o0 1: ret//--GL CORE---#ifdef VERTEXin vec4 in_POSITION0;void main(){ gl_Position = in_POSITION0; return;}#endifView Code 扭曲的球. 使用MVP：model_view_projection矩阵变换顶点坐标，该值定义在UnityShaderVariables文件，变量名是UNITY_MATRIX_MVP。改为：mul函数定义return mul(UNITY_MATRIX_MVP, position);贴汇编看看-- Vertex shader for \"d3d11\":// Stats: 8 mathUses vertex data channel \"Vertex\"//cbuffers常量数据Constant Buffer \"UnityPerDraw\" (160 bytes) on slot 0 { Matrix4x4 unity_ObjectToWorld at 0}Constant Buffer \"UnityPerFrame\" (384 bytes) on slot 1 { Matrix4x4 unity_MatrixVP at 272}Shader Disassembly: //版本 vs_4_0 //声明常量缓冲区cbuffers，逐字索引 dcl_constantbuffer CB0[4], immediateIndexed //cbuffers dcl_constantbuffer CB1[21], immediateIndexed //声明输入v0 dcl_input v0.xyz //声明输入o0 dcl_output_siv o0.xyzw, position //声明临时寄存器2个(r0-r1) dcl_temps 2 //将v0与cb0[1]相乘传递给r0 0: mul r0.xyzw, v0.yyyy, cb0[1].xyzw /*第0行的计算步骤 dest.x = cb0[0].x * v0.x + r0.x; dest.y = cb0[0].y * v0.x + r0.y; dest.z = cb0[0].z * v0.x + r0.z; dest.w = cb0[0].w * v0.x + r0.w;*/ //r0 = cb0 * v0 + r0 1: mad r0.xyzw, cb0[0].xyzw, v0.xxxx, r0.xyzw //同理1： 2: mad r0.xyzw, cb0[2].xyzw, v0.zzzz, r0.xyzw //r0 = r0 + cb0 3: add r0.xyzw, r0.xyzw, cb0[3].xyzw //r1 = r0 * cb1 4: mul r1.xyzw, r0.yyyy, cb1[18].xyzw 5: mad r1.xyzw, cb1[17].xyzw, r0.xxxx, r1.xyzw //同理1： 6: mad r1.xyzw, cb1[19].xyzw, r0.zzzz, r1.xyzw //同理1： 7: mad o0.xyzw, cb1[20].xyzw, r0.wwww, r1.xyzw //同理1： 8: retView Code 正确的球. 像素颜色先给Fragment函数返回点东西，float4 MyFragmentProgram(float4 position : SV_POSITION) : SV_TARGET{ return float4(1, 1, 0, 1);} 黄色球. 使用Shader属性Properties需要在pass块内声明一个同类型的同命名变量float4 _Tint;float4 MyFragmentProgram(float4 position : SV_POSITION) : SV_TARGET{ return _Tint;}看看片元函数的汇编-- Fragment shader for \"d3d11\":Constant Buffer \"$Globals\" (48 bytes) on slot 0 { Vector4 _Tint at 32}Shader Disassembly: ps_4_0 dcl_constantbuffer CB0[3], immediateIndexed dcl_output o0.xyzw 0: mov o0.xyzw, cb0[2].xyzw 1: retView Code 纯色球. 从顶点到片元上图纯色球，每个像素都是同一个颜色，但是美术给的效果图是五彩斑斓的，就需要GPU光栅化三角形，取三个处理过的顶点进行插值，找到三角形内所有像素并着色。 shader程序执行流程. 处理过的顶点数据不直接传递给Fragment片元函数，而在片元函数中访问插值本地数据，需要增加一个参数，并指定语义TEXCOORD0，它表示贴图的UV坐标。float4 MyVertexProgram( float4 position: POSITION, out float3 localPosition : TEXCOORD0) : SV_POSITION{ localPosition = position.xyz; return UnityObjectToClipPos(position);}float4 MyFragmentProgram( float4 position : SV_POSITION, float3 localPosition : TEXCOORD0) : SV_TARGET{ return float4(localPosition, 1);} 插值本地数据作为颜色. 结构体简化传递Fragment函数的参数，新建一个结构体struct Interpolators{ float4 position : SV_POSITION; float3 localPosition : TEXCOORD0;};Interpolators MyVertexProgram (float4 position: POSITION ){ Interpolators i; i.localPosition = position.xyz; i.position = UnityObjectToClipPos(position); return i;}float4 MyFragmentProgram (Interpolators i) : SV_TARGET{ return float4(i.localPosition, 1); } UnityObjectToClipPos是Unity5.6之后的优化：它对应mul(UNITY_MATRIX_MVP, v.vertex)，但是该函数使用了常数1作为第四个坐标而不是依赖网格数据，源码: inline float4 UnityObjectToClipPosInstanced(in float3 pos){ float4 w = mul(unity_ObjectToWorldArray[unity_InstanceID], float4(pos, 1.0) return mul(UNITY_MATRIX_VP, w));} 因为通过网格提供的数始终为1，但是编译器不能知晓。所幸干脆就直接写死为1.0，优化掉运行时再去计算第四个数到底是多少这一步。调整颜色因为负颜色被约束限制为零，我们的球体最终变得相当暗。 球体的自身半径为$1\\over 2$，因此颜色通道最终介于$-{1\\over 2}$和$1\\over 2$之间。我们希望将它们移动到 0-1 范围内，我们可以通过向所有通道添加$1\\over 2$来实现。return float4(i.localPosition + 0.5, 1);再看看汇编代码uniform \tvec4 _Tint;in vec3 vs_TEXCOORD0;layout(location = 0) out vec4 SV_TARGET0;vec4 t0;void main(){ t0.xyz = vs_TEXCOORD0.xyz + vec3(0.5, 0.5, 0.5); t0.w = 1.0; SV_TARGET0 = t0 * _Tint; return;}ConstBuffer \"$Globals\" 128Vector 96 [_Tint]BindCB \"$Globals\" 0 ps_4_0 dcl_constantbuffer cb0[7], immediateIndexed dcl_input_ps linear v0.xyz dcl_output o0.xyzw dcl_temps 1 0: add r0.xyz, v0.xyzx, l(0.500000, 0.500000, 0.500000, 0.000000) 1: mov r0.w, l(1.000000) 2: mul o0.xyzw, r0.xyzw, cb0[6].xyzw 3: ret 调整之后的颜色. 纹理如果想在模型网格上不添加更多三角形面数的情况下为网格添加更多更明显的细节和多样性呈现，可以使用纹理投影到网格的三角形上。纹理坐标用于控制投影，下图是2D坐标。不管纹理的实际纵横比如何，其水平坐标称为U，垂直坐标称为V，它们通常称为UV坐标。 uv坐标图. U坐标从左到右递增，起始点为0终点为1。 V坐标从下到上增加，除了Direct3D它从上到下。使用UV坐标Unity的默认网格具有适合纹理映射的UV坐标。顶点程序可以通过参数访问它们TEXCOORD0语义。然后传递给片元函数使用。struct Interpolators { float4 position : SV_POSITION; float2 uv : TEXCOORD0;};Interpolators MyVertexProgram (VertedData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.uv = v.uv; return i;}float4 MyFragmentProgram (Interpolators i) : SV_TARGET { return float4(i.uv, 1, 1);}再看看汇编in vec4 in_POSITION0;in vec2 in_TEXCOORD0;out vec2 vs_TEXCOORD0;vec4 t0;void main(){ t0 = in_POSITION0.yyyy * glstate_matrix_mvp[1]; t0 = glstate_matrix_mvp[0] * in_POSITION0.xxxx + t0; t0 = glstate_matrix_mvp[2] * in_POSITION0.zzzz + t0; gl_Position = glstate_matrix_mvp[3] * in_POSITION0.wwww + t0; //将UV坐标从顶点数据复制到Interpolators输出 vs_TEXCOORD0.xy = in_TEXCOORD0.xy; return;}Bind \"vertex\" VertexBind \"texcoord\" TexCoord0ConstBuffer \"UnityPerDraw\" 352Matrix 0 [glstate_matrix_mvp]BindCB \"UnityPerDraw\" 0 vs_4_0 dcl_constantbuffer cb0[4], immediateIndexed dcl_input v0.xyzw dcl_input v1.xy dcl_output_siv o0.xyzw, position dcl_output o1.xy dcl_temps 1 0: mul r0.xyzw, v0.yyyy, cb0[1].xyzw 1: mad r0.xyzw, cb0[0].xyzw, v0.xxxx, r0.xyzw 2: mad r0.xyzw, cb0[2].xyzw, v0.zzzz, r0.xyzw 3: mad o0.xyzw, cb0[3].xyzw, v0.wwww, r0.xyzw 4: mov o1.xy, v1.xyxx//将UV坐标从v1.xy传递到o1.xy 5: retUnity将UV坐标包裹在其球体周围，在球体两极处图像的顶部和底部。图像的左侧和右侧接缝连接在一起。 沿着该接缝，UV坐标值从0到1。 球体缝隙. 添加纹理要使用纹理，必须添加另一个着色器属性。 常规纹理属性的类型是2D ，因为还有其他类型的纹理。 默认值是引用 Unity 的默认纹理之一的字符串，可以是white 、 black 或 gray 。主纹理命名约定是_MainTex。 这也使您可以使用方便的 Material.mainTexture属性以通过脚本访问它。Properties { _Tint (\"Tint\", Color) = (1, 1, 1, 1) _MainTex (\"Texture\", 2D) = \"white\" {}} “white” {} 这个花括号有什么用？ 上古时代开发的固定功能着色器曾经需要纹理设置，这些设置被放在这些括号内，但现在它们不再使用了。即使它们现在已经无用，着色编译器仍然需要它们，如果忽略它们会产生错误。选中材质，查看inspector信息 纹理_white显示. 通过使用sampler2D类型为变量来访问着色器中的纹理。float4 _Tint;sampler2D _MainTex;通过在片段程序中使用tex2D函数，完成对纹理UV坐标进行采样。float4 MyFragmentProgram (Interpolators i) : SV_TARGET { return tex2D(_MainTex, i.uv);}再查看汇编后的shader代码uniform sampler2D _MainTex;in vec2 vs_TEXCOORD0;layout(location = 0) out vec4 SV_TARGET0;void main(){ SV_TARGET0 = texture(_MainTex, vs_TEXCOORD0.xy); return;}SetTexture 0 [_MainTex] 2D 0 ps_4_0 dcl_sampler s0, mode_default dcl_resource_texture2d (float,float,float,float) t0 dcl_input_ps linear v0.xy dcl_output o0.xyzw 0: sample o0.xyzw, v0.xyxx, t0.xyzw, s0 1: ret 带纹理球体. 球体两极附近会显得非常杂乱。 为什么会这样？ 发生纹理失真是因为插值在三角形中是线性的。 Unity 的球体在两极附近只有几个三角形，其中UV坐标失真最大。所以UV坐标在三角形与三角形的顶点之间是非线性变化，但在三角形内部的顶点之间的变化是线性的。所以球体两极纹理中的直线在三角形边界处突然改变方向。 跨三角形插值. 不同的网格具有不同的 UV 坐标，从而产生不同的映射。 Unity 的默认球体使用经纬度纹理映射，而网格是低分辨率立方体球体。 这足以进行测试，但您最好使用自定义球体网格以获得更好的结果。平铺和偏移为着色器添加纹理属性后，材质检查器不仅显示了纹理字段。它还显示了平铺和偏移控制。但是，更改这些 2D 向量目前没有任何效果。这些额外的纹理数据存储在材质中，也可以由着色器访问。 可以通过与关联材料同名的变量加上_ST 后缀来执行此操作。这个变量的类型必须是float4.sampler2D _MainTex;float4 _MainTex_ST;平铺向量用于缩放纹理，因此默认为$(1, 1)$。 它存储在变量的XY部分。 要使用它只需将它与UV 坐标相乘。 这可以在顶点着色器或片段着色器中完成。 在顶点着色器中这样做是有意义的，只为每个顶点而不是每个像素执行乘法。Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.uv = v.uv * _MainTex_ST.xy; return i;}偏移向量用于移动纹理，并存储在变量的ZW部分中。i.uv = v.uv * _MainTex_ST.xy + _MainTex_ST.zw;UnityCG.cginc包含一个方便的宏:TRANSFORM_TEXi.uv = TRANSFORM_TEX(v.uv, _MainTex);纹理设置 默认的纹理设置. Wrap Mode决定了在使用0-1范围之外的UV坐标进行采样时的输出。 设置为clamped时，UV被限制在 0–1 范围内。 这意味着超出边缘的像素与位于边缘的像素相同。 设置为repeat时，UV从0-1循环。这意味着超出边缘的像素与纹理另一侧的像素相同。Wrap Mode默认模式是重复纹理，这会导致它平铺。 在(2, 2)开始平铺. Mipmap和Filter当纹理的像素与它们投影到网格的像素不完全匹配时会发生什么？ 存在不匹配，必须以某种方式解决。这是由Filter Mode完成如何控制。 Point (no filter) 。这意味着当在某些 UV 坐标处对纹理进行采样时，将使用最近的像素。 这将使纹理呈现块状外观，除非像素精确映射到显示像素。 因此，它通常用于像素完美的渲染，或者需要块状样式时。 bilinear filtering双线性过滤。 当纹理在两个像素之间的某处被采样时，这两个像素被插值。 由于纹理是 2D 的，这发生在 U 轴和 V 轴上。 因此是双线性过滤。双线性过滤方法在像素密度小于显示像素密度时有效，因此当放大纹理时。结果会看起来很模糊。 当缩小纹理时，几乎不起作用。 相邻的显示像素最终会得到相距超过一个像素的样本。 这意味着将跳过部分纹理，这将导致粗糙的过渡，就像图像被锐化一样。双线性过滤问题的解决方案是在像素密度变得太高时使用较小的纹理。 显示屏上出现的纹理越小，应使用的版本越小。 这些较小的版本称为 mipmap，unity会自动为您生成。 每个连续的 mipmap 的宽度和高度都是上一层的一半。 所以当原始纹理大小为 512x512 时，mip 贴图为 256x256、128x128、64x64、32x32、16x16、8x8、4x4 和 2x2。 mipmap 是什么？ mipmap这个词是 MIP map 的缩写。 字母 MIP 代表拉丁短语 multum in parvo ，意思 是狭小空间中的众多 。 它是由 Lance Williams 在首次描述 mipmap 技术时创造的。 mipmap上有下无. 那么在何时使用哪个mipmap级别，它们看起来有什么不同呢？ 先通过在高级纹理设置中启用Fadeout Mip Maps。启用一个淡入淡出范围后，inspector将显示滑块。它定义了一个mipmap范围，在该范围内 mipmap 将转换为纯灰色。 越向右滑动过渡级别越小。 mipmap过渡级别. mipmap过渡之间呈现出模糊到锐利的快速，过渡不自然。这可以通过将过滤器模式切换为Trilinear三线性。 这与双线性过滤的工作方式相同，但它是在相邻的mipmap级别之间进行插值，这使得采样成本更高，但它平滑了 mipmap 级别之间的转换。另一种有用的技术是各向异性过滤。 当把Aniso Level设置为 0 时，纹理变得更加模糊。这与mipmap 级别的选择有关。 各向异性是什么意思？ 粗略地说，当某物在不同方向上看起来相似时，它就是各向同性的。 例如，一个无特征的立方体。 如果不是这种情况，则它是各向异性的。 例如，一个三角体，因为它的纹理朝着一个方向而不是另一个方向。当纹理以某个角度投影时，由于透视关系，通常最终会发现其中一个维度比另一个维度扭曲得更多。一个很好的例子是带纹理的地面。在远处，纹理的前后维度会显得比左右维度小得多。Aniso Level选择哪个 mipmap 级别基于最差维度。 如果差异很大，那么会得到一个在一维上非常模糊的结果。各向异性过滤通过解耦维度来缓解这种情况。除了统一缩小纹理外，它还提供在任一维度上缩放不同数量的版本。 因此，您不仅有 256x256 的 mipmap，而且还有 256x128、256x64 等的 mipmap。 没有和有Aniso Level. 请注意，这些额外的 mipmap 不像常规 mipmap 那样预先生成。 相反，它们是通过执行额外的纹理样本来模拟的。 因此它们不需要更多空间，但采样成本更高。 各向异性双线性过滤，过渡到灰色. 各向异性过滤的深度由 Aniso Level 控制。 为 0 时，它被禁用。 为 1 时，它变为启用并提供最小的效果。 16达到最大值。 但是，这些设置会受到项目质量设置的影响。 可以通过 Edit / Project Settings / Quality 访问质量设置。 找到 各向异性纹理 项目设置 渲染质量设置. 项目设置禁用各向异性纹理时，无论纹理设置如何，都不会发生各向异性过滤。当它设置为 Per Texture时，它​​完全由纹理自身设置控制。也可以设置为 Forced On ，强制把每个纹理都开启Ansio Level，但是若纹理设置Aniso Level为0，仍然不会使用各向异性过滤。" }, { "title": "Unity Transform&Matrix(翻译一)", "url": "/posts/Unity-Matrix&Transform/", "categories": "翻译, Shader", "tags": "Unity3D, Shader", "date": "2018-01-01 09:00:00 +0800", "snippet": "本篇摘要信息 matrix介绍 matrix推导 模拟transform缩放 旋转 位移功能可视空间Unity Shader是怎么知道一个像素该画在哪个位置？下面是先展示一组Cube，一步步分析下去 cube数组. 操控一组3维坐标创建一组10*10*10的3维Cube数组，并作为UnityMatrices对象的成员变量，接下来显示这些Cube在空间中的位置void InitCubeArray(){ for (int i =0 , z = 0; z &lt; generalCount; z++) { for (int y = 0; y &lt; generalCount; y++) { for (int x = 0; x &lt; generalCount; x++) { cubes[i++] = CreateCubesPoint(x, y, z); } } }}Transform CreateCubesPoint(int x, int y, int z){ GameObject cube = GameObject.CreatePrimitive(PrimitiveType.Cube); cube.transform.localScale = new Vector3(0.5f, 0.5f, 0.5f); cube.transform.localPosition = CreateCoordinate(x, y, z); cube.GetComponent&lt;MeshRenderer&gt;().material.color = CreateColor(x, y, z); return cube.transform;}设置每个Cube的位置，都以(0,0,0)为原点，(10-1)*0.5为Center左右两边对称Vector3 CreateCoordinate(int x, int y, int z){ return new Vector3( x - center, y - center, z - center );}然后再用自身坐标xyz分量与center的比率初始化颜色rgb。效果如上图Color CreateColor(int x, int y, int z){ return new Color( (float)x / generalCount, (float)y / generalCount, (float)z / generalCount );}空间变换positionning, rotating, and scalingCube数组中每个元素在空间中的变换有可能会有差异，虽然每个Cube变换的细节不同，但它们都需要经过某个方法来变换到空间中的某个坐标点。为此我们可以为所有变换创建一个abstract 基类，包含一个抽象的_Applay()_成员方法，由具体的变换组件去实现这个方法。public abstract class Transformation : MonoBehaviour{ public abstract Vector3 Apply(Vector3 point);}我们给这个UnityMatrices对象添加Transformation组件，同时检索Cube数组每个对象，将其坐标传入这个组件的_Apply()_方法进行计算得到新坐标并应用，这里始终以(0，0，0)作为每个Cube对象的原点坐标，而不能依赖其实际坐标，因为会每帧实时计算并改变。最后我们用泛型列表存储这种一系列变换组件方便统一计算。private void Update(){ GetComponents&lt;Transformation&gt;(transformations); // for (int i = 0; i &lt; cubes.Length; i++) // { // cubes[i].localPosition = TransformPoint(cubes[i].localPosition); // } for (int i =0 , z = 0; z &lt; generalCount; z++) { for (int y = 0; y &lt; generalCount; y++) { for (int x = 0; x &lt; generalCount; x++) { cubes[i++].localPosition = TransformPoint(x, y, z); } } }}Vector3 TransformPoint(int x, int y, int z){ Vector3 coordinates = CreateCoordinate(x, y, z); for (int i = 0; i &lt; transformations.Count; i++) { coordinates = transformations[i].Apply(coordinates); } return coordinates; }位移现在来做第一种变换：translation位移，这很简单。首先创建一个继承自Transformation组件子类，并定义一个表示自身位置属性的变量，并实现基类的抽象方法。然后添加给Cube数组对象public class PositionTransformation : Transformation{ public Vector3 position; public override Vector3 Apply(Vector3 point) { return point + position; }}现在可以向UnityMatrices对象添加PositionTransformation组件。这允许我们在不移动UnityMatrices对象的情况下移动数组中每个对象的坐标，所有的变换都发生在cube的局部空间。 位移. 缩放接下来做第二种变换：Scaling缩放。public class ScaleTransformation : Transformation{ public Vector3 scale = new Vector3(1, 1, 1); public override Vector3 Apply(Vector3 point) { point.x *= scale.x; point.y *= scale.y; point.z *= scale.z; return point; }} 缩放. 这里有一个问题：当进行缩放时，缩放会改变每个Cube对象的position。这是因为我们先计算了空间坐标，然后才缩放的它。而Unity中Transform组件是先缩放后位移。所以正确的计算顺序是：先缩放后位移。旋转(二维)第三种变换：Rotation旋转。public class RotationTransform : Transformation{ public Vector3 rotation; public override Vector3 Apply(Vector3 point) { return point;//先占位 }}旋转该如何工作呢？现在先假定在2维空间下一点P，绕Z轴旋转。Unity使用了左手坐标系，正向旋转是逆时针方向，如下图： 2维空间下绕Z轴旋转. 旋转一个点坐标后会发什么吗？先简单的考虑一个以原点为中心的单位圆上的一点P，设p初始位置为(1,0)，然后再以每90°增量进行一次旋转，如下图： 0°旋转到90°和180°变化. 由上图可知，点p(1,0)旋转一次(90°)变为了(0,1)，再旋转一次(180°)变为了(-1,0)，再往下旋转会变为(0,-1)，最后回到原位置(1,0). 那如果用点(0,1)作为初始位置，其变换顺序(0,1)$\\rightarrow$(-1,0)$\\rightarrow$(0,-1)$\\rightarrow$(1,0)$\\rightarrow$(0,1). 因此这个点坐标始终围绕0，1，0，-1进行循环，唯一得区别是起始点位置不同。那如果以45°增量进行旋转呢?它会在XY平面对角线上产生一点，其坐标为($\\pm \\sqrt{1 \\over 2},\\pm \\sqrt{1 \\over 2}$)，这些点到原点的距离始终是一致的。而这个循环顺序也类似上面，是$0, \\sqrt{1 \\over 2}, 1, \\sqrt{1 \\over 2}, 0, −\\sqrt{1 \\over 2}, −1, −\\sqrt{1 \\over 2}$。如果继续减小增量值，我们就可以得到一个Sine曲线。 Sine 和 Cosine曲线. 结合上面两张图，Sine曲线代表了Y分量，Cosine曲线代表了X分量，坐标用曲线表示就是$(\\cos z, \\sin z)$，若起始点为(1,0)则结果为$(cosz,sinz)$，逆时针旋转90°后(根据$sin(-z) = –sin z, cos(-z) = cos z$的对称性质)则结果为$(−sin z,cos z)$。因此我们可以用绕Z轴计算sine和cosine曲线，由于提供的是角度，但实际上sin及cos只能作用于弧度，所以我们需要转化它:public override Vector3 Apply(Vector3 point){ float radz = rotation.z * Mathf.Deg2Rad; float sinz = Mathf.Sin(radz); float cosz = Mathf.Cos(radz); return point;} 什么是弧度? 像度数一样，可以用作旋转的度量。使用单位圆时，弧度与圆周行进的距离相等。由于圆周的长度等于2π乘以圆的半径，因此1度等于2π/360 = π/180弧度。π 是圆的周长与其直径之间的比率。上述方法对于旋转(1,0)或(0,1)或许很好，那有米有旋转任意点的方式呢？ 这些点都是由X和Y定义的，我们可以把2维点(x,y)拆分为一个公式$xX+yY$，那么$x(1,0)+y(0,1)=(x, y)$是成立的。当旋转之后，可以用$x(cos z, sin z)+y(-sin z, cos z)$来得到经过正确旋转后的点。组合为坐标就变成了$(xcosZ−ysinZ,xsinZ+ycosZ)$.public override Vector3 Apply(Vector3 point){ float radz = rotation.z * Mathf.Deg2Rad; float sinz = Mathf.Sin(radz); float cosz = Mathf.Cos(radz); //return point; return new Vector3( point.x * cosz - point.y * sinz, point.x * sinz + point.y * cosz, point.z );}按照上述分析的缩放、旋转、位移的先后计算顺序，再在Unity内对比Transform的拖动旋转缩放的显示，二者的效果是一致的。void Start (){ //... gameObject.AddComponent&lt;ScaleTransformation&gt;(); gameObject.AddComponent&lt;RotationTransform&gt;(); gameObject.AddComponent&lt;PositionTransformation&gt;();} 最终变换效果. 旋转完全体现在我们只能绕Z轴旋转，但是为了能够复刻Unity的Transform组件那样的旋转，现在就得要支持绕X轴和绕Y轴旋转。虽然分别绕这些轴旋转与绕Z轴旋转的方法相似，但是当一次同时绕多个轴旋转时这就很复杂了。目标：一次同时绕多个轴旋转，迎难而上。2D矩阵现在开始，我们要把坐标书写格式由水平式替代为垂直式。把(x,y)被改写为\\(\\begin{bmatrix} x\\\\ y\\\\\\end{bmatrix}\\)把$(xcosZ−ysinZ,xsinZ+ycosZ)$也同样被拆分为\\(\\begin{bmatrix} xcosZ-ysinZ\\\\ xsinZ+ycosZ\\\\\\end{bmatrix}\\)，再把这个表达式进一步拆分：\\(\\begin{bmatrix} cosZ&amp;-sinZ\\\\ sinZ&amp;cosZ\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\\\end{bmatrix}\\)这就是矩阵乘法，2x2矩阵的第一列值代表X轴，第二列值代表了Y轴，计算公式如下：\\(\\begin{bmatrix} Xx&amp;Yx\\\\ Xy&amp;Yy\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} a\\\\ b\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} aXx + bYx\\\\ aXy + bYy\\\\\\end{bmatrix}\\) 由于Unity是采用左手法则，在上文中单位圆上一点绕Z轴旋转的增量度不同，cos代表X轴，sin代表Y轴，在结合本文的矩阵可得\\(\\begin{bmatrix} cos\\theta&amp;-sin\\theta\\\\ sin\\theta&amp;cos\\theta\\\\\\end{bmatrix}\\)$=cos\\theta$\\(\\begin{bmatrix} 1&amp;0\\\\ 0&amp;1\\\\\\end{bmatrix}\\)$+sin\\theta$\\(\\begin{bmatrix} 0&amp;-1\\\\ 1&amp;0\\\\\\end{bmatrix}\\)数学上定义，当两个矩阵相乘时，只有在第一个矩阵的列数（column）和第二个矩阵的行数（row）相同时才有意义。结果矩阵的每项元素等于第一个矩阵行元素与第二个矩阵列元素的乘积之和\\(\\begin{bmatrix} 1&amp;2\\\\ 3&amp;4\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} a&amp;c\\\\ b&amp;d\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} 1a+2b&amp;1c+2d\\\\ 3a+4b&amp;3c+4d\\\\\\end{bmatrix}\\) A矩阵 * B矩阵 = A矩阵的行 * B矩阵的列；只有当A矩阵列数 = B矩阵行数时，矩阵相乘才有效。因此结果矩阵的行数等于第一个矩阵的行，列数等于第二个矩阵的列相同。3D矩阵到目前为止，我们有了一个2x2阶矩阵，可以用这个矩阵来绕Z轴旋转一个2D点。但我们实际上使用的是3D坐标。若试图用这个矩阵乘法\\(\\begin{bmatrix} cosZ&amp;-sinZ\\\\ sinZ&amp;cosZ\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\\\end{bmatrix}\\)就是错误的，因为这两个矩阵的行与列的个数不匹配。为确保满足矩阵相乘，我们就需要填充这个第三维Z轴，先用0填充第一个矩阵第三行：\\(\\begin{bmatrix} cosZ&amp;-sinZ&amp;0\\\\ sinZ&amp;cosZ&amp;0\\\\ 0&amp;0&amp;0\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} xcosZ-ysinZ+0z\\\\ xsinZ+ycosZ+0z\\\\ 0x+0y+0z\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} xcosZ-ysinZ\\\\ xsinZ+ycosZ\\\\ 0\\\\\\end{bmatrix}\\)得到的结果中X轴和Y轴是正确的，但是Z轴结果总是为0。为了确保绕Z旋转而不改变Z轴的值，我们先插入一个数字1在旋转矩阵的右下角位置。简化理解，这个第三列值就是代表了Z轴：\\(\\begin{bmatrix} cosZ&amp;-sinZ&amp;0\\\\ sinZ&amp;cosZ&amp;0\\\\ 0&amp;0&amp;1\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} xcosZ-ysinZ\\\\ xsinZ+ycosZ\\\\ z\\\\\\end{bmatrix}\\) 由数学上定义，任何矩阵与单位矩阵相乘都等于本身，单位矩阵如同乘法中的1\\(\\begin{bmatrix} 1&amp;0&amp;0\\\\ 0&amp;1&amp;0\\\\ 0&amp;0&amp;1\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\\\end{bmatrix}\\)绕X轴和Y轴的旋转矩阵推导根据绕Z轴旋转的方式推理可以得出绕X轴和Y轴的旋转矩阵。以绕Y轴为例，首先，X轴是以\\(\\begin{bmatrix} 1\\\\ 0\\\\ 0\\\\\\end{bmatrix}\\)开始，经过逆时针旋转90°后以\\(\\begin{bmatrix} 0\\\\ 0\\\\ -1\\\\\\end{bmatrix}\\)结束。那么经过旋转后的X轴可以表示为\\(\\begin{bmatrix} cosY\\\\ 0\\\\ -sinY\\\\\\end{bmatrix}\\)而Z轴与X轴垂直，所以Z轴就是\\(\\begin{bmatrix} sinY\\\\ 0\\\\ cosY\\\\\\end{bmatrix}\\)而Y轴始终保持不变，最后绕Y轴的旋转矩阵：\\(\\begin{bmatrix} cosY&amp;0&amp;sinY\\\\ 0&amp;1&amp;0\\\\ -sinY&amp;0&amp;cosY\\\\\\end{bmatrix}\\)同理绕X轴的旋转矩阵， X轴不变：\\(\\begin{bmatrix} 1&amp;0&amp;0\\\\ 0&amp;cosX&amp;-sinX\\\\ 0&amp;sinX&amp;cosX\\\\\\end{bmatrix}\\)那么就此可以得出三个矩阵： 绕X轴旋转矩阵\\(\\begin{bmatrix} 1&amp;0&amp;0\\\\ 0&amp;cos\\alpha&amp;sin\\alpha\\\\ 0&amp;-sin\\alpha&amp;cos\\alpha\\\\\\end{bmatrix}\\) 绕Y轴旋转矩阵\\(\\begin{bmatrix} cos\\theta&amp;0&amp;-sin\\theta\\\\ 0&amp;1&amp;0\\\\ sin\\theta&amp;0&amp;cos\\theta\\\\\\end{bmatrix}\\) 绕Z轴旋转矩阵\\(\\begin{bmatrix} cos\\theta&amp;sin\\theta&amp;0\\\\ -sin\\theta&amp;cos\\theta&amp;0\\\\ 0&amp;0&amp;1\\\\\\end{bmatrix}\\)统一的旋转矩阵通过上文我们分别得到了单独绕某个轴的旋转矩阵，现在开始我们要组合起来使用。这里的同时旋转本质上也是分步进行的，先绕Z轴旋转，然后绕Y轴，最后是绕X轴。这里有两种算法：第一种：先计算坐标点绕Z旋转，得出的结果坐标再计算绕Y轴旋转，再得出的结果坐标计算绕X轴旋转，最后得到最终的旋转坐标。第二种：把每个旋转矩阵相乘得到一个最终的新的旋转矩阵，这将同时作用与三个轴旋转。首先计算Y乘Z，这个结果矩阵的第一项的值是$cosYcosZ−0sinZ−0sinY=cosYcosZ$，最终矩阵\\(\\begin{bmatrix} cosYcosZ&amp;-cosYsinZ&amp;sinY\\\\ sinZ&amp;cosZ&amp;0\\\\ -sinYcosZ&amp;sinYsinZ&amp;cosY\\\\\\end{bmatrix}\\)最后计算X × (Y × Z)得出最终矩阵：\\(\\begin{bmatrix} cosYcosZ&amp;-cosYsinZ&amp;sinY\\\\ cosXsinZ+sinXsinYcosZ&amp;cosXcosZ-sinXsinYsinZ&amp;-sinXcosY\\\\ sinXsinZ-cosXsinYcosZ&amp;sinXcosZ+cosXsinYsinZ&amp;cosXcosY\\\\\\end{bmatrix}\\)public Vector3 rotation;//每个分量表示角度public int rotDelta; private void Update(){ rotation = new Vector3(rotDelta, rotDelta, rotDelta);} public override Vector3 Apply(Vector3 point){ float radx = rotation.x * Mathf.Deg2Rad; float rady = rotation.y * Mathf.Deg2Rad; float radz = rotation.z * Mathf.Deg2Rad; float sinx = Mathf.Sin(radx); float cosx = Mathf.Cos(radx); float siny = Mathf.Sin(rady); float cosy = Mathf.Cos(rady); float sinz = Mathf.Sin(radz); float cosz = Mathf.Cos(radz); Vector3 xRot = new Vector3( cosy * cosz, cosx * sinz + sinx * siny * cosz, sinx * sinz - cosx * siny * cosz ); Vector3 yRot = new Vector3( -cosy * sinz, cosx * cosz - sinx * siny * sinz, sinx * cosz + cosx * siny * sinz ); Vector3 zRot = new Vector3( siny, -sinx * cosy, cosx * cosy ); return xRot * point.x + yRot * point.y + zRot * point.z;}矩阵变换实现一个矩阵完成缩放、旋转、位移的计算为了实现这个目标，所以借鉴3.4旋转矩阵组合的方式，先对缩放和位移组合，即位移 x 缩放。缩放，根据单位矩阵的性质，任何矩阵与单位矩阵相乘的结果都是本身。那么对单位矩阵进行缩放即可：\\(\\begin{bmatrix} 2&amp;0&amp;0\\\\ 0&amp;3&amp;0\\\\ 0&amp;0&amp;4\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} 2x\\\\ 3y\\\\ 4z\\\\\\end{bmatrix}\\)位移，不是对三个分量完全重新计算，而是在现有的坐标之上进行偏移。因此现在不能简单的重新表示为3x3阶矩阵，而是需要额外增加一列表示偏移。\\(\\begin{bmatrix} 1&amp;0&amp;0&amp;2\\\\ 0&amp;1&amp;0&amp;3\\\\ 0&amp;0&amp;1&amp;4\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} x+2\\\\ y+3\\\\ z+4\\\\\\end{bmatrix}\\)但是，又由于矩阵乘法规定，第一个矩阵的列数等于第二个矩阵的行数才有意义。上图就是错误的。所以我们需要给坐标矩阵增加第四个元素，偏移矩阵增加一行。当它们增加的这个分量进行矩阵相乘时，其结果为1(我们先保留下这个数字1，以备后续使用).那就变成了4x4阶矩阵样式和一个4D点。\\(\\begin{bmatrix} 1&amp;0&amp;0&amp;2\\\\ 0&amp;1&amp;0&amp;3\\\\ 0&amp;0&amp;1&amp;4\\\\ 0&amp;0&amp;0&amp;0\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ 1\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} x+2\\\\ y+3\\\\ z+4\\\\ 1\\\\\\end{bmatrix}\\)根据位移矩阵，所以我们要统一用4×4的变换矩阵。缩放和旋转矩阵会额外增加一行一列，其右下角是1。所有的点都带有一个第四维坐标分量，它总是1——其次坐标。其次坐标(Homogeneous Coordinates)不知道就问： 这个坐标的第四分量坐标是个啥？ 它有啥用啊？ 我们只知道上文提到位移时有用，那么缩放、旋转有用吗？ 当它的值为0，1，-1时会发生什么呢？有这样一个东西不叫坐标而叫向量，它可以被缩放和旋转，但不能被移动。向量描述了相对于某个点的偏移，具有方向和长度属性，没有位置属性。它\\(\\begin{bmatrix}x\\\\y\\\\z\\\\1\\\\\\end{bmatrix}\\)表示为一个点，而它\\(\\begin{bmatrix}x\\\\y\\\\z\\\\0\\\\\\end{bmatrix}\\)表示为一个向量。这样区分非常有用，因为我们可以使用相同的矩阵来变换一个点的位置、法线和切线。当第四个坐标值是0或1或其他数值时会发生什么？答案是什么也不会，准确的说是没有差异。这个坐标的术语叫做其次坐标，它的意思是空间中每个点都可以用一个无穷数量坐标集和来表示。而现在普遍做法的形式是使用1作为第四个坐标值，所有其他的数字都能通过使用整个集合乘以任意数来找到\\(\\begin{bmatrix}x\\\\y\\\\z\\\\1\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix}2x\\\\2y\\\\2z\\\\2\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix}3x\\\\3y\\\\3z\\\\3\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix}wx\\\\wy\\\\wz\\\\w\\\\\\end{bmatrix}\\)=$w$\\(\\begin{bmatrix}x\\\\y\\\\z\\\\1\\\\\\end{bmatrix}\\)当我们知道了一个其次坐标时，需要转为3D坐标，只需要把第四个坐标化为1，怎么做呢？没错，就是把每个坐标除以第四个坐标，然后再舍弃第四个坐标\\(\\begin{bmatrix}x\\\\y\\\\z\\\\w\\\\\\end{bmatrix}\\)=$1 \\over w$\\(\\begin{bmatrix}x\\\\y\\\\z\\\\w\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix}x \\over w\\\\y \\over w\\\\z \\over w\\\\1\\\\\\end{bmatrix}\\)$\\rightarrow$\\(\\begin{bmatrix}x \\over w\\\\y \\over w\\\\z \\over w\\\\\\end{bmatrix}\\)所以当第四个坐标为0时是不能做上面的除法的，因此当第四个坐标值为0时，表示为向量，这就是为什么它们像方向一样。使用矩阵我们能用Unity的Matrix4x4结构体来完成矩阵乘法。从现在开始，我们将用它来代替上面的3D旋转方法。在Transformation增加一个抽象只读属性以检索变换矩阵。public abstract Matrix4x4 Matrix { get; }Transformation组件的Apply方法不再需要设为抽象，它将获取到矩阵并执行乘法运算。public Vector3 Apply (Vector3 point) { return Matrix.MultiplyPoint(point);}注意这个Matrix4x4.MultiplyPoint需要一个3D坐标参数，坐标参数假定了第四个值为1.该方法会负责把得到的其次坐标转为3D坐标，若只想计算方向向量可以使用Matrix4x4.MultiplyVector.该方法会忽略第四个坐标。public Vector3 MultiplyPoint(Vector3 v){ Vector3 vector; vector.x = (((this.m00 * v.x) + (this.m01 * v.y)) + (this.m02 * v.z)) + this.m03; vector.y = (((this.m10 * v.x) + (this.m11 * v.y)) + (this.m12 * v.z)) + this.m13; vector.z = (((this.m20 * v.x) + (this.m21 * v.y)) + (this.m22 * v.z)) + this.m23; float num = (((this.m30 * v.x) + (this.m31 * v.y)) + (this.m32 * v.z)) + this.m33;//其次坐标 num = 1f / num; vector.x *= num;//转换计算 vector.y *= num;//转换计算 vector.z *= num;//转换计算 return vector;} public Vector3 MultiplyVector(Vector3 v){ Vector3 vector; vector.x = ((this.m00 * v.x) + (this.m01 * v.y)) + (this.m02 * v.z); vector.y = ((this.m10 * v.x) + (this.m11 * v.y)) + (this.m12 * v.z); vector.z = ((this.m20 * v.x) + (this.m21 * v.y)) + (this.m22 * v.z); return vector;}具体的Transformation类现在必须将其Apply()方法更改为Matrix属性。首先是PositionTransformation组件，Matrix4x4.SetRow接口能很简易地填充这个矩阵。public override Matrix4x4 Matrix { get { Matrix4x4 matrix = new Matrix4x4(); matrix.SetRow(0, new Vector4(1f, 0f, 0f, position.x)); matrix.SetRow(1, new Vector4(0f, 1f, 0f, position.y)); matrix.SetRow(2, new Vector4(0f, 0f, 1f, position.z)); matrix.SetRow(3, new Vector4(0f, 0f, 0f, 1f)); return matrix; }}其次是ScaleTransformation.public override Matrix4x4 Matrix { get { Matrix4x4 matrix = new Matrix4x4(); matrix.SetRow(0, new Vector4(scale.x, 0f, 0f, 0f)); matrix.SetRow(1, new Vector4(0f, scale.y, 0f, 0f)); matrix.SetRow(2, new Vector4(0f, 0f, scale.z, 0f)); matrix.SetRow(3, new Vector4(0f, 0f, 0f, 1f)); return matrix; }}最后是RotationTransformation, 它设置行与列就更简单了，把之前的方法改改就能用。public override Matrix4x4 Matrix { get { float radx = rotation.x * Mathf.Deg2Rad; float rady = rotation.y * Mathf.Deg2Rad; float radz = rotation.z * Mathf.Deg2Rad; float sinx = Mathf.Sin(radx); float cosx = Mathf.Cos(radx); float siny = Mathf.Sin(rady); float cosy = Mathf.Cos(rady); float sinz = Mathf.Sin(radz); float cosz = Mathf.Cos(radz); Matrix4x4 matrix = new Matrix4x4(); matrix.SetColumn(0, new Vector4( cosy * cosz, cosx * sinz + sinx * siny * cosz, sinx * sinz - cosx * siny * cosz, 0f )); matrix.SetColumn(1, new Vector4( -cosy * sinz, cosx * cosz - sinx * siny * sinz, sinx * cosz + cosx * siny * sinz, 0f )); matrix.SetColumn(2, new Vector4( siny, -sinx * cosy, cosx * cosy, )); matrix.SetColumn(3, new Vector4(0f,0f,0f,1f)); return matrix; }}合并矩阵现在我们把上述所有变换矩阵合并为一个矩阵。 为此先在UnityMatrices类增加一个矩阵类型字段transformation。我们将在Update函数每帧更新该变量值，该步骤为先获取到第一个Transformation组件的矩阵，并依次与其他矩阵相乘，需要确保这块正确的相乘顺序。private void Update() { UpdateTransformation(); for (int i =0 , z = 0; z &lt; generalCount; z++) { //... } }void UpdateTransformation() { GetComponents&lt;Transformation&gt;(transformations); if(transformations.Count &gt; 0) { transformation = transformations[0].Matrix; for (int i = 1; i &lt; transformations.Count; i++) { transformation = transformations[i].Matrix * transformation; } }}最后不再执行Apply方法，而改用矩阵乘法代替：Vector3 TransformPoint(int x, int y, int z){ Vector3 coordinates = CreateCoordinate(x, y, z); // for (int i = 0; i &lt; transformations.Count; i++) // { // coordinates = transformations[i].Apply(coordinates); // } return transformation.MultiplyPoint(coordinates);;}这个新方法是非常有效的，因为我们之前使用的方法是分别给每个点乘一个变换矩阵。而现在我们只需要一次创建一个统一的变换矩阵作用与所有点。Unity使用类似的方案将每个对象的变换层次结构简化为单个变换矩阵。在这个例子中，我们可以使它更有效。所有的变换矩阵都有一个相同的行——[0 0 0 1]。知道了这一点，我们可以忽略这一行，跳过所有0的计算和最后的除法转换。Matrix4x4.MultiplayPoint3x4方法就是这样做的。public Vector3 MultiplyPoint3x4(Vector3 v){ Vector3 vector; vector.x = (((this.m00 * v.x) + (this.m01 * v.y)) + (this.m02 * v.z)) + this.m03; vector.y = (((this.m10 * v.x) + (this.m11 * v.y)) + (this.m12 * v.z)) + this.m13; vector.z = (((this.m20 * v.x) + (this.m21 * v.y)) + (this.m22 * v.z)) + this.m23; return vector;}这个方法有时候有用，有时候不能用。因为有时我们需要的一些变换矩阵会改变这最后一行。到目前为止只有位移变换需要第四行。所以缩放、旋转使用Matrix4x4.MultiplayPoint3x4计算速度会更快。现在就把Apply()方法改为虚方法，再由旋转、缩放组件重写，代码就不贴了。3D到2D投影矩阵到目前为止，我们能够把一个点的坐标从一个3D空间变换到另一个3D空间。但是这些点又如何展示到2D空间呢？这肯定需要一个从3D到2D的变换矩阵。那么我们开始寻找这个矩阵吧！先构造一个新的继承自Transformation的实体变换组件作用于摄像机的投影，默认值为单位矩阵。public class CameraTransformation : Transformation{ public override Matrix4x4 Matrix { get { Matrix4x4 matrix = new Matrix4x4(); matrix.SetRow(0, new Vector4(1f, 0f, 0f, 0f)); matrix.SetRow(1, new Vector4(0f, 1f, 0f, 0f)); matrix.SetRow(2, new Vector4(0f, 0f, 1f, 0f)); matrix.SetRow(3, new Vector4(0f, 0f, 0f, 1f)); return matrix; } }}正交相机Orthographic Camera从3D变换到2D空间最直接粗暴的方式是丢弃一个维度数据。就好像把3维空间压缩到2维平面，这个平面就像一个画布，用来渲染屏幕。现在我们把Z轴丢弃，试试看会发生什么\\[\\begin{bmatrix} 1&amp;0&amp;0&amp;0\\\\ 0&amp;1&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;1\\\\\\end{bmatrix}\\]把代码修改为public class CameraTransformation : Transformation{ public override Matrix4x4 Matrix { get { Matrix4x4 matrix = new Matrix4x4(); matrix.SetRow(0, new Vector4(1f, 0f, 0f, 0f)); matrix.SetRow(1, new Vector4(0f, 1f, 0f, 0f)); matrix.SetRow(2, new Vector4(0f, 0f, 0f, 0f)); matrix.SetRow(3, new Vector4(0f, 0f, 0f, 1f)); return matrix; } }} 3d转换到2d. 实际上，这种粗暴的方法还蛮像那么回事，确实变成了2D了。其他的X、Y轴同理，就不演示了。这就是正交投影。不管相机如何缩放、旋转、位移，始终呈现的2D效果。移动相机的视觉效果和移动世界的相反方向是一致的，也就是3个变换组件的变量与摄像机的缩放、旋转、位移变量是互为正负关系。透视相机Perspective Camera正交相机不能模拟3D世界就很尴尬。所以我们需要一个透视相机，由于视角的原因，呈现一个原小近大的视觉。那么基于此，我们可以根据点到摄像机的距离重建这个视觉效果。以Z轴为例，把单位矩阵代表Z轴的列元素全部置0，再把单位矩阵最后一行改为[0,0,1,0]，这步改变将确保结果坐标的第四个值等于Z坐标，最后所有坐标都除以Z\\(\\begin{bmatrix} 1&amp;0&amp;0&amp;0\\\\ 0&amp;1&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;1&amp;0\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ 1\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} x\\\\ y\\\\ 0\\\\ z\\\\\\end{bmatrix}\\)$\\rightarrow$\\(\\begin{bmatrix} x \\over z\\\\ y \\over z\\\\ 0\\\\\\end{bmatrix}\\)public override Matrix4x4 Matrix { get { Matrix4x4 matrix = new Matrix4x4(); matrix.SetRow(0, new Vector4(1f, 0f, 0f, 0f)); matrix.SetRow(1, new Vector4(0f, 1f, 0f, 0f)); matrix.SetRow(2, new Vector4(0f, 0f, 0f, 0f)); matrix.SetRow(3, new Vector4(0f, 0f, 1f, 0f)); return matrix; }}与正交投影最大的不同是这些点不会直接移向到平面，而是他们会移向摄像机的位置，当然这只对位于摄像机前面的点有效，而在摄像机后面的点就不会正确的投影。先确保所有点都能位于摄像机的前方，把摄像机的Unity.Transform组件Position.Z值调好，保证所有点都先可见。 透视投影. 设置一个点到平面的投影距离，它也会影响投影视觉效果。它就像相机的焦距，值越大视野就越小。现在我们先定义一个变量focalLength值默认为1，这能产生90°的视野。public float focalLength = 1f;当这个focalLength值越大就像相机在进行聚焦，这有效地增加了所有点的比例(想象一下相机变焦)。当我们压缩Z轴时，是不必进行缩放的\\(\\begin{bmatrix} fl&amp;0&amp;0&amp;0\\\\ 0&amp;fl&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;1&amp;0\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ 1\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} xfl\\\\ yfl\\\\ 0\\\\ z\\\\\\end{bmatrix}\\)$\\rightarrow$\\(\\begin{bmatrix} xfl \\over z\\\\ yfl \\over z\\\\ 0\\\\\\end{bmatrix}\\)public override Matrix4x4 Matrix { get { Matrix4x4 matrix = new Matrix4x4(); matrix.SetRow(0, new Vector4(focalLength, 0f, 0f, 0f)); matrix.SetRow(1, new Vector4(0f, focalLength, 0f, 0f)); matrix.SetRow(2, new Vector4(0f, 0f, 0f, 0f)); matrix.SetRow(3, new Vector4(0f, 0f, 1f, 0f)); return matrix; }}现在有了一个简单的透视相机，如果要完全模拟Unity的透视相机，我们还必须处理近平面和远平面。这将需要处理投影到一个立方体而不是一个平面，因此需要保留深度信息。然后还有视野裁切方面的问题。此外，Unity的摄像头是在负Z方向上拍摄的，这需要对一些数字进行求负。矩阵不可怕。" }, { "title": "CG函数标准库", "url": "/posts/CG-function-library/", "categories": "Unity3D, Shader", "tags": "CG", "date": "2017-10-03 20:17:00 +0800", "snippet": "数学函数（Mathematical Functions）下表中列举了 Cg 标准函数库中所有的数学函数， 这些数学函数用于执行数学上常用计算，包括：三角函数、幂函数、园函数、向量和矩阵的操作函数。这些函数都被重载，以支持标量数据和不同长度的向量作为输入参数。 函数 功能 abs(x) 返回输入参数的绝对值 all(x) 如果输入参数均不为 0，则返回 ture；否则返回 flase。 &amp;&amp;运算 any(x) 输入参数只要有其中一个不为 0，则返回true。 clamp(x,a,b) 如果 x 值小于 a，则返回 a；如果 x 值大于 b，返回 b；否则返回 x degrees(x) 输入参数为弧度值(radians)， 函数将其转换为角度值(degrees) determinant(m) 计算矩阵的行列式因子 cross(A,B) 返回两个三元向量的叉积(cross product)。注意，输入参数必须是三元向量！ dot(A,B) 返回 A 和 B 的点积(dot product)。参数 A 和 B可以是标量，也可以是向量（输入参数方面，点积和叉积函数有很大不同） exp(x) 计算 ex 的值， e= 2.71828182845904523536 exp2(x) 计算 2x 的值 floor(x) 向下取整。例如 floor(float(1.3))返回的值为 1.0；但是floor(float(-1.3))返回的值为-2.0。该函数与 ceil(x)函数相对应 ceil(x) 对输入参数向上取整。 例如：ceil(float(1.3)) ，返回值为2.0 fmod(x,y) 返回 x/y 的余数。如果 y 为 0， 结果不可预料 frac(x) 返回标量或每个向量分量的小数部分 frexp(x, out exp) 将浮点数 x 分解为尾数和指数，即x = m* 2^exp， 返回 m， 并将指数存入exp 中；如果 x 为 0，则尾数和指数都返回 0 isfinite(x) 判断标量或者向量中的每个数据是否是有限数，如果是返回true；否则返回 false;无限的或者非数据(not-a-number NaN) isinf(x) 判断标量或者向量中的每个数据是否是无限，如果是返回 true；否则返回 false isnan(x) 判断标量或者向量中的每个数据是否是非数据(not-a-number NaN)，如果是返回 true；否则返回 false ldexp(x, n) 计算$x*2n$的值 lerp(a, b, f) 计算$ a+(b-a)f $ 或者$ (1-f)a+f*b $ 的值。即在下限 a 和上限 b 之间进行插值， f 表示权值。注意，如果 a 和 b 是向量，则权值f必须是标量或者等长的向量 lit(N•dot•L,N•dot•H, m) Blinn–Phong反射公式,N表示法向量，L表示入射光向量；H表示半角向量；m表示高光系数；N • L代表散射光的贡献，如果其值 &lt; 0 ，则为 0。Z 位代表镜面光的贡献，如果 N • L &lt; 0 或者N•H &lt; 0 ，则位 0；否则为 (N • L)m。函数计算环境光、散射光、镜面光的贡献，返回的 4 元向量 log(x) 计算 ln( x) 的值， x 必须大于 0 log2(x) 计算 log2(x) 的值， x 必须大于 0 log10(x) 计算 log10 (x) 的值， x 必须大于 0 max(a, b) 比较两个标量或等长向量元素，返回最大值 min(a,b) 比较两个标量或等长向量元素，返回最小值 modf(x, out ip) 将浮点数num分解成整数部分和小数部分，返回小数部分，将整数部分存入ip。不常用 mul(M, N) 计算两个矩阵相乘，如果 M 为 AxB 阶矩阵，N 为 BxC 阶矩阵，则返回AxC 阶矩阵。下面两个函数为其重载函数 mul(M, v) 计算矩阵和向量相乘 mul(v, M) 计算向量和矩阵相乘 noise(x) 噪声函数，返回值始终在 0， 1 之间；对于同样的输入，始终返回相同的值（也就是说，并不是真正意义上的随机噪声） pow(x, y) $ x^y $ radians(x) 函数将角度值转换为弧度值 round(x) 即四舍五入 sqrt(x) 求 x 的平方根， x ， x 必须大于 0 rsqrt(x) 平方根倒数$ y= \\frac{1}{\\sqrt{x}} $，x 必须大于 0 saturate(x) 如果 x 小于 0，返回 0；如果 x 大于 1，返回1；否则，返回 x sign(x) 如果 x 大于 0，返回 1；如果 x 小于 0，返回01；否则返回 0 sin(x) 输入参数为弧度，计算正弦值，返回值范围为[-1,1] cos(x) 返回弧度 x 的余弦值。返回值范围为[-1,1] tan(x) 输入参数为弧度，计算正切值 acos(x) 反余切函数，输入参数范围为[-1,1]，返回[0,π ]区间的角度值 asin(x) 反正弦函数,输入参数取值区间为[-1,1]，返回角度值范围为 atan(x) 反正切函数，返回角度值范围为 atan2(y,x) 计算$\\frac{y}{x}$ 的反正切值。实际上和 atan(x)函数功能完全一样，至少输入参数不同。 atan(x)= atan2(x, float(1)) sincos(float x,out s, out c) 该函数是同时计算 x 的 sin 值和 cos 值，其中s=sin(x)，c=cos(x)。该函数用于”同时需要计算 sin 值和 cos 值的情况”，比分别运算要快很多 sinh(x) 计算双曲正弦（hyperbolic sine）值 cosh(x) 双曲余弦（hyperbolic cosine）函数，计算 x的双曲余弦值 tanh(x) 计算双曲正切值 smoothstep(min,max, x) 值 x 位于 min、 max 区间中。如果x=min，返回 0；如果x=max，返回 1；如果 x 在两者之间，按照下列公式返回数据 step(a, x) 如果 x&lt;a，返回 0；否则，返回 1 transpose(M) M 为矩阵，计算其转置矩阵 几何函数（Geometric Functions）如下表所示，几何函数用于执行和解析几何相关的计算，例如根据入射光向量和顶点法向量，求取反射光和折射光方向向量。 Cg 语言标准函数库中有3个几何函数会经常被使用到分别是 normalize 函数，对向量进行归一化 reflect函数，计算反射光方向向量 refract 函数，计算折射光方向向量 着色程序中的向量最好进行归一化之后再使用，否则会出现难以预料的错误. reflect 函数和 refract 函数都存在以”入射光方向向量”作为输入参数，注意这两个函数中使用的入射光方向向量，是从外指向几何顶点的；平时我们在着色程序中或者在课本上都是将入射光方向向量作为从顶点出发. 函数 功能 distance( pt1, pt2) 两点之间的欧几里德距离（Euclidean distance） faceforward(N,I,Ng) 如果 Ng • I &lt; 0 ，返回 N；否则返回-N length(v) 返回一个向量的模，即 sqrt(dot(v,v)) normalize( v) 归一化向量 reflect(I, N) 根据入射光方向向量 I，和顶点法向量N，计算反射光方向向量。其中 I 和 N必须被归一化，需要非常注意的是，这个 I 是指向顶点；函数只对三元向量有效。 refract(I,N,eta) 计算折射向量， I 为入射光线， N 为法向量， eta 为折射系数；其中 I 和 N 必须被归一化， 如果 I 和 N 之间的夹角太大，则返回（0， 0， 0），也就是没有折射光线； I 是指向顶点的；函数只对三元向量有效 纹理映射函数（texture Map Functions）下表提供 Cg 标准函数库中的纹理映射函数。这些函数被 ps_2_0、 ps_2_x、arbfp1、 fp30 和fp40 等 profiles 完全支持（fully supported）。所有的这些函数返回四元向量值。 函数 功能 tex1D (sampler1D tex, float s)一维纹理查询 tex1D(sampler1D tex, float s, float dsdx, float dsdy) 使用导数值（derivatives）查询一维纹理 tex1D(sampler1D tex, float2 sz) 一维纹理查询，并进行深度值比较 tex1D(sampler1D tex, float2 sz, float dsdx,float dsdy) 使用导数值（derivatives）查询一维纹理， 并进行深度值比较 tex1Dproj(sampler1D tex, float2 sq) 一维投影纹理查询 tex1Dproj(sampler1D tex, float3 szq) 一维投影纹理查询，并比较深度值 tex2D(sampler2D tex, float2 s) 二维纹理查询—采样 tex2D(sampler2D tex, float2 s, float2 dsdx, float2 dsdy) 使用导数值（derivatives）查询二维纹理 tex2D(sampler2D tex, float3 sz) 二维纹理查询，并进行深度值比较 tex2D(sampler2D tex, float3 sz, float2 dsdx,float2 dsdy) 使用导数值（derivatives）查询二维纹理，并进行深度值比较 tex2Dproj(sampler2D tex, float3 sq) 二维投影纹理查询 tex2Dproj(sampler2D tex, float4 szq) 二维投影纹理查询，并进行深度值比较 texRECT(samplerRECT tex, float2 s) 二维非投影矩形纹理查询（OpenGL独有） texRECT (samplerRECT tex, float2 s, float2 dsdx, float2 dsdy) 二维非投影使用导数的矩形纹理查询（OpenGL独有） texRECT (samplerRECT tex, float3 sz) 二维非投影深度比较矩形纹理查询（OpenGL独有） texRECT (samplerRECT tex, float3 sz, float2 dsdx,float2 dsdy) 二维非投影深度比较并使用导数的矩形纹理查询（OpenGL独有） texRECT proj(samplerRECT tex, float3 sq) 二维投影矩形纹理查询（OpenGL独有） texRECT proj(samplerRECT tex, float3 szq) 二维投影矩形纹理查询（OpenGL独有） tex3D(sampler3D tex, float s) 三维纹理查询 tex3D(sampler3D tex, float3 s, float3 dsdx, float3 dsdy) 结合导数值（derivatives）查询三维纹理 tex3Dproj(sampler3D tex, float4 szq) 查询三维投影纹理，并进行深度值比较 texCUBE(samplerCUBE tex, float3 s) 查询立方体纹理 texCUBE(samplerCUBE tex, float3 s, float3 dsdx, float3 dsdy) 结合导数值（derivatives）查询立方体纹理 texCUBEproj(samplerCUBE tex, float4 sq) 查询投影立方体纹理 偏导函数（Derivative Functions） 函数 功能 ddx(a) 参数 a 对应一个像素位置， 返回该像素值在 X 轴上的偏导数 ddy(a) 参数 a 对应一个像素位置， 返回该像素值在 X 轴上的偏导数 调试函数（Debugging Function） 函数 功能 void debug(float4 x) 如果在编译时设置了DEBUG，片段着色程序中调用该函数可以将值x作为COLOR 语义的最终输出；否则该函数什么也不做 " }, { "title": "Markdown语法集合速查表", "url": "/posts/markdown-cheat-sheet/", "categories": "随笔, Markdown", "tags": "Markdown", "date": "2016-03-17 11:17:00 +0800", "snippet": "Thanks for visiting The Markdown Guide!This Markdown cheat sheet provides a quick overview of all the Markdown syntax elements. It can’t cover every edge case, so if you need more information about any of these elements, refer to the reference guides for basic syntax and extended syntax.Basic SyntaxThese are the elements outlined in John Gruber’s original design document. All Markdown applications support these elements.HeadingH1H2H3Boldbold textItalicitalicized textBlockquote blockquoteOrdered List First item Second item Third itemUnordered List First item Second item Third itemCodecodeHorizontal RuleLinkMarkdown GuideImage&lt;center class=\"half\"&gt; &lt;img src=\"\" width=\"15%\" /&gt; **&lt;/center&gt; *居中描述*Extended SyntaxThese elements extend the basic syntax by adding additional features. Not all Markdown applications support these elements.Table Syntax Description Header Title Paragraph Text :— 或者| 代表左对齐:–: 代表居中对齐—: 代表右对齐Fenced Code Block{ \"firstName\": \"John\", \"lastName\": \"Smith\", \"age\": 25}FootnoteHere’s a sentence with a footnote. 1Heading IDMy Great HeadingDefinition List term definitionStrikethroughThe world is flat.Task List Write the press release Update the website Contact the mediaEmojiThat is so funny! :joy:(See also Copying and Pasting Emoji)HighlightI need to highlight these ==very important words==.SubscriptH~2~OSuperscriptX^2^MathematicsThe mathematics powered by MathJax:数学公式符号两个$$包围，公式会另起一行$$ a + b = c$$例如：\\(a + b = c\\)例如：\\[{\\sum_{n=1}^\\infty} {1 \\over n^2} = \\frac{\\pi^2}{6}\\]一个$包围，公式与文字同一行When $a \\ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are例如：When $a \\ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are\\[x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}\\] 两个$$ xxx $$包围住，数学公式会换行. 一个$ xxx $包围住，数学公式不换行，如When $a \\ne 0$.&gt; 一个红色提示{: .prompt-danger } 一个红色提示- gem \"jekyll-theme-chirpy\", \"~&gt; 3.2\", \"&gt;= 3.2.1\"+ gem \"jekyll-theme-chirpy\", \"~&gt; 3.3\", \"&gt;= 3.3.0\"矩阵公式1、矩阵 数学公式放在 $$ 之间 起始标记 \\begin{matrix}，结束标记 \\end{matrix} 每一行末尾标记 \\，行间元素之间用 &amp; 分隔$$\\begin{matrix}0&amp;1&amp;1\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\\\end{matrix}$$表现如下：\\(\\begin{matrix}0&amp;1&amp;1\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\\\end{matrix}\\)2、矩阵边框 在起始、结束标记用下列词替换 matrix pmatrix：小括号边框 bmatrix：中括号边框$\\begin{bmatrix}0&amp;1&amp;11&amp;1&amp;01&amp;0&amp;1\\end{bmatrix}$ Bmatrix：大括号边框 vmatrix：单竖线边框 Vmatrix：双竖线边框表现如下：\\(\\begin{pmatrix}0&amp;1&amp;1\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\\\end{pmatrix}\\)\\[\\begin{bmatrix}0&amp;1&amp;1\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\\\end{bmatrix}\\]\\[\\begin{Bmatrix}0&amp;1&amp;1\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\\\end{Bmatrix}\\]\\[\\begin{vmatrix}0&amp;1&amp;1\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\\\end{vmatrix}\\]\\[\\begin{Vmatrix}0&amp;1&amp;1\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\\\end{Vmatrix}\\]3、省略元素 横省略号：\\cdots 竖省略号：\\vdots 斜省略号：\\ddots$$\\begin{bmatrix}{a_{11}}&amp;{a_{12}}&amp;{\\cdots}&amp;{a_{1n}}\\\\{a_{21}}&amp;{a_{22}}&amp;{\\cdots}&amp;{a_{2n}}\\\\{\\vdots}&amp;{\\vdots}&amp;{\\ddots}&amp;{\\vdots}\\\\{a_{m1}}&amp;{a_{m2}}&amp;{\\cdots}&amp;{a_{mn}}\\\\\\end{bmatrix}$$表现如下：\\(\\begin{bmatrix}{a_{11}}&amp;{a_{12}}&amp;{\\cdots}&amp;{a_{1n}}\\\\{a_{21}}&amp;{a_{22}}&amp;{\\cdots}&amp;{a_{2n}}\\\\{\\vdots}&amp;{\\vdots}&amp;{\\ddots}&amp;{\\vdots}\\\\{a_{m1}}&amp;{a_{m2}}&amp;{\\cdots}&amp;{a_{mn}}\\\\\\end{bmatrix}\\)4、阵列 需要array环境：起始、结束处以{array}声明 对齐方式：在{array}后以{}逐行统一声明 左对齐：l；居中：c；右对齐：r 竖直线：在声明对齐方式时，插入 建立竖直线 插入水平线：\\hline$$\\begin{array}{c|lll}{↓}&amp;{a}&amp;{b}&amp;{c}\\\\\\hline{R_1}&amp;{c}&amp;{b}&amp;{a}\\\\{R_2}&amp;{b}&amp;{c}&amp;{c}\\\\\\end{array}$$表现如下：\\[\\begin{array}{c|lll}{↓}&amp;{a}&amp;{b}&amp;{c}\\\\\\hline{R_1}&amp;{c}&amp;{b}&amp;{a}\\\\{R_2}&amp;{b}&amp;{c}&amp;{c}\\\\\\end{array}\\]5、方程组 需要cases环境：起始、结束处以{cases}声明 $$\\begin{cases}a_1x+b_1y+c_1z=d_1\\\\a_2x+b_2y+c_2z=d_2\\\\a_3x+b_3y+c_3z=d_3\\\\\\end{cases}$$ \\(\\begin{cases}a_1x+b_1y+c_1z=d_1\\\\a_2x+b_2y+c_2z=d_2\\\\a_3x+b_3y+c_3z=d_3\\\\\\end{cases}\\) 30^\\circ\t30∘” role=”presentation”&gt;30∘30∘\t\\bot\t⊥” role=”presentation”&gt;⊥⊥\t\\angle A\t∠A” role=”presentation”&gt;∠A∠A\\sin\tsin” role=”presentation”&gt;sinsin\t\\cos\tcos” role=”presentation”&gt;coscos\t\\tan\ttan” role=”presentation”&gt;tantan\\csc\tcsc” r箭头 短箭头形状 MarkDown $\\uparrow$ $\\uparrow$ $\\Uparrow$ $\\Uparrow$ $\\downarrow$ $\\downarrow$ $\\Downarrow$ $\\Downarrow$ $\\leftarrow$ $\\leftarrow$ $\\Leftarrow$ $\\Leftarrow$ $\\rightarrow$ $\\rightarrow$ $\\Rightarrow$ $\\Rightarrow$ $\\updownarrow$ $\\updownarrow$ $\\Updownarrow$ $\\Updownarrow$ $\\leftrightarrow$ $\\leftrightarrow$ $\\Leftrightarrow$ $\\Leftrightarrow$ 长箭头形状 MarkDown $\\longleftarrow$ $\\longleftarrow$ $\\Longleftarrow$ $\\Longleftarrow$ $\\longrightarrow$ $\\longrightarrow$ $\\Longrightarrow$ $\\Longrightarrow$ $\\longleftrightarrow$ $\\longleftrightarrow$ $\\Longleftrightarrow$ $\\Longleftrightarrow$ 给字体加颜色&lt;font color=red&gt;想变成红色的内容&lt;/font&gt;想变成红色的内容加边框&lt;kbd&gt;2 3&lt;/kbd&gt;2 3创建脚注格式类似这样 2。查看代码 &nbsp;这里写需要被折叠的代码&nbsp; This is the footnote. &#8617; 菜鸟教程 – 学的不仅是技术，更是梦想！！！ &#8617; " } ]
