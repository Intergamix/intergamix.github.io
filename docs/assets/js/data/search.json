[ { "title": "Unity光照第一讲(翻译四)", "url": "/posts/Unity-First-light/", "categories": "翻译, Shader", "tags": "Unity3D, Shader", "date": "2018-01-03 08:00:00 +0800", "snippet": "本篇摘要 将法线从对象空间转换为世界空间。 使用方向光。 计算光的漫反射和镜面反射。 调整光的能耗强度。 应用金属工作流程。 学习使用Unity的PBS算法。The First LightIt is time to shine a light on things.NormalsWe can see things, because our eyes can detect electromagnetic radiation. Individual quanta of light are known as photons. We can see a part of the electromagnetic spectrum, which is know to us as visible light. The rest of the spectrum is invisible to us.What’s the entire electromagnetic spectrum?The spectrum is split into spectral bands. From low to high frequency, these are known as radio waves, microwaves, infrared, visible light, ultraviolet, X-rays, and gamma rays.A light source emits light. Some of this light hits objects. Some of this light bounces off the object. If that light then ends up hitting our eyes – or the camera lens – then we see the object.To work this all out, we have to know our object’s surface. We already know its position, but not its orientation. For that, we need the surface normal vectors.Using Mesh NormalsDuplicate our first shader, and use that as our first lighting shader. Create a material with this shader and assign it to some cubes and spheres in the scene. Give the objects different rotations and scales, some non-uniform, to get a varied scene.Shader \"Custom/My First Lighting Shader\" {\t…}Some cubes and spheres.Unity’s cube and sphere meshes contain vertex normals. We can grab them and pass them straight to the fragment shader.\t\t\tstruct VertexData {\t\t\t\tfloat4 position : POSITION;\t\t\t\tfloat3 normal : NORMAL;\t\t\t\tfloat2 uv : TEXCOORD0;\t\t\t};\t\t\t\t\t\tstruct Interpolators {\t\t\t\tfloat4 position : SV\\_POSITION;\t\t\t\tfloat2 uv : TEXCOORD0;\t\t\t\tfloat3 normal : TEXCOORD1;\t\t\t};\t\t\tInterpolators MyVertexProgram (VertexData v) {\t\t\t\tInterpolators i;\t\t\t\ti.uv = TRANSFORM\\_TEX(v.uv, \\_MainTex);\t\t\t\ti.position = mul(UNITY\\_MATRIX\\_MVP, v.position);\t\t\t\ti.normal = v.normal;\t\t\t\treturn i;\t\t\t}Now we can visualize the normals in our shader.\t\t\tfloat4 MyFragmentProgram (Interpolators i) : SV\\_TARGET {\t\t\t\treturn float4(i.normal \\* 0.5 + 0.5, 1);\t\t\t}Normal vectors as colors.These are the raw normals, directly from the mesh. The faces of the cubes appear flat, because each face is a separate quad with four vertices. The normals of these vertices all point in the same direction. In contrast, the vertex normals of the spheres all point in different directions, resulting in a smooth interpolation.Dynamic BatchingThere is something strange going on with the cube normals. We’d expect each cube to show the same colors, but this is not the case. Even weirder, the cubes can change color, depending on how we look at them.Color-changing cubes.This is caused by dynamic batching. Unity dynamically merges small meshes together, to reduce draw calls. The meshes of the spheres are too large for this, so they aren’t affected. But the cubes are fair game.To merge meshes, they have to be converted from their local space to world space. Whether and how objects are batched depends, among other things, on how they are sorted for rendering. As this conversion affects the normals as well, this is why we see the colors change.If you want to, you can switch dynamic batching off via the player settings.Batching settings.Besides dynamic batching, Unity can also do static batching. This works differently for static geometry, but also involves a conversion to world space. It happens at build time.Normals, without dynamic batching.While you need to be aware of dynamic batching, it’s nothing to be worried about. In fact, we have to do the same thing for our normals. So you can leave it enabled.Normals in World SpaceExcept for dynamically batched objects, all our normals are in object space. But we have to know the surface orientation in world space. So we have to transform the normals from object to world space. We need the object’s transformation matrix for that.Unity collapses an object’s entire transformation hierarchy into a single transformation matrix, just like we did in part 1. We could write this as `O = T_1 T_2 T_3 …` where `T` are the individual transformations and `O` is the combined transformation. This matrix is known as the object-to-world matrix.Unity makes this matrix available in shaders via a float4x4 unity_ObjectToWorld variable, which is defined in UnityShaderVariables. Multiply this matrix with the normal in the vertex shader to transform it to world space. And because it’s a direction, repositioning should be ignored. So the fourth homogeneous coordinate must be zero.\t\t\tInterpolators MyVertexProgram (VertexData v) {\t\t\t\tInterpolators i;\t\t\t\ti.position = mul(UNITY\\_MATRIX\\_MVP, v.position);\t\t\t\ti.normal = mul(unity\\_ObjectToWorld, float4(v.normal, 0));\t\t\t\ti.uv = TRANSFORM\\_TEX(v.uv, \\_MainTex);\t\t\t\treturn i;\t\t\t}Alternatively, we can multiply with only the 3 by 3 part of the matrix. The compiled code ends up the same, because the compilers will eliminate everything that gets multiplied with the constant zero.\t\t\t\ti.normal = mul((float3x3)unity\\_ObjectToWorld, v.normal);Going from object to world space.The normals are now in world space, but some appear brighter than others. That’s because they got scaled as well. So we have to normalize them after the transformation.\t\t\ti.normal = mul(unity\\_ObjectToWorld, float4(v.normal, 0));\t\t\ti.normal = normalize(i.normal);Normalized normals.While we have normalized vectors again, they look weird for objects that don’t have a uniform scale. That’s because when a surface gets stretched in one dimension, its normals don’t stretch in the same way.Scaling X, both vertices and normals by ½.When the scale is not uniform, it should be inverted for the normals. That way the normals will match the shape of the deformed surface, after they’ve been normalized again. And it doesn’t make a difference for uniform scales.Scaling X, vertices by ½ and normals by 2.So we have to invert the scale, but the rotation should remain the same. How can we do this?We described an object’s transformation matrix as `O = T_1 T_2 T_3 …` but we can be more specific than that. We know that each step in the hierarchy combines a scaling, rotating, and positioning. So each `T` can be decomposed into `S R P`.This means that `O = S_1 R_1 P_1 S_2 R_2 P_2 S_3 R_3 P_3 …` but let’s just say `O = S_1 R_1 P_1 S_2 R_2 P_2` to keep it short.Because normals are direction vectors, we don’t care about repositioning. So we can shorten it further to `O = S_1 R_1 S_2 R_2` and we only have to consider 3 by 3 matrices.We want to invert the scaling, but keep the rotations the same. So we want a new matrix `N = S_1^-1 R_1 S_2^-1 R_2`.How do inverse matrices work?The inverse of a matrix `M` is written as `M^-1`. It is a matrix that will undo the operation of another matrix when they are multiplied. Each is the inverse of the other. So `M M^-1 = M^-1 M = I`.To undo a sequence of steps, you have to perform the inverse steps in reverse order. The mnemonic for this involves socks and shoes. This means that `(A B)^-1 = B^-1 A^-1`.In the case of a single number `x`, its inverse is simply `1/x`, because `x/x = 1`. This also demonstrates that zero has no inverse. Neither does every matrix have an inverse.We’re working with scaling, rotating, and repositioning matrices. As long as we’re not scaling by zero, all these matrices can be inverted.The inverse of a reposition matrix is made by simply negating the XYZ offset in it’s fourth column.`[[1,0,0,x],[0,1,0,y],[0,0,1,z],[0,0,0,1]]^-1 = [[1,0,0,-x],[0,1,0,-y],[0,0,1,-z],[0,0,0,1]]`The inverse of a scaling matrix is made by inverting its diagonal. We only need to consider the 3 by 3 matrix.`[[x,0,0],[0,y,0],[0,0,z]]^-1 = [[1/x,0,0],[0,1/y,0],[0,0,1/z]]`Rotation matrices can be considered one axis at a time, for example around the Z axis. A rotation by `z` radians can be undone by simply rotating by `-z` radians. When you study the sine and cosine waves, you’ll notice that `sin (-z) = -sin z` and `cos (-z) = cos z`. This makes the inverse matrix simple.`[[cos z, -sin z, 0],[sin z, cos z, 0],[0,0,1]]^-1 = [[cos z, sin z, 0],[-sin z, cos z, 0],[0,0,1]]`Notice that the rotation inverse is the same as the original matrix flipped across its main diagonal. Only the signs of the sine components changed.Besides the object-to-world matrix, Unity also provides an object’s world-to-object matrix. These matrices are indeed inverses of each other. So we also have access to `O^-1 = R_2^-1 S_2^-1 R_1^-1 S_1^-1`.That gives us the inverse scaling that we need, but also gives us the inverse rotations and a reversed transformation order. Fortunately, we can remove those unwanted effects by transposing the matrix. Then we get `(O^-1)^T = N`.What is the transpose of a matrix?The transpose of a matrix `M` is written as `M^T`. You transpose a matrix by flipping its main diagonal. So its rows become columns, and its columns become rows. Note that this means that the diagonal itself is unchanged.`[[1,2,3],[4,5,6],[7,8,9]]^T = [[1,4,7],[2,5,8],[3,6,9]]`Like inversion, transposing a sequence of matrix multiplications reverses its order. `(A B)^T = B^T A^T`. This makes sense when working with matrices that aren’t square, otherwise you could end up with invalid multiplications. But it’s true in general, and you can look up the proof for it.Of course flipping twice gets you back where you started. So `(M^T)^T = M`.Why does transposing produce the correct matrix?First, notice that `R^-1 = R^T`, as observed above.This leads to `O^-1 = R_2^-1 S_2^-1 R_1^-1 S_1^-1 = R_2^T S_2^-1 R_1^T S_1^-1`.Now let’s transpose `(O^-1)^T = (S_1^-1)^T (R_1^T)^T (S_2^1)^T (R_2^T)^T = (S_1^-1)^T R_1 (S_2^-1)^T R_2`.Next, notice that `S^T = S`, because these matrices have zeros everywhere, except along their main diagonal.This leads to `(O^-1)^T = S_1^-1 R_1 S_2^-1 R_2 = N`.So let’s transpose the world-to-object matrix and multiply that with the vertex normal.\t\t\t\ti.normal = mul(\t\t\t\t\ttranspose((float3x3)unity\\_WorldToObject),\t\t\t\t\tv.normal\t\t\t\t);\t\t\t\ti.normal = normalize(i.normal);Correct world-space normals.Actually, UnityCG contains a handy UnityObjectToWorldNormal function that does exactly this. So we can use that function. It also does it with explicit multiplications, instead of using transpose. That should result in better compiled code.\t\t\tInterpolators MyVertexProgram (VertexData v) {\t\t\t\tInterpolators i;\t\t\t\ti.position = mul(UNITY\\_MATRIX\\_MVP, v.position);\t\t\t\ti.normal = UnityObjectToWorldNormal(v.normal);\t\t\t\ti.uv = TRANSFORM\\_TEX(v.uv, \\_MainTex);\t\t\t\treturn i;\t\t\t}What does UnityObjectToWorldNormal look like?Here it is. The inline keyword doesn’t do anything, in case you’re wondering.// Transforms normal from object to world spaceinline float3 UnityObjectToWorldNormal( in float3 norm ) {\t// Multiply by transposed inverse matrix,\t// actually using transpose() generates badly optimized code\treturn normalize(\t\tunity\\_WorldToObject\\[0\\].xyz \\* norm.x +\t\tunity\\_WorldToObject\\[1\\].xyz \\* norm.y +\t\tunity\\_WorldToObject\\[2\\].xyz \\* norm.z\t);}RenormalizingAfter producing correct normals in the vertex program, they are passed through the interpolator. Unfortunately, linearly interpolating between different unit-length vectors does not result in another unit-length vector. It will be shorter.So we have to normalize the normals again in the fragment shader.\t\t\tfloat4 MyFragmentProgram (Interpolators i) : SV\\_TARGET {\t\t\t\ti.normal = normalize(i.normal);\t\t\t\treturn float4(i.normal \\* 0.5 + 0.5, 1);\t\t\t}Renormalized normals.While this produces better results, the error is usually very small. You could decide to not renormalize in the fragment shader, if you value performance more. This is a common optimization for mobile devices.Exaggerated error.unitypackageDiffuse ShadingWe see objects that aren’t themselves light sources, because they reflect light. There are different ways in which this reflection can happen. Let’s first consider diffuse reflection.Diffuse reflection happens because a ray of light doesn’t just bounce off a surface. Instead, it penetrates the surface, bounces around for a bit, gets split up a few times, until it exits the surface again. In reality, the interaction between photons and atoms is more complex than that, but we don’t need to know the real-world physics in that much detail.How much light is diffusely refected off a surface depends on the angle at which the light ray hits it. Most light is reflected when the surface is hit head-on, at a 0° angle. As this angle increases, the reflections will decrease. At 90°, no light hits the surface anymore, so it stays dark. The amount of diffused light is directly proportional to the cosine of the angle between the light direction and the surface normal. This is known as Lambert’s cosine law.Diffuse reflections.We can determine this Lamberterian reflectance factor by computing the dot product of the surface normal and the light direction. We already know the normal, but not yet the light direction. Let’s start with a fixed light direction, coming directly from above.\t\t\tfloat4 MyFragmentProgram (Interpolators i) : SV\\_TARGET {\t\t\t\ti.normal = normalize(i.normal);\t\t\t\treturn dot(float3(0, 1, 0), i.normal);\t\t\t} Lit from above, in gamma and linear space.What’s a dot product?The dot product between two vectors is geometrically defined as `A * B = ||A|| ||B|| cos theta`. This means that it is the cosine of the angle between the vectors, multiplied by their lengths. So in the case of two unity vectors, `A * B = cos theta`.Algebraically, it is defined as `A * B = sum_(i=1)^n A_i B_i = A_1 B_1 + A_2 B_2 + … + A_n B_n`. This means that you can compute it by multiplying all component pairs and sum them.float dotProduct = v1.x \\* v2.x + v1.y \\* v2.y + v1.z \\* v2.z;Visually, this operation projects one vector straight down to the other. As if casting a shadow on it. In doing so, you end up with a right triangle of which the bottom side’s length is the result of the dot product. And if both vectors are unit length, that’s the cosine of their angle.Dot product.Clamped LightingComputing the dot product works when the surface is directed towards the light, but not when it is directed away from it. In that case, the surface would logically be in its own shadow and it should receive no light at all. As the angle between the light direction and the normal must be larger than 90° at this point, its cosine and thus the dot product becomes negative. As we don’t want negative light, we have to clamp the result. We can use the standard max function for that.\t\t\t\treturn max(0, dot(float3(0, 1, 0), i.normal));Instead of max, you’ll often see shaders use saturate instead. This standard function clamps between 0 and 1.\t\t\t\treturn saturate(dot(float3(0, 1, 0), i.normal));This seems unnecessary, as we know that our dot product will never produce a result that is greater than 1. However, in some cases it can actually be more efficient, depending on the hardware. But we shouldn’t worry about such micro optimizations at this point. In fact, we can delegate that to the folks at Unity.The UnityStandardBRDF include file defines the convenient DotClamped function. This function performs a dot product and makes sure it is never negative. This is exactly what we need. It contains a lot of other lighting function as well, and includes other useful files too, which we’ll need later. So let’s use it!\t\t\t#include \"UnityCG.cginc\"\t\t\t#include \"UnityStandardBRDF.cginc\"\t\t\t…\t\t\tfloat4 MyFragmentProgram (Interpolators i) : SV\\_TARGET {\t\t\t\ti.normal = normalize(i.normal);\t\t\t\treturn DotClamped(float3(0, 1, 0), i.normal);\t\t\t}What does DotClamped look like?Here it is. Apparently, they decided that it’s better to use saturate when targeting low-capability shader hardware, and when targeting PS3.inline half DotClamped (half3 a, half3 b) {\t#if (SHADER\\_TARGET &lt; 30 || defined(SHADER\\_API\\_PS3))\t\treturn saturate(dot(a, b));\t#else\t\treturn max(0.0h, dot(a, b));\t#endif}This shader uses half-precision numbers, but you don’t need to worry about numerical precision yet. It only makes a difference for mobile devices.Because UnityStandardBRDF already includes UnityCG and some other files, we don’t have to explicitly include it anymore. It is not wrong to do so, but we might as well keep it short.~//\t\t\t#include \"UnityCG.cginc\"~\t\t\t#include \"UnityStandardBRDF.cginc\"Include file hierarchy, starting at UnityStandardBRDF.Light SourceInstead of a hard-coded light direction, we should use the direction of the light that’s in our scene. By default, each Unity scene has a light that represents the sun. It is a directional light, which means that it is considered to be infinitely far away. As a result, all its light rays come from exactly the same direction. Of course this isn’t true in real life, but the sun is so far away that it is a fair approximation.Default scene light, moved out of the way.UnityShaderVariables defines float4 _WorldSpaceLightPos0, which contains the position of the current light. Or the direction that the light rays are coming from, in case of a directional light. It has four components, because these are homogeneous coordinates. So the fourth component is 0 for our directional light.\t\t\t\tfloat3 lightDir = \\_WorldSpaceLightPos0.xyz;\t\t\t\treturn DotClamped(lightDir, i.normal);Light ModeBefore this produces correct result, we have to tell Unity which light data we want to use. We do so by adding a LightMode tag to our shader pass.Which light mode we need depends on how we’re rendering the scene. We can either use the forward or the deferred rendering path. There are also two older rendering modes, but we won’t bother with those. You choose the rendering path via the player rendering settings. It sits right above the color space choice. We’re using forward rendering, which is the default.Rendering path choice.We have to use the ForwardBase pass. This is the first pass used when rendering something via the forward rendering path. It gives us access to the main directional light of the scene. It sets up some other things as well, but we’ll cover those later.\t\tPass {\t\t\tTags {\t\t\t\t\"LightMode\" = \"ForwardBase\"\t\t\t}\t\t\tCGPROGRAM\t\t\t…\t\t\tENDCG\t\t}Diffuse light.Light ColorOf course light isn’t always white. Each light source has its own color, which we can get to via the fixed4 _LightColor0 variable, which is defined in UnityLightingCommon.What is fixed4?These are low-precision numbers, which trade precision for speed on mobile devices. On desktops, fixed is just an alias for float. Precision optimizations are a subject for later.This variable contains the light’s color, multiplied by its intensity. Although it provides all four channels, we only need the RGB components.\t\t\tfloat4 MyFragmentProgram (Interpolators i) : SV\\_TARGET {\t\t\t\ti.normal = normalize(i.normal);\t\t\t\tfloat3 lightDir = \\_WorldSpaceLightPos0.xyz;\t\t\t\tfloat3 lightColor = \\_LightColor0.rgb;\t\t\t\tfloat3 diffuse = lightColor \\* DotClamped(lightDir, i.normal);\t\t\t\treturn float4(diffuse, 1);\t\t\t}Colored light.AlbedoMost materials absorb part of the electromagnetic spectrum. This gives them their color. For example, if all visible red frequencies are absorbed, what escapes will appear cyan.What happens to the light that doesn’t escape?The light’s energy gets stored in the object, typically as heat. That’s why black things tend to be warmer than white things.The color of the diffuse reflectivity of a material is known as its albedo. Albedo is Latin for whiteness. So it describes how much of the red, green, and blue color channels are diffusely reflected. The rest is absorbed. We can use the material’s texture and tint to define this.\t\t\tfloat4 MyFragmentProgram (Interpolators i) : SV\\_TARGET {\t\t\t\ti.normal = normalize(i.normal);\t\t\t\tfloat3 lightDir = \\_WorldSpaceLightPos0.xyz;\t\t\t\tfloat3 lightColor = \\_LightColor0.rgb;\t\t\t\tfloat3 albedo = tex2D(\\_MainTex, i.uv).rgb \\* \\_Tint.rgb;\t\t\t\tfloat3 diffuse =\t\t\t\t\talbedo \\* lightColor \\* DotClamped(lightDir, i.normal);\t\t\t\treturn float4(diffuse, 1);\t\t\t}Let’s also change the label of the main texture to Albedo in the inspector.\tProperties {\t\t\\_Tint (\"Tint\", Color) = (1, 1, 1, 1)\t\t\\_MainTex (\"Albedo\", 2D) = \"white\" {}\t} Diffuse shading with albedo, in gamma and linear space.unitypackageSpecular ShadingBesides diffuse reflections, there are also specular reflections. This happens when light doesn’t get diffused after hitting a surface. Instead, the light ray bounces off the surface at and angle equal to the angle at which it hit the surface. This is what causes the reflections that you see in mirrors.Unlike with diffuse reflections, the position of the viewer matters for specular reflections. Only light that ends up reflected directly towards you is visible. The rest goes somewhere else, so you won’t see it.So we need to know the direction from the surface to the viewer. This requires the world-space positions of the surface and the camera.We can determine the world position of the surface in the vertex program, via the object-to-world matrix, then pass it to the fragment program.\t\t\tstruct Interpolators {\t\t\t\tfloat4 position : SV\\_POSITION;\t\t\t\tfloat2 uv : TEXCOORD0;\t\t\t\tfloat3 normal : TEXCOORD1;\t\t\t\tfloat3 worldPos : TEXCOORD2;\t\t\t};\t\t\tInterpolators MyVertexProgram (VertexData v) {\t\t\t\tInterpolators i;\t\t\t\ti.position = mul(UNITY\\_MATRIX\\_MVP, v.position);\t\t\t\ti.worldPos = mul(unity\\_ObjectToWorld, v.position);\t\t\t\ti.normal = UnityObjectToWorldNormal(v.normal);\t\t\t\ti.uv = TRANSFORM\\_TEX(v.uv, \\_MainTex);\t\t\t\treturn i;\t\t\t}The position of the camera can be accessed via float3 _WorldSpaceCameraPos, which is defined in UnityShaderVariables. We find the view direction subtracting the surface position from this and normalizing.\t\t\tfloat4 MyFragmentProgram (Interpolators i) : SV\\_TARGET {\t\t\t\ti.normal = normalize(i.normal);\t\t\t\tfloat3 lightDir = \\_WorldSpaceLightPos0.xyz;\t\t\t\tfloat3 viewDir = normalize(\\_WorldSpaceCameraPos - i.worldPos);\t\t\t\t\t\t\t\tfloat3 lightColor = \\_LightColor0.rgb;\t\t\t\tfloat3 albedo = tex2D(\\_MainTex, i.uv).rgb \\* \\_Tint.rgb;\t\t\t\tfloat3 diffuse =\t\t\t\t\talbedo \\* lightColor \\* DotClamped(lightDir, i.normal);\t\t\t\treturn float4(diffuse, 1);\t\t\t}Don’t Unity’s shaders interpolate the view direction?Yes. Unity’s shaders compute the view direction in the vertex program and interpolates that. Normalization is done in the fragment program, or in the vertex program for less-capable hardware. Either approach is fine.Reflecting LightTo know where the reflected light goes, we can use the standard reflect function. It takes the direction of an incoming light ray and reflects it based on a surface normal. So we have to negate our light direction.\t\t\t\tfloat3 reflectionDir = reflect(-lightDir, i.normal);\t\t\t\treturn float4(reflectionDir \\* 0.5 + 0.5, 1);Reflection directions.How does reflecting a vector work?You can reflect a direction `D` with a normal `N` by computing `D - 2N (N · D)`.In case of a perfectly smooth mirror, we’d only see reflected light where the surface angle is just right. In all other places, the reflected light misses us and the surface would appear black to us. But objects aren’t perfectly smooth. They have lots of microscopic bumps, which means that the surface normal can vary a lot.So we could see some of the reflection, even if our view direction doesn’t exactly match the reflection direction. The more we deviate from the reflection direction, the less of it we’ll see. Once again, we can use the clamped dot product to figure out how much light reaches us.\t\t\t\treturn DotClamped(viewDir, reflectionDir); Specular reflections.SmoothnessThe size of the highlight produced by this effect depends on the roughness of the material. Smooth materials focus the light better, so they have smaller highlights. We can control this smoothness by making it a material property. It is typically defined as a value between 0 and 1, so let’s make it a slider.\tProperties {\t\t\\_Tint (\"Tint\", Color) = (1, 1, 1, 1)\t\t\\_MainTex (\"Texture\", 2D) = \"white\" {}\t\t\\_Smoothness (\"Smoothness\", Range(0, 1)) = 0.5\t}\t\t\t\t…\t\t\tfloat \\_Smoothness;We narrow the highlight by raising the dot product to a higher power. We use the smoothness value for that, but it has to be much larger than 1 to have the desired effect. So let’s just multiply it by 100.\t\t\t\treturn pow(\t\t\t\t\tDotClamped(viewDir, reflectionDir),\t\t\t\t\t\\_Smoothness \\* 100\t\t\t\t);Pretty smooth.Blinn-PhongWe’re currently computing the reflection according to the Blinn reflection model. But the most-often used model is Blinn-Phong. It uses a vector halfway between the light direction and the view direction. The dot product between the normal and the half vector determines the specular contribution.~//\t\t\t\tfloat3 reflectionDir = reflect(-lightDir, i.normal);~\t\t\t\tfloat3 halfVector = normalize(lightDir + viewDir);\t\t\t\treturn pow(\t\t\t\t\tDotClamped(halfVector, i.normal),\t\t\t\t\t\\_Smoothness \\* 100\t\t\t\t); Blinn-Phong specular.This approach produces a larger highlight, but that can be countered by using a higher smoothness value. The result turns out to visually match reality a bit better than Phong, although both methods are still approximations. One big limitation is that it can produce invalid highlights for objects that are lit from behind.Incorrect specular, with smoothness 0.01.These artifacts become noticeable when using low smoothness values. They can be hidden by using shadows, or by fading out the specular based on the light angle. Unity’s legacy shaders have this problem too, so we’ll not worry about it either. We’ll move on to another lighting method soon anyway.Specular ColorOf course the color of the specular reflection matches that of the light source. So let’s factor it in.\t\t\t\tfloat3 halfVector = normalize(lightDir + viewDir);\t\t\t\tfloat3 specular = lightColor \\* pow(\t\t\t\t\tDotClamped(halfVector, i.normal),\t\t\t\t\t\\_Smoothness \\* 100\t\t\t\t);\t\t\t\treturn float4(specular, 1);But that is not all. The color of the reflection also depends on the material. This is not the same as the albedo. Metals tend to have very little if any albedo, while having strong and often colored specular reflectivity. In contrast, nonmetals tend to have a distinct albedo, while their specular reflectivity is weaker and not colorized.We can add a texture and tint to define the specular color, just as we do for the albedo. But let’s not bother with another texture and just use a tint.\tProperties {\t\t\\_Tint (\"Tint\", Color) = (1, 1, 1, 1)\t\t\\_MainTex (\"Albedo\", 2D) = \"white\" {}\t\t\\_SpecularTint (\"Specular\", Color) = (0.5, 0.5, 0.5)\t\t\\_Smoothness (\"Smoothness\", Range(0, 1)) = 0.1\t}\t…\t\t\tfloat4 \\_SpecularTint;\t\t\tfloat \\_Smoothness;\t\t\t…\t\t\tfloat4 MyFragmentProgram (Interpolators i) : SV\\_TARGET {\t\t\t\t…\t\t\t\tfloat3 halfVector = normalize(lightDir + viewDir);\t\t\t\tfloat3 specular = \\_SpecularTint.rgb \\* lightColor \\* pow(\t\t\t\t\tDotClamped(halfVector, i.normal),\t\t\t\t\t\\_Smoothness \\* 100\t\t\t\t);\t\t\t\treturn float4(specular, 1);\t\t\t}We can control both the colorizing and strength of the specular reflection with a color property.Tinted specular reflection.Can’t we use the tint’s alpha as smoothness?That is certainly possible. You can also store specular color and smoothness in a single texture that way.Diffuse and SpecularDiffuse and specular reflections are two parts of the lighting puzzle. We can add them together to make our picture more complete.\t\t\t\treturn float4(diffuse + specular, 1); Diffuse plus specular, in gamma and linear space.unitypackageEnergy ConservationThere is a problem with just adding the diffuse and specular reflections together. The result can be brighter than the light source. This is very obvious when using a fully white specular combined with low smoothness.White specular, 0.1 smoothness. Too bright.When light hits a surface, part of it bounces off as specular light. The rest of it penetrates the surface and either comes back out as diffuse light, or is absorbed. But we currently do not take this into consideration. Instead, our light both reflects and diffuses at full strength. So we could end up doubling the light’s energy.We have to make sure that the sum of the diffuse and specular parts of our material never exceed 1. That guarantees that we’re not creating light out of nowhere. It is fine if the total is less than 1. That just means that part of the light is absorbed.As we’re using a constant specular tint, we can simply adjust the albedo tint by multiplying it by 1 minus the specular. But it is inconvenient to do this manually, especially if we want to use a specific albedo tint. So let’s do this in the shader.\t\t\t\tfloat3 albedo = tex2D(\\_MainTex, i.uv).rgb \\* \\_Tint.rgb;\t\t\t\talbedo \\*= 1 - \\_SpecularTint.rgb;No longer too bright.The diffuse and specular contributions are now linked. The stronger the specular, the fainter the diffuse part. A black specular tint produces zero reflections, in which case you’ll see the albedo at full strength. A white specular tint results in a perfect mirror, so the albedo is completely eliminated.Energy conservation.MonochromeThis approach works fine when the specular tint is a grayscale color. But it produces weird results when other colors are used. For example, a red specular tint will only reduce the red component of the diffuse part. As a result, the albedo will be tinted cyan.Red specular, cyan albedo.To prevent this coloration, we can use monochrome energy conservation. This just means that we use the strongest component of the specular color to reduce the albedo.\t\t\t\talbedo \\*= 1 -\t\t\t\t\tmax(\\_SpecularTint.r, max(\\_SpecularTint.g, \\_SpecularTint.b));Monochome energy conservation.Utility FunctionAs you might expect, Unity has a utility function to take care of the energy conservation. It is EnergyConservationBetweenDiffuseAndSpecular and is defined in UnityStandardUtils.\t\t\t#include \"UnityStandardBRDF.cginc\"\t\t\t#include \"UnityStandardUtils.cginc\"Include file hierarchy, starting at UnityStandardUtils.This function takes albedo and specular colors as input, and output an adjusted albedo. But it also has a third output parameter, known as one-minus-reflectivity. This is one minus the specular strength, the factor we multiply the albedo with. It is an extra output, because reflectivity is needed for other lighting computations as well.\t\t\t\tfloat3 albedo = tex2D(\\_MainTex, i.uv).rgb \\* \\_Tint.rgb;~//\t\t\t\talbedo \\*= 1 -~~//\t\t\t\t\tmax(\\_SpecularTint.r, max(\\_SpecularTint.g, \\_SpecularTint.b));~\t\t\t\tfloat oneMinusReflectivity;\t\t\t\talbedo = EnergyConservationBetweenDiffuseAndSpecular(\t\t\t\t\talbedo, \\_SpecularTint.rgb, oneMinusReflectivity\t\t\t\t);What does EnergyConservationBetweenDiffuseAndSpecular look like?Here it is. It has three modes, either no conservation, monochrome, or colored. These are controlled with #define statements. The default is monochrome.half SpecularStrength(half3 specular) {\t#if (SHADER\\_TARGET &lt; 30)\t\t// SM2.0: instruction count limitation\t\t// SM2.0: simplified SpecularStrength\t\t// Red channel - because most metals are either monochrome\t\t// or with redish/yellowish tint\t\treturn specular.r;\t#else\t\treturn max(max(specular.r, specular.g), specular.b);\t#endif}// Diffuse/Spec Energy conservationinline half3 EnergyConservationBetweenDiffuseAndSpecular (\thalf3 albedo, half3 specColor, out half oneMinusReflectivity) {\toneMinusReflectivity = 1 - SpecularStrength(specColor);\t#if !UNITY\\_CONSERVE\\_ENERGY\t\treturn albedo;\t#elif UNITY\\_CONSERVE\\_ENERGY\\_MONOCHROME\t\treturn albedo \\* oneMinusReflectivity;\t#else\t\treturn albedo \\* (half3(1, 1, 1) - specColor);\t#endif}Metallic WorkflowThere are basically two kinds of materials that we are concerned with. There are metals, and there are nonmetals. The latter are also known as dielectric materials. Currently, we can create metals by using a strong specular tint. And we can create dielectrics by using a weak monochrome specular. This is the specular workflow.It would be much simpler if we could just toggle between metal and nonmetal. As metals don’t have albedo, we could use that color data for their specular tint instead. And nonmetals don’t have a colored specular anyway, so we don’t need a separate specular tint at all. This is known as the metallic workflow. Let’s go with that.Which is the better workflow?Both approaches are fine. That’s why Unity has a standard shader for each. The metallic workflow is simpler, because you have only one color source plus a slider. This is sufficient to create realistic materials. The specular workflow can produce the same results, but because you have more control, unrealistic materials are also possible.We can use another slider property as a metallic toggle, to replace the specular tint. Typically, it should be set to either 0 or 1, because something is either a metal or not. A value in between represents a material that has a mix of metal and nonmetal components.\tProperties {\t\t\\_Tint (\"Tint\", Color) = (1, 1, 1, 1)\t\t\\_MainTex (\"Albedo\", 2D) = \"white\" {}~//\t\t\\_SpecularTint (\"Specular\", Color) = (0.5, 0.5, 0.5)~\t\t\\_Metallic (\"Metallic\", Range(0, 1)) = 0\t\t\\_Smoothness (\"Smoothness\", Range(0, 1)) = 0.1\t}\t…~//\t\t\tfloat4 \\_SpecularTint;~\t\t\tfloat \\_Metallic;\t\t\tfloat \\_Smoothness;Metallic slider.Now we can derive the specular tint from the albedo and metallic properties. The albedo can then simply be multiplied by one minus the metallic value.\t\t\t\tfloat3 specularTint = albedo \\* \\_Metallic;\t\t\t\tfloat oneMinusReflectivity \\= 1 - \\_Metallic;~//\t\t\t\talbedo = EnergyConservationBetweenDiffuseAndSpecular(~~//\t\t\t\t\talbedo, \\_SpecularTint.rgb, oneMinusReflectivity~~//\t\t\t\t);~\t\t\t\talbedo \\*= oneMinusReflectivity;\t\t\t\t\t\t\t\tfloat3 diffuse =\t\t\t\t\talbedo \\* lightColor \\* DotClamped(lightDir, i.normal);\t\t\t\tfloat3 halfVector = normalize(lightDir + viewDir);\t\t\t\tfloat3 specular = specularTint \\* lightColor \\* pow(\t\t\t\t\tDotClamped(halfVector, i.normal),\t\t\t\t\t\\_Smoothness \\* 100\t\t\t\t);However, this is an oversimplification. Even pure dielectrics still have some specular reflection. So the specular strength and reflection values do not exactly match the metallic slider’s value. And this is also influenced by the color space. Fortunately, UnityStandardUtils also has the DiffuseAndSpecularFromMetallic function, which takes care of this for us.\t\t\t\tfloat3 specularTint; ~// = albedo \\* \\_Metallic;~\t\t\t\tfloat oneMinusReflectivity; ~// = 1 - \\_Metallic;~~//\t\t\t\talbedo \\*= oneMinusReflectivity;~\t\t\t\talbedo = DiffuseAndSpecularFromMetallic(\t\t\t\t\talbedo, \\_Metallic, specularTint, oneMinusReflectivity\t\t\t\t);Metallic workflow.What does DiffuseAndSpecularFromMetallic look like?Here it is. Note that it uses the half4 unity_ColorSpaceDielectricSpec variable, which is set by Unity based on the color space.inline half OneMinusReflectivityFromMetallic(half metallic) {\t// We'll need oneMinusReflectivity, so\t// 1-reflectivity = 1-lerp(dielectricSpec, 1, metallic)\t// = lerp(1-dielectricSpec, 0, metallic)\t// store (1-dielectricSpec) in unity\\_ColorSpaceDielectricSpec.a, then\t//\t 1-reflectivity = lerp(alpha, 0, metallic)\t// = alpha + metallic\\*(0 - alpha)\t// = alpha - metallic \\* alpha\thalf oneMinusDielectricSpec = unity\\_ColorSpaceDielectricSpec.a;\treturn oneMinusDielectricSpec - metallic \\* oneMinusDielectricSpec;}inline half3 DiffuseAndSpecularFromMetallic (\thalf3 albedo, half metallic,\tout half3 specColor, out half oneMinusReflectivity) {\tspecColor = lerp(unity\\_ColorSpaceDielectricSpec.rgb, albedo, metallic);\toneMinusReflectivity = OneMinusReflectivityFromMetallic(metallic);\treturn albedo \\* oneMinusReflectivity;}One detail is that the metallic slider itself is supposed to be in gamma space. But single values are not automatically gamma corrected by Unity, when rendering in linear space. We can use the Gamma attribute to tell Unity that it should also apply gamma correction to our metallic slider.\t\t\\[Gamma\\] \\_Metallic (\"Metallic\", Range(0, 1)) = 0Unfortunately, by now the specular reflections have now become rather vague for nonmetals. To improve this, we need a better way to compute the lighting.unitypackagePhysically-Based ShadingBlinn-Phong has long been the workhorse of the game industry, but nowadays physically-based shading – known as PBS – is all the rage. And for good reason, because it is a lot more realistic and predictable. Ideally, game engines and modeling tools all use the same shading algorithms. This makes content creation much easier. The industry is slowly converging on a standard PBS implementation.Unity’s standard shaders use a PBS approach as well. Unity actually has multiple implementations. It decides which to used based on the target platform, hardware, and API level. The algorithm is accessible via the UNITY_BRDF_PBS macro, which is defined in UnityPBSLighting. BRDF stands for bidirectional reflectance distribution function.~//\t\t\t#include \"UnityStandardBRDF.cginc\"~~//\t\t\t#include \"UnityStandardUtils.cginc\"~\t\t\t#include \"UnityPBSLighting.cginc\"Partial include file hierarchy, starting at UnityPBSLighting.What does UNITY_BRDF_PBS look like?It defines an alias for one of Unity’s BRDF functions. UNITY_PBS_USE_BRDF1 is set by Unity by default, as a platform define. This will select the best shader, unless the shader target is below 3.0.// Default BRDF to use:#if !defined (UNITY\\_BRDF\\_PBS)\t// allow to explicitly override BRDF in custom shader\t// still add safe net for low shader models,\t// otherwise we might end up with shaders failing to compile\t#if SHADER\\_TARGET &lt; 30\t\t#define UNITY\\_BRDF\\_PBS BRDF3\\_Unity\\_PBS\t#elif UNITY\\_PBS\\_USE\\_BRDF3\t\t#define UNITY\\_BRDF\\_PBS BRDF3\\_Unity\\_PBS\t#elif UNITY\\_PBS\\_USE\\_BRDF2\t\t#define UNITY\\_BRDF\\_PBS BRDF2\\_Unity\\_PBS\t#elif UNITY\\_PBS\\_USE\\_BRDF1\t\t#define UNITY\\_BRDF\\_PBS BRDF1\\_Unity\\_PBS\t#elif defined(SHADER\\_TARGET\\_SURFACE\\_ANALYSIS)\t\t// we do preprocess pass during shader analysis and we dont\t\t// actually care about brdf as we need only inputs/outputs\t\t#define UNITY\\_BRDF\\_PBS BRDF1\\_Unity\\_PBS\t#else\t\t#error something broke in auto-choosing BRDF\t#endif#endifI don’t include the actual functions, because they are large. You can see them by downloading Unity’s include files, or by finding the files in your Unity installation. They’re in UnityStandardBRDF.These functions are quite math-intensive, so I won’t go into the details. They still compute diffuse and specular reflections, just in a different way than Blinn-Phong. Besides that, there also is a Fresnel reflection component. This adds the reflections that you get when viewing objects at grazing angles. Those will become obvious once we include environmental reflections.To make sure that Unity selects the best BRDF function, we have to target at least shader level 3.0. We do this with a pragma statement.\t\t\tCGPROGRAM\t\t\t#pragma target 3.0\t\t\t#pragma vertex MyVertexProgram\t\t\t#pragma fragment MyFragmentProgramUnity’s BRDF functions return an RGBA color, with the alpha component always set to 1. So we can directly have our fragment program return its result.~//\t\t\t\tfloat3 diffuse =~~//\t\t\t\t\talbedo \\* lightColor \\* DotClamped(lightDir, i.normal);~~//\t\t\t\tfloat3 halfVector = normalize(lightDir + viewDir);~~//\t\t\t\tfloat3 specular = specularTint \\* lightColor \\* pow(~~//\t\t\t\t\tDotClamped(halfVector, i.normal),~~//\t\t\t\t\t\\_Smoothness \\* 100~~//\t\t\t\t);~\t\t\t\treturn UNITY\\_BRDF\\_PBS();Of course we have to invoke it with arguments. The functions each have eight parameters. The first two are the diffuse and specular colors of the material. We already have those.\t\t\t\treturn UNITY\\_BRDF\\_PBS(\t\t\t\t\talbedo, specularTint\t\t\t\t);The next two arguments have to be the reflectivity and the roughness. These parameters must be in one-minus form, which is an optimization. We already got oneMinusReflectivity out of DiffuseAndSpecularFromMetallic. And smoothness is the opposite of roughness, so we can directly use that.\t\t\t\treturn UNITY\\_BRDF\\_PBS(\t\t\t\t\talbedo, specularTint,\t\t\t\t\toneMinusReflectivity, \\_Smoothness\t\t\t\t);Of course the surface normal and view direction are also required. These become the fifth and sixth arguments.\t\t\t\treturn UNITY\\_BRDF\\_PBS(\t\t\t\t\talbedo, specularTint,\t\t\t\t\toneMinusReflectivity, \\_Smoothness,\t\t\t\t\ti.normal, viewDir\t\t\t\t);The last two arguments must be the direct and indirect light.Light StructuresUnityLightingCommon defines a simple UnityLight structure which Unity shaders use to pass light data around. It contains a light’s color, its direction, and an ndotl value, which is the diffuse term. Remember, these structures are purely for our convenience. It doesn’t affect the compiled code.We have all this information, so all we have to do is put it in a light structure and pass it as the seventh argument.\t\t\t\tUnityLight light;\t\t\t\tlight.color = lightColor;\t\t\t\tlight.dir = lightDir;\t\t\t\tlight.ndotl = DotClamped(i.normal, lightDir);\t\t\t\t\t\t\t\treturn UNITY\\_BRDF\\_PBS(\t\t\t\t\talbedo, specularTint,\t\t\t\t\toneMinusReflectivity, \\_Smoothness,\t\t\t\t\ti.normal, viewDir,\t\t\t\t\tlight\t\t\t\t);Why does the light data include the diffuse term?As the BRDF functions have all they need to calculate it themselves, why do we have to provide it? This is the case because the light structure is used in other contexts as well.Actually, the GGX BRDF version doesn’t even use ndotl. It computes it on its own, as its fiddles with the normal. As always, the shader compilers will get rid of all unused code. So you don’t have to worry about it.The final argument is for the indirect light. We have to use the UnityIndirect structure for that, which is also defined in UnityLightingCommon. It contains two colors, a diffuse and a specular one. The diffuse color represents the ambient light, while the specular color represents environmental reflections.We’ll cover indirect light later, so simply set these colors to black for now.\t\t\tfloat4 MyFragmentProgram (Interpolators i) : SV\\_TARGET {\t\t\t\ti.normal = normalize(i.normal);\t\t\t\tfloat3 lightDir = \\_WorldSpaceLightPos0.xyz;\t\t\t\tfloat3 viewDir = normalize(\\_WorldSpaceCameraPos - i.worldPos);\t\t\t\tfloat3 lightColor = \\_LightColor0.rgb;\t\t\t\tfloat3 albedo = tex2D(\\_MainTex, i.uv).rgb \\* \\_Tint.rgb;\t\t\t\tfloat3 specularTint;\t\t\t\tfloat oneMinusReflectivity;\t\t\t\talbedo = DiffuseAndSpecularFromMetallic(\t\t\t\t\talbedo, \\_Metallic, specularTint, oneMinusReflectivity\t\t\t\t);\t\t\t\t\t\t\t\tUnityLight light;\t\t\t\tlight.color = lightColor;\t\t\t\tlight.dir = lightDir;\t\t\t\tlight.ndotl = DotClamped(i.normal, lightDir);\t\t\t\tUnityIndirect indirectLight;\t\t\t\tindirectLight.diffuse = 0;\t\t\t\tindirectLight.specular = 0;\t\t\t\treturn UNITY\\_BRDF\\_PBS(\t\t\t\t\talbedo, specularTint,\t\t\t\t\toneMinusReflectivity, \\_Smoothness,\t\t\t\t\ti.normal, viewDir,\t\t\t\t\tlight, indirectLight\t\t\t\t);\t\t\t} Nonmetal and metal, in gamma and linear space.The next tutorial is Multiple Lights.unitypackage PDF本文转自 https://catlikecoding.com/unity/tutorials/rendering/part-4/，如有侵权，请联系删除。" }, { "title": "Unity 多纹理融合(翻译三)", "url": "/posts/Unity-Combine-Texture/", "categories": "翻译, Shader", "tags": "Unity3D, Shader", "date": "2018-01-02 20:00:00 +0800", "snippet": "本篇摘要： 采样多个纹理 应用细节纹理 处理线性空间中的颜色 使用 splat 地图纹理合并 融合多张纹理 贴图在游戏应用广泛，但它们有局限性。无论以何种尺寸显示，它们都有固定数量的像素。如果需要被渲染到很小网格，可以使用mipmap来保持它们的部分细节。但是当渲染到很大的网格上，会变得模糊。我们也不能无中生有地渲染更多额外的细节。本文讨论了一些解决办法。细节纹理通常可以使用更大的纹理，意味着更多的像素和更多的细节。但是纹理的大小也是有限制的，取决于游戏包体大小和目标平台的内存，以及gpu采样能力。另一种增加像素密度的方法是平铺纹理。出一张尽可能小的贴图，设置为重复模式。近距离观察下重复感可能不会很明显。毕竟当你站着用鼻子接触墙壁时，你只会看到整面墙壁的一小部分。因此，我们能够通过拉伸与平铺纹理相结合的方式来尽可能地添加细节。为了尝试这一点，我们使用一张棱角明显的纹理。这是一个方格图，放入的工程内使用默认导入设置。 略微扭曲的网格纹理 新建一个纹理融合shaderShader \"Custom/Textured With Detail\" { Properties { _Tint (\"Tint\", Color) = (1, 1, 1, 1) _MainTex (\"Texture\", 2D) = \"white\" {} } SubShader { }}使用此着色器创建一个新材质，然后为其指定该shader和网格纹理。 网格纹理 将材质分配给quad并查看它。从远处看效果还行。但是靠得太近看会变得模糊不清。缺失一些细节，同时纹理压缩造成的伪影也会变得很明显。网格特写，显示低纹素密度和 DXT1 伪影。多个纹理样本 带有低像素密度和DXT1伪影 多张纹理贴图采样现在我们只采样了单个纹理样本并将其用作片段着色器的结果，将采样的颜色存储在一个临时变量中。float4 MyFragmentProgram (Interpolators i) : SV_TARGET { float4 color = tex2D(_MainTex, i.uv) * _Tint; return color;}先假设可以通过引入平铺纹理的方式来增加像素密度。执行一次纹理采样函数，给它一个十倍采样面积，用这个结果替换临时存储的颜色原来的颜色输出到屏幕。float4 color = tex2D(_MainTex, i.uv) * _Tint;color = tex2D(_MainTex, i.uv * 10);return color;屏幕上会产生很多小网格。靠的很近再观察，结果不那么糟糕了。因为采样纹理用了平铺10次，所以很明显是一个重复的图案。硬编码平铺。 平铺纹理 请注意，此时我们正在执行两个纹理采样，但最终却只使用其中一个。这似乎很浪费。是吗？看看编译的顶点程序。uniform sampler2D _MainTex;in vec2 vs_TEXCOORD0;layout(location = 0) out vec4 SV_TARGET0;vec2 t0;void main(){ t0.xy = vs_TEXCOORD0.xy * vec2(10.0, 10.0); SV_TARGET0 = texture(_MainTex, t0.xy); return;}SetTexture 0 [_MainTex] 2D 0 ps_4_0 dcl_sampler s0, mode_default dcl_resource_texture2d (float,float,float,float) t0 dcl_input_ps linear v0.xy dcl_output o0.xyzw dcl_temps 1 0: mul r0.xy, v0.xyxx, l(10.000000, 10.000000, 0.000000, 0.000000) 1: sample o0.xyzw, r0.xyxx, t0.xyzw, s0 2: ret是否注意到编译后的代码中只有一个纹理采样？没错，编译器为我们去掉了不必要的代码！编译器基本上会丢弃任何最终未使用的内容。我们不想丢弃原始采样到颜色，就要合并两次采样结果。让我们通过将它们相乘来做到这一点。再添加一个_Tint属性，叠加一层自定义颜色。float4 color = tex2D(_MainTex, i.uv) * _Tint;color *= tex2D(_MainTex, i.uv);return color;着色编译器会生成什么样的代码呢，对此有何影响？uniform sampler2D _MainTex;in vec2 vs_TEXCOORD0;layout(location = 0) out vec4 SV_TARGET0;mediump vec4 t16_0;lowp vec4 t10_0;void main(){ t10_0 = texture(_MainTex, vs_TEXCOORD0.xy); t16_0 = t10_0 * t10_0; SV_TARGET0 = t16_0 * _Tint; return;}SetTexture 0 [_MainTex] 2D 0ConstBuffer \"$Globals\" 144Vector 96 [_Tint]BindCB \"$Globals\" 0 ps_4_0 dcl_constantbuffer cb0[7], immediateIndexed dcl_sampler s0, mode_default dcl_resource_texture2d (float,float,float,float) t0 dcl_input_ps linear v0.xy dcl_output o0.xyzw dcl_temps 1 0: sample r0.xyzw, v0.xyxx, t0.xyzw, s0 1: mul r0.xyzw, r0.xyzw, r0.xyzw 2: mul o0.xyzw, r0.xyzw, cb0[6].xyzw 3: ret这次的纹理采样，编译器检测到重复对_MainTex采样代码。对其进行优化后纹理只采样一次，结果存储在寄存器中并重复使用。即使使用_Tint中间变量等，编译器也足够聪明，可以检测到此类代码重复。最终将所有结果汇总后输出。现在再对UV坐标平铺×10次，最终看到大网格和小网格的融合在一起color *= tex2D(_MainTex, i.uv * 10); 平铺纹理 由于纹理采样时参数不再相同，编译器也必须保留两次采样。uniform sampler2D _MainTex;in vec2 vs_TEXCOORD0;layout(location = 0) out vec4 SV_TARGET0;vec4 t0;lowp vec4 t10_0;vec2 t1;lowp vec4 t10_1;void main(){ t10_0 = texture(_MainTex, vs_TEXCOORD0.xy); t0 = t10_0 * _Tint; t1.xy = vs_TEXCOORD0.xy * vec2(10.0, 10.0); t10_1 = texture(_MainTex, t1.xy); SV_TARGET0 = t0 * t10_1; return;}SetTexture 0 [_MainTex] 2D 0ConstBuffer \"$Globals\" 144Vector 96 [_Tint]BindCB \"$Globals\" 0 ps_4_0 dcl_constantbuffer cb0[7], immediateIndexed dcl_sampler s0, mode_default dcl_resource_texture2d (float,float,float,float) t0 dcl_input_ps linear v0.xy dcl_output o0.xyzw dcl_temps 2 0: sample r0.xyzw, v0.xyxx, t0.xyzw, s0 1: mul r0.xyzw, r0.xyzw, cb0[6].xyzw 2: mul r1.xy, v0.xyxx, l(10.000000, 10.000000, 0.000000, 0.000000) 3: sample r1.xyzw, r1.xyxx, t0.xyzw, s0 4: mul o0.xyzw, r0.xyzw, r1.xyzw 5: ret单独的细节纹理将两个纹理相乘时，结果会更暗。除非至少其中一种纹理是白色的。这是因为像素的每个颜色通道都有一个介于 0 和 1 之间的值。当向纹理添加细节时，可以通过该通道值来实现变暗或变亮。要使原始纹理变亮，给原始颜色乘2，使得每个颜色值都增大。color *= tex2D(_MainTex, i.uv * 10) * 2; 增亮颜色 这种直接扩大倍数的做法很粗暴。我们知道任何数乘以1不变，但是对细节纹理色加倍时，但对于1/2这个分界值就有用了。颜色区间是0-1，低于1/2的值将是结果变暗，高于1/2的值将变亮。这里引入一张特殊的灰度细节纹理来处理。 细节灰度图 灰度细节纹理？ 一般都是用灰度细节纹理来增白或加深原始颜色做二次细节调整，不是灰度图跳出的颜色不是那么直观的结果。要使用这个单独的细节纹理，我们必须在着色器中添加第二个纹理属性。使用灰色作为默认值，因为这不会改变主纹理的外观。Properties { _Tint (\"Tint\", Color) = (1, 1, 1, 1) _MainTex (\"Texture\", 2D) = \"white\" {} _DetailTex (\"Detail Texture\", 2D) = \"gray\" {}}将细节纹理分配给我们的材质并将其平铺设置为10。 两种纹理 我们必须添加变量来访问细节纹理及其平铺和偏移数据sampler2D _MainTex, _DetailTex;float4 _MainTex_ST, _DetailTex_ST;使用两个UV我们应该使用细节纹理的平铺和偏移数据，而不是使用硬编码乘10。struct Interpolators { float4 position : SV_POSITION; float2 uv : TEXCOORD0; float2 uvDetail : TEXCOORD1;}通过使用主纹理uv对细节纹理进行采样，得到一个新的细节纹理uv。Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.uv = TRANSFORM_TEX(v.uv, _MainTex); i.uvDetail = TRANSFORM_TEX(v.uv, _DetailTex); return i;}再一次看看汇编代码uniform \tvec4 _Tint;uniform \tvec4 _MainTex_ST;uniform \tvec4 _DetailTex_ST;in vec4 in_POSITION0;in vec2 in_TEXCOORD0;out vec2 vs_TEXCOORD0;out vec2 vs_TEXCOORD1;vec4 t0;void main(){ t0 = in_POSITION0.yyyy * glstate_matrix_mvp[1]; t0 = glstate_matrix_mvp[0] * in_POSITION0.xxxx + t0; t0 = glstate_matrix_mvp[2] * in_POSITION0.zzzz + t0; gl_Position = glstate_matrix_mvp[3] * in_POSITION0.wwww + t0; vs_TEXCOORD0.xy = in_TEXCOORD0.xy * _MainTex_ST.xy + _MainTex_ST.zw; vs_TEXCOORD1.xy = in_TEXCOORD0.xy * _DetailTex_ST.xy + _DetailTex_ST.zw; return;}Vector 112 [_MainTex_ST]Vector 128 [_DetailTex_ST]ConstBuffer \"UnityPerDraw\" 352Matrix 0 [glstate_matrix_mvp]BindCB \"$Globals\" 0BindCB \"UnityPerDraw\" 1 vs_4_0 dcl_constantbuffer cb0[9], immediateIndexed dcl_constantbuffer cb1[4], immediateIndexed dcl_input v0.xyzw dcl_input v1.xy dcl_output_siv o0.xyzw, position dcl_output o1.xy dcl_output o1.zw dcl_temps 1 0: mul r0.xyzw, v0.yyyy, cb1[1].xyzw 1: mad r0.xyzw, cb1[0].xyzw, v0.xxxx, r0.xyzw 2: mad r0.xyzw, cb1[2].xyzw, v0.zzzz, r0.xyzw 3: mad o0.xyzw, cb1[3].xyzw, v0.wwww, r0.xyzw 4: mad o1.xy, v1.xyxx, cb0[7].xyxx, cb0[7].zwzz 5: mad o1.zw, v1.xxxy, cb0[8].xxxy, cb0[8].zzzw 6: ret注意两个 UV 输出是如何在两个编译器顶点程序中定义的。OpenGLCore使用vs_TEXCOORD0和vs_TEXCOORD1输出，相反Direct3D11只使用一个输出o1.// Output signature://// Name Index Mask Register SysValue Format Used// -------------------- ----- ------ -------- -------- ------- ------// SV_POSITION 0 xyzw 0 POS float xyzw// TEXCOORD 0 xy 1 NONE float xy // TEXCOORD 1 zw 1 NONE float zw上面代码意味着两个 UV 对都被打包到一个输出寄存器中。第一个在 X 和 Y 通道，第二个在 Z 和 W 通道。因为寄存器总是由四个数字组成的组。Direct3D 11 编译器利用了这一点。 试着手动打包输出?手动打包输出的常见原因是只有少数几个插值器可用。Shader Model 2硬件支持8个通用插补器，而Shader Model 3硬件支持10个。复杂着色器可能会遇到这个限制。现在我们可以在片段程序中使用额外的UV对。float4 MyFragmentProgram (Interpolators i) : SV_TARGET {\tfloat4 color = tex2D(_MainTex, i.uv) * _Tint;\tcolor *= tex2D(_DetailTex, i.uvDetail) * 2;\treturn color;}uniform \tvec4 _Tint;uniform \tvec4 _MainTex_ST;uniform \tvec4 _DetailTex_ST;uniform sampler2D _MainTex;uniform sampler2D _DetailTex;in vec2 vs_TEXCOORD0;in vec2 vs_TEXCOORD1;layout(location = 0) out vec4 SV_TARGET0;vec4 t0;lowp vec4 t10_0;lowp vec4 t10_1;void main(){ t10_0 = texture(_MainTex, vs_TEXCOORD0.xy); t0 = t10_0 * _Tint; t10_1 = texture(_DetailTex, vs_TEXCOORD1.xy); t0 = t0 * t10_1; SV_TARGET0 = t0 + t0; return;}SetTexture 0 [_MainTex] 2D 0SetTexture 1 [_DetailTex] 2D 1ConstBuffer \"$Globals\" 144Vector 96 [_Tint]BindCB \"$Globals\" 0 ps_4_0 dcl_constantbuffer cb0[7], immediateIndexed dcl_sampler s0, mode_default dcl_sampler s1, mode_default dcl_resource_texture2d (float,float,float,float) t0 dcl_resource_texture2d (float,float,float,float) t1 dcl_input_ps linear v0.xy dcl_input_ps linear v0.zw dcl_output o0.xyzw dcl_temps 2 0: sample r0.xyzw, v0.xyxx, t0.xyzw, s0 1: mul r0.xyzw, r0.xyzw, cb0[6].xyzw 2: sample r1.xyzw, v0.zwzz, t1.xyzw, s1 3: mul r0.xyzw, r0.xyzw, r1.xyzw 4: add o0.xyzw, r0.xyzw, r0.xyzw 5: ret基于细节纹理，主纹理变得更亮和更暗。$\\rightarrow$ 明暗两张纹理 细节纹理渐变融合添加细节的想法是它们可以在近距离或放大时改善材质的外观。它们不应该在远处可见或缩小，因为这会使平铺变得明显。所以我们需要一种方法来随着纹理显示尺寸的减小而淡化细节。我们可以通过将细节纹理淡化为灰色来做到这一点，因为这不会导致颜色变化。需要做的就是在细节纹理的导入设置中启用Fadeout Mip Maps属性。这也会自动将过滤器模式切换为三线性，以便渐变为灰色是渐进的。$\\rightarrow$ 纹理过渡 网格从详细到不详细的过渡非常明显，但通常不会注意到它。例如，这里是大理石材质的主纹理和细节纹理。$\\rightarrow$ 大理石纹理 一旦我们的材质使用了这些纹理，细节纹理的过渡痕迹就不再明显了。$\\rightarrow$ 大理石材质 然而，由于细节纹理的过渡加持，大理石材质在近距离看起来要好得多。$\\rightarrow$ 没有细节和有细节特写 线性色彩空间当我们在 gamma 颜色空间中渲染场景时，我们的着色器工作正常，但如果我们切换到线性颜色空间，它就会出错。色彩空间在项目中设置。它配置在Other Settings播放器设置面板，可以通过Edit / Project Settings / Player. 选择颜色空间 什么是gamma色彩空间?伽玛空间是指经过伽玛校正的色彩空间。伽玛校正是对光强度的调整。最简单的方法是将原始值提高到某个幂次，即$value^gamma$。 gamma为 1 表示没有变化， 随着gamma递增光强度递增。这种转换最初是为了适应 CRT 显示器的非线性特性而引入的。另一个好处是它也大致对应于我们的眼睛对不同光强度的敏感程度。我们注意到深色之间的差异比明亮颜色之间的差异更大。因此，将更多的数字位分配给较暗的值而不是较浅的值是有意义的。求幂运算允许将较低的值扩展到更大的范围，同时压缩较高的值。最广泛使用的图像颜色格式是sRGB。它使用比简单求幂更复杂的公式，sRGB以$1\\over 2.2$的平均gamma值存储颜色，正常情况下这是一个合理的近似值。要将此数据转换回其原始颜色，请2.2幂的伽马校正。 gamma$1\\over 2.2$encoding vs. $2.2$deconding 假设Unity纹理的颜色存储为sRGB。在伽马空间中渲染时，着色器直接访问原始颜色和纹理数据。这是我们到目前为止所假设的。但是在线性空间中渲染时，就不再适用。 GPU会将采样的纹理颜色转换为线性空间。此外，Unity 也会将材质颜色属性转换为线性空间。然后着色器使用这些线性颜色进行操作。之后，片段程序的输出将被转换回伽马空间。使用线性颜色的优点之一是它可以实现更逼真的照明计算。这是因为光的相互作用在现实生活中是线性的，而不是指数的。不幸的是,切换到线性空间后，细节材料变得更暗。为什么会这样？$\\rightarrow$ gama vs. linear 色彩空间 因为我们将细节纹理采样后的颜色乘了2，所以$1\\over 2$的值不会导致主纹理发生任何变化。但是，转换为线性空间后，它会变为接近${1\\over 2} ^{2.2} \\approx 0.22$。加倍大约是0.44，远小于 1。这就解释了为什么变得更暗。我们可以通过在细节纹理的导入设置中启用Bypass sRGB Sampling来解决这个错误。这可以防止从伽玛到线性空间的转换，因此着色器将始终访问原始图像数据。但是，细节纹理是sRGB图像，所以结果仍然是错误的。最好的解决方案是重新对齐细节颜色，使它们再次以 1 为中心。我们可以通过${1 \\over {1\\over 2} ^{2.2}} \\approx 4.59$而不是乘以2来做到这一点。但我们只能在线性空间中渲染时才这样计算。UnityCG.cginc定义了一个统一变量，它将包含要与之相乘的正确数字。它是一个 float4，它的 rgb 分量有2或约4.59，这两个值视情况而定。由于伽马校正未应用于Alpha通道，因此始终为2。color *= tex2D(_DetailTex, i.uvDetail) * unity_ColorSpaceDouble;通过这种更改，无论我们在哪个颜色空间中渲染，我们的细节材质看起来都一样。纹理splat过渡遮罩细节纹理的一个限制是对整个表面使用相同的细节。 这适用于均匀的表面，如大理石板。但是，如果材质没有统一的外观，不希望在任何地方都使用相同的细节。考虑一个大地形。它可以有草、沙、岩石、雪等。希望这些地形类型有一定的细节。但是覆盖整个地形的纹理永远不会有足够的细节。可以通过为每种表面类型使用单独的纹理并平铺这些纹理来解决该问题。但是怎么知道在哪里使用哪种纹理呢？假设我们有一个具有两种不同表面类型的地形，什么时候决定使用哪种表面纹理呢。不是一就是二。我们可以用一个布尔值来表示这个逻辑。如果设置为true，我们使用第一个纹理，否则使用第二个。我们可以使用灰度纹理来存储这个选择。值1表示第一个纹理，而值0表示第二个纹理。事实上，我们可以使用这些值在两个纹理之间进行线性插值。然后介于 0 和 1 之间的值表示两种纹理之间的混合，这使得平滑过渡成为可能。这样的纹理被称为splat贴图。就像将多个地形特征喷溅到画布上一样。由于插值，这张地图甚至不需要高分辨率。 splat遮罩贴图 将其添加到项目后，将其导入类型切换为高级。启用Bypass sRGB Sampling并指示其mipmap应在Linear Space中生成。因为该纹理不需要sRGB颜色。所以在线性空间中渲染时不应该进行转换。另外，将其 Wrap Mode 设置为clamp，因为我们不会平铺这张地图。 splat导入设置 创建一个新的 Texture Splatting 着色器。Shader \"Custom/Texture Splatting\" { Properties { MainTex (\"Splat Map\", 2D) = \"white\" {} } SubShader { Pass { CGPROGRAM #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #include \"UnityCG.cginc\" sampler2D _MainTex; float4 _MainTex_ST; struct VertexData { float4 position : POSITION; float2 uv : TEXCOORD0; } struct Interpolators { float4 position : SV_POSITION; float2 uv : TEXCOORD0; } Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.uv = TRANSFORM_TEX(v.uv, _MainTex); return i; } float4 MyFragmentProgram (Interpolators i) : SV_TARGET { return tex2D(_MainTex, i.uv); } ENDCG } }}创建一个新材质并引用该shader，并将splat贴图指定为其主要纹理。 splat图渲染 增加融合纹理为了能够在两个纹理之间进行选择，作为属性添加命名为Texture1和Texture2到我们的着色器中。Properties { _MainTex (\"Splat Map\", 2D) = \"white\" {} _Texture1 (\"Texture 1\", 2D) = \"white\" {} _Texture2 (\"Texture 2\", 2D) = \"white\" {}}可以为他们使用任何你想要的纹理,这里使用网格纹理和大理石纹理。 增加的额外纹理 为添加到着色器的每个纹理修改平铺和偏移控制值。这需要我们将更多数据从顶点传递到片段着色器，或者在片元着色器中计算UV调整。 这很好，但通常地形的所有纹理都平铺相同。并且 splat 地图根本没有平铺。所以我们只需要一个平铺和偏移控件的实例。可以将属性控制添加到着色器属性之前，就像在C#代码中一样。NoScaleOffset将禁用纹理平铺和偏移。Properties { _MainTex (\"Splat Map\", 2D) = \"white\" {} [NoScaleOffset] _Texture1 (\"Texture 1\", 2D) = \"white\" {} [NoScaleOffset] _Texture2 (\"Texture 2\", 2D) = \"white\" {}}同时修改splat贴图tiling为4。 不需要额外的贴图纹理 将采样器变量添加到我们的着色器代码中，但是不必添加它们对应的 _ST 变量。sampler2D _MainTex;float4 _MainTex_ST;sampler2D _Texture1, _Texture2;对两张纹理采样后叠加，颜色会得到加深，然后输出float4 MyFragmentProgram (Interpolators i) : SV_TARGET { return tex2D(_Texture1, i.uv) + tex2D(_Texture2, i.uv);} 纹理叠加 使用splat贴图采样splat纹理需要顶点程序提供的UV坐标struct Interpolators { float4 position : SV_POSITION; float2 uv : TEXCOORD0; float2 uvSplat : TEXCOORD1;};Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.uv = TRANSFORM_TEX(v.uv, _MainTex); i.uvSplat = v.uv; return i;}然后，以在对其他纹理进行采样之前对splat贴图进行采样。float4 MyFragmentProgram (Interpolators i) : SV_TARGET { float4 splat = tex2D(_MainTex, i.uvSplat); return tex2D(_Texture1, i.uv) + tex2D(_Texture2, i.uv);}因为splat本身是单通道，可以任选一个RGB通道来来存储值，这里先决定使用第一个纹理与splat贴图R通道相乘。return tex2D(_Texture1, i.uv) * splat.r + tex2D(_Texture2, i.uv); 调制第一个纹理 第一个纹理现在由splat贴图R通道调制。为了完成插值，我们必须将另一个纹理1-R相乘。return tex2D(_Texture1, i.uv) * splat.r + tex2D(_Texture2, i.uv) * (1 - splat.r); 调制两个纹理 RGB Splat贴图我们有一个功能性的splat材质，但它只支持两种纹理。我们可以支持更多吗？我们现在只使用了R通道，那么我们添加G和B通道怎么样？那么(1,0,0)代表第一个纹理，(0,1,0)代表第二个纹理，(0,0,1)代表第三个纹理。 为了在这三个之间获得正确的插值，我们只需要确保RGB通道的总和为1即可。但是等等，当我们只使用一个通道时，我们可以支持两个纹理。这是因为第二个纹理的权重是通过1-R得出的。同样的技巧适用于任意数量的通道。因此可以通过1-R-G-B支持另一种纹理。这导致了一个具有三种颜色和黑色的splat贴图。只要三个通道加起来不超过1，它就是一个有效的贴图。这里给出一张这样的贴图，导入Unity。 RGB splat 贴图 当 R + G + B 超过1时会发生什么？ 那么前三个纹理的组合会太强。 同时，第四个纹理将被减去而不是被添加。 如果错误很小，那么不会注意到并且结果足够好。 示例 RGB 映射实际上并不完美，但不会注意到。 纹理压缩引入了更多错误，但同样难以察觉。 可以使用alpha通道吗？ 确实可以！ 这意味着单个 RGBA splat 贴图最多可以支持五种不同的地形类型。 但是对于本教程，四个就足够了。 如果要使用超过五个纹理，则必须使用多个splat贴图。虽然这是可能的，但最终会得到很多纹理采样此时可以使用更好的技术，例如纹理数组。为了支持 RGB splat贴图，我们必须在着色器中添加两个额外的纹理。为它们分配了大理石细节和测试纹理。Properties { _MainTex (\"Splat Map\", 2D) = \"white\" {} [NoScaleOffset] _Texture1 (\"Texture 1\", 2D) = \"white\" {} [NoScaleOffset] _Texture2 (\"Texture 2\", 2D) = \"white\" {} [NoScaleOffset] _Texture3 (\"Texture 3\", 2D) = \"white\" {} [NoScaleOffset] _Texture4 (\"Texture 4\", 2D) = \"white\" {}} Four textures 将所需的变量添加到着色器。 再一次，没有额外的 _ST需要的变量。sampler2D _Texture1, _Texture2, _Texture3, _Texture4;在片段程序中，添加额外的纹理样本。第二个样本现在使用G通道，第三个使用B通道。最终样本用 (1 - R - G - B) 调制。retur1n tex2D(_Texture1, i.uv) * splat.r + tex2D(_Texture2, i.uv) * splat.g + tex2D(_Texture3, i.uv) * splat.b + tex2D(_Texture4, i.uv) * (1 - splat.r - splat.g - splat.b); 四个纹理飞溅 为什么混合区域在线性色彩空间中看起来不同？ 我们的 splat 贴图绕过了 sRGB 采样，所以混合不应该取决于我们使用的颜色空间，对吧？ splat 地图确实不受影响。 但是发生混合的色彩空间确实发生了变化。 在伽马空间渲染的情况下，样本在伽马空间中混合，仅此而已。 但是在线性空间中渲染时，它们首先转换为线性空间，然后混合，然后再转换回伽马空间。 结果略有不同。 在线性空间中，混合也是线性的。 但在伽马空间中，混合偏向较深的颜色。现在知道如何应用细节纹理以及如何将多个纹理与splat贴图混合。也可以组合这些方法。可以将四个细节纹理添加到splat着色器并使用贴图在它们之间进行混合。当然，这需要四个额外的纹理采样，性能有限。还可以使用贴图来控制应用细节纹理的位置以及省略的位置。在这种情况下，需要一张单色贴图，它可以用作遮罩。当单个纹理同时包含表示多个不同材质的区域但没有地形那么大的面积时，这很有用。例如，如果我们的大理石纹理还包含金属片，则不希望在此处应用大理石细节。" }, { "title": "Unity Shader Fundamentals(翻译二)", "url": "/posts/Unity-Shader-Fundamentals/", "categories": "翻译, Shader", "tags": "Unity3D, Shader", "date": "2018-01-02 16:00:00 +0800", "snippet": "本篇摘要信息 顶点变换 Color pixels shader 属性 从顶点传数据至片元函数 查看编译后的shader代码场景初始化新建一个默认场景，新建一个圆球。这个默认场景本身进行了大量复杂的渲染，为了更容易的掌握Unity的渲染过程，我们先做一些简化设置，把默认的某些花里胡哨的东西先剥离掉。剥离天空盒打开Window-Lighting，查看光照设置选项。弹出带有3个选项卡的面板，我们先关注Scene选项卡. 默认光照 第一选项卡Environment是跟环境光照相关，在这里可以设置天空盒。这个Default-Skybox当前正被用于场景的背景光、环境光、和反射光。设置为none就能关闭这些光。顺便把下面的Realtime Ligting和Mixed Lighting也关掉，现在还用不上，后面会陆续介绍。关闭了天空盒，环境颜色自动切换为了纯色，这个颜色默认是带着一丝蓝的黑灰色(说好的纯呢，外表很黑内心很蓝？)。而反射光会变成纯黑色。如下所示，设置后球体变暗了，背景变成了纯色。而这个背景深蓝色从哪里来的呢？$\\rightarrow$ sample-light 这个深蓝色被定义在摄像机，它默认使用天空盒渲染，当天空盒失效后场景会默认退回到使用相机纯色模式。 默认的摄像机设置 为了进一步简化渲染，再隐藏或删除方向光对象。这将消除场景中的直接光照，以及所有它投射的阴影。剩下纯色背景和球体的轮廓。 未着色球体 图像渲染分两步绘制上面的场景，一是使用相机的背景色填充图像，然后再在上面画出球体的轮廓。Unity如何知道该画这个球体呢?我们有一个球体对象并且绑定了_MeshRenderer_组件，如果这个球体位于摄像机的视野内，那么它就会被渲染出来。Unity通过检测球体的边界盒是否与摄像机的视锥体相机来验证这一点。包围盒在Unity中定义为Bounds结构体Collider.bounds, Mesh.bounds, Renderer.bounds. 球体默认自带组件 Transform组件用于更改坐标、方向，以及网格和包围盒的尺寸。这里有对Transform层次结构的清晰描述。如果一个物体最终处于摄像机视野内，它就会被安排渲染。最后，GPU负责渲染物体的mesh。这些具体的渲染指令在物体的material定义好的，这个material引用了一个shader-GPU程序。 2u分工 当前这个球体使用了Unity的默认材质，自带了一个标准shader。我们现在把它去掉替换成自己的shader，从头开始写。创建一个Shader通过点击_Assets / Create / Shader / Unlit Shader创建并命名自己的shader，双击shader文件打开，并删除里面的内容从头写。 第一个shader Shader是通过shader关键字定义，关键字是一个字符串，在下拉界面中选择时显示的也是该关键字。它不必与文件名相同。Shader \"Unlit/MyShader\"{ //...}保存文件，回到编辑器会收到警告提示none of subshaders/fallbacks are suitable，因为它是空的，没有sub-shader或回调shader。尽管这个shader没有内容也有警告，我们仍能指定给material。点击_Assets / Create / Material_创建，然后通过下拉菜单指定。 给材质指定Shader 给球体指定上我们新建的Material，替换掉默认的。这时的球体会立即变成紫红色。发生这个的原因是Unity切换到了错误的shader，它故意使用这个颜色来提醒开发者这是一个错误。 指定MyMaterial shader warning中提到了没有sub-shader. 我们可以使用sub-shader操作shader变量进行分组, 这允许程序员为不同的编译平台提供不同的sub-shader.例如我们可以用一个sub-shader既支持pc又支持手机平台.定义一个SubShader块Shader \"Unlit/MyShader\"{ SubShader { //... } }sub-shader至少包含一个以上的pass块, pass代码块是物体实际被渲染的地方，我们先写一个pass，然后在写多个pass。为了呈现多种效果，pass数量可能会超过一个以上，而则代表着物体要被渲染多次。Shader \"Unlit/MyShader\"{ SubShader { Pass { //... } }}我们的球体现在应该变成了白色，因为我们使用了一个空pass渲染，这也意味着我们的Shader没有出现任何错误了。 空shader效果 Shader程序现在我们要开始编写shader代码了，我们用的Unity着色器语言是HLSL和CG着色器语言的变体。所以必须指示CGPROGRAM关键字为代码的开始，同时要用ENDCG关键字做为结束。Pass{ CGPROGRAM ENDCG}再次打开编辑器编译后有一个警告Both vertex and fragment programs must be present,表示没有顶点和片元程序。shader由这两个程序组成，vertex顶点程序负责处理网格的顶点数据，这包含了从对象空间到显示空间的转换；而fragment片元程序负责为网格的三角形内的单个像素进行着色。 顶点片元函数 同时，我们必须通过pragma指令告诉编译器使用哪些程序CGPROGRAM#pragma vertex MyVertexProgram#pragma fragment MyFragmentProgramENDCG编译器再次发出来错误提示，这次是因为它不能找到我们指定的程序片段，因为我们光声明没实现。首先vertex和fragment被写成方法，类似C#函数。先简单地创建两个同名的void方法。CGPROGRAM#pragma vertex MyVertexProgram#pragma fragment MyFragmentProgramvoid MyVertexProgram() {}void MyFragmentProgram() {}ENDCG这次编译后没有报错，但是球体从屏幕上消失了。Shader汇编Unity的shader编译器把我们的代码根据不同target-compile成了不同程序。不同的平台需要不同的解决方案，例如Direct3D是服务于Windows平台，OpenGL针对MacOs，OpenGL ES针对手机平台。这里我们不是在处理单个编译器，而是多个编译器。最终使用哪种编译器取决于目标平台，这些编译器也是不完全相同的，每个平台可能得到不同的结果。在这个例子中，我们的空程序在OpenGL和Direct3D 11下能很好的工作，但在Direct3D 9就会报错。在编辑器下点选MyFirstShader，在inspector面板可以查看该shader的一些信息，以及编译错误。这也有一个Compiled code and show按钮和下拉菜单。 shader检视面板信息 如果你点击该按钮，Unity将会编译该shader并打开它，接着就可以查看生成的代码。 shader检视信息 我们试着先选择OpenGL Core，然后再选择D3D11，看看底层代码是怎么回事的。Shader \"Unlit/MyShader\" {SubShader { Pass { No keywords set in this variant. -- Vertex shader for \"glcore\": Shader Disassembly: #ifdef VERTEX #version 150 #extension GL_ARB_explicit_attrib_location : require #extension GL_ARB_shader_bit_encoding : enable void main() { return; } #endif #ifdef FRAGMENT #version 150 #extension GL_ARB_explicit_attrib_location : require #extension GL_ARB_shader_bit_encoding : enable void main() { return; } #endif -- Fragment shader for \"glcore\": Shader Disassembly: // All GLSL source is contained within the vertex program }}}提炼出两个main函数，有vertex和fragment程序#ifdef VERTEXvoid main(){ return;}#endif#ifdef FRAGMENTvoid main(){ return;}#endifD3D11自行查看，因为编译后的代码实在是太长了，不方便贴上来。只选取了一个片段：Pass { No keywords set in this variant. -- Vertex shader for \"d3d11\": Shader Disassembly: vs_4_0 0: ret -- Fragment shader for \"d3d11\": Shader Disassembly: ps_4_0 0: ret}引入其他文件编写shader代码很费劲，有时需要重复写类似的函数，为了简化书写，这里有一个类似C#程序的功能，引用其他类中的通用变量、函数等。使用#include指令就能加载一个文件。先试着加载Unity内部自带的UnityCG.cgincCGPROGRAM #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #include \"UnityCG.cginc\" void MyVertexProgram() { } void MyFragmentProgram() { }ENDCG下面是_UnityCg.cginc_的引用层次结构 UnityCG.cginc结构 UnityShaderVariables.cginc定义了一大堆渲染所需的着色器变量，比如矩阵变换、相机和光照数据等等。UnityInstancing.cginc内置在引擎安装包内，这是一种减少绘制调用的特定呈现技术。虽然它不直接包含文件，但它依赖于UnityShaderVariables。HLSLSupport.cginc设置了一些无论您的目标是哪个平台都可以使用相同的代码的功能。请注意，这些文件的内容将被复制到文件中，取代include指令。这发生在预处理步骤中，该步骤执行所有预处理指令。比如#include和#pragma。产生输出(输出语义)为了渲染物体，shader必须要产生结果。Vertex顶点函数必须要返回每个顶点的最终坐标：SV_POSITION。一个顶点有几个坐标分量？4个，因为我们使用了4x4变换矩阵。现在把函数类型从void改为float4,一个float4类型是一个由4个float类型简单组成。float4 MyVertexProgram() : SV_POSITION{ return 0;}Fragment片元函数返回像素的最终颜色：SV_TARGET。也是float4。float4 MyFragmentProgram() : SV_TARGET{ return 0;}Vertex顶点函数的输出作为Fragment片元函数的输入。输入的参数需要匹配！float4 MyFragmentProgram( float4 position : SV_POSITION) : SV_TARGET{ return 0;}然后看看Unity的shader汇编//--------------D3D11------------------- Vertex shader for \"d3d11\":Shader Disassembly: vs_4_0 //顶点着色器版本 dcl_output_siv o0.xyzw, position //声明o0作为输出值，带有系统值 0: mov o0.xyzw, l(0,0,0,0) //把(0,0,0,0)移动到o0中 1: ret //返回-- Fragment shader for \"d3d11\":Shader Disassembly: ps_4_0 dcl_output o0.xyzw 0: mov o0.xyzw, l(0,0,0,0) 1: ret//---------------GL CORE-----------#ifdef VERTEXvoid main(){ gl_Position = vec4(0.0, 0.0, 0.0, 0.0); return;}#endif#ifdef FRAGMENTlayout(location = 0) out vec4 SV_TARGET0;void main(){ SV_TARGET0 = vec4(0.0, 0.0, 0.0, 0.0); return;}#endif顶点变换把球给我画出来！为了得到模型空间的顶点坐标，给ertex顶点函数增加一条语义POSITON。而模型空间的顶点坐标是其次坐标。先直接返回这个顶点坐标，贴汇编：//---D3d11------- vs_4_0 //版本 dcl_input v0.xyzw //申明v0 输入系统值 dcl_output_siv o0.xyzw, position //申明o0 输出系统值 0: mov o0.xyzw, v0.xyzw //把v0值 移动到 o0 1: ret//--GL CORE---#ifdef VERTEXin vec4 in_POSITION0;void main(){ gl_Position = in_POSITION0; return;}#endifView Code 扭曲的球 使用MVP：model_view_projection矩阵变换顶点坐标，该值定义在UnityShaderVariables文件，变量名是UNITY_MATRIX_MVP。改为：mul函数定义return mul(UNITY_MATRIX_MVP, position);贴汇编看看-- Vertex shader for \"d3d11\":// Stats: 8 mathUses vertex data channel \"Vertex\"//cbuffers常量数据Constant Buffer \"UnityPerDraw\" (160 bytes) on slot 0 { Matrix4x4 unity_ObjectToWorld at 0}Constant Buffer \"UnityPerFrame\" (384 bytes) on slot 1 { Matrix4x4 unity_MatrixVP at 272}Shader Disassembly: //版本 vs_4_0 //声明常量缓冲区cbuffers，逐字索引 dcl_constantbuffer CB0[4], immediateIndexed //cbuffers dcl_constantbuffer CB1[21], immediateIndexed //声明输入v0 dcl_input v0.xyz //声明输入o0 dcl_output_siv o0.xyzw, position //声明临时寄存器2个(r0-r1) dcl_temps 2 //将v0与cb0[1]相乘传递给r0 0: mul r0.xyzw, v0.yyyy, cb0[1].xyzw /*第0行的计算步骤 dest.x = cb0[0].x * v0.x + r0.x; dest.y = cb0[0].y * v0.x + r0.y; dest.z = cb0[0].z * v0.x + r0.z; dest.w = cb0[0].w * v0.x + r0.w;*/ //r0 = cb0 * v0 + r0 1: mad r0.xyzw, cb0[0].xyzw, v0.xxxx, r0.xyzw //同理1： 2: mad r0.xyzw, cb0[2].xyzw, v0.zzzz, r0.xyzw //r0 = r0 + cb0 3: add r0.xyzw, r0.xyzw, cb0[3].xyzw //r1 = r0 * cb1 4: mul r1.xyzw, r0.yyyy, cb1[18].xyzw 5: mad r1.xyzw, cb1[17].xyzw, r0.xxxx, r1.xyzw //同理1： 6: mad r1.xyzw, cb1[19].xyzw, r0.zzzz, r1.xyzw //同理1： 7: mad o0.xyzw, cb1[20].xyzw, r0.wwww, r1.xyzw //同理1： 8: retView Code 正确的球 像素颜色先给Fragment函数返回点东西，float4 MyFragmentProgram(float4 position : SV_POSITION) : SV_TARGET{ return float4(1, 1, 0, 1);} 上色 Shader属性Properties$\\rightarrow$ 属性结构 使用属性需要在pass块内声明一个同类型的同命名变量float4 _Tint;float4 MyFragmentProgram(float4 position : SV_POSITION) : SV_TARGET{ return _Tint;}看看片元函数的汇编-- Fragment shader for \"d3d11\":Constant Buffer \"$Globals\" (48 bytes) on slot 0 { Vector4 _Tint at 32}Shader Disassembly: ps_4_0 dcl_constantbuffer CB0[3], immediateIndexed dcl_output o0.xyzw 0: mov o0.xyzw, cb0[2].xyzw 1: retView Code 纯色球 从顶点到片元上图纯色球，每个像素都是同一个颜色，但是美术给的效果图是五彩斑斓的，就需要GPU光栅化三角形，取三个处理过的顶点进行插值，找到三角形内所有像素并着色。 shader程序执行流程 处理过的顶点数据不直接传递给Fragment片元函数，而在片元函数中访问插值本地数据，需要增加一个参数，并指定语义TEXCOORD0，它表示贴图的UV坐标。float4 MyVertexProgram( float4 position: POSITION, out float3 localPosition : TEXCOORD0) : SV_POSITION{ localPosition = position.xyz; return UnityObjectToClipPos(position);}float4 MyFragmentProgram( float4 position : SV_POSITION, float3 localPosition : TEXCOORD0) : SV_TARGET{ return float4(localPosition, 1);} 插值本地数据作为颜色 结构体简化传递Fragment函数的参数，新建一个结构体struct Interpolators{ float4 position : SV_POSITION; float3 localPosition : TEXCOORD0;};Interpolators MyVertexProgram (float4 position: POSITION ){ Interpolators i; i.localPosition = position.xyz; i.position = UnityObjectToClipPos(position); return i;}float4 MyFragmentProgram (Interpolators i) : SV_TARGET{ return float4(i.localPosition, 1); } UnityObjectToClipPos是Unity5.6之后的优化：它对应mul(UNITY_MATRIX_MVP, v.vertex)，但是该函数使用了常数1作为第四个坐标而不是依赖网格数据，源码: inline float4 UnityObjectToClipPosInstanced(in float3 pos){ float4 w = mul(unity_ObjectToWorldArray[unity_InstanceID], float4(pos, 1.0) return mul(UNITY_MATRIX_VP, w));} 因为通过网格提供的数始终为1，但是编译器不能知晓。所幸干脆就直接写死为1.0，优化掉运行时再去计算第四个数到底是多少这一步。调整颜色因为负颜色被约束限制为零，我们的球体最终变得相当暗。 球体的自身半径为$1\\over 2$，因此颜色通道最终介于$-{1\\over 2}$和$1\\over 2$之间。我们希望将它们移动到 0-1 范围内，我们可以通过向所有通道添加$1\\over 2$来实现。return float4(i.localPosition + 0.5, 1);再看看汇编代码uniform \tvec4 _Tint;in vec3 vs_TEXCOORD0;layout(location = 0) out vec4 SV_TARGET0;vec4 t0;void main(){ t0.xyz = vs_TEXCOORD0.xyz + vec3(0.5, 0.5, 0.5); t0.w = 1.0; SV_TARGET0 = t0 * _Tint; return;}ConstBuffer \"$Globals\" 128Vector 96 [_Tint]BindCB \"$Globals\" 0 ps_4_0 dcl_constantbuffer cb0[7], immediateIndexed dcl_input_ps linear v0.xyz dcl_output o0.xyzw dcl_temps 1 0: add r0.xyz, v0.xyzx, l(0.500000, 0.500000, 0.500000, 0.000000) 1: mov r0.w, l(1.000000) 2: mul o0.xyzw, r0.xyzw, cb0[6].xyzw 3: ret 调整之后的颜色 纹理如果想在模型网格上不添加更多三角形面数的情况下为网格添加更多更明显的细节和多样性呈现，可以使用纹理投影到网格的三角形上。纹理坐标用于控制投影，下图是2D坐标。不管纹理的实际纵横比如何，其水平坐标称为U，垂直坐标称为V，它们通常称为UV坐标。 uv坐标图 U坐标从左到右递增，起始点为0终点为1。 V坐标从下到上增加，除了Direct3D它从上到下。使用UV坐标Unity的默认网格具有适合纹理映射的UV坐标。顶点程序可以通过参数访问它们TEXCOORD0语义。然后传递给片元函数使用。struct Interpolators { float4 position : SV_POSITION; float2 uv : TEXCOORD0;};Interpolators MyVertexProgram (VertedData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.uv = v.uv; return i;}float4 MyFragmentProgram (Interpolators i) : SV_TARGET { return float4(i.uv, 1, 1);}再看看汇编in vec4 in_POSITION0;in vec2 in_TEXCOORD0;out vec2 vs_TEXCOORD0;vec4 t0;void main(){ t0 = in_POSITION0.yyyy * glstate_matrix_mvp[1]; t0 = glstate_matrix_mvp[0] * in_POSITION0.xxxx + t0; t0 = glstate_matrix_mvp[2] * in_POSITION0.zzzz + t0; gl_Position = glstate_matrix_mvp[3] * in_POSITION0.wwww + t0; //将UV坐标从顶点数据复制到Interpolators输出 vs_TEXCOORD0.xy = in_TEXCOORD0.xy; return;}Bind \"vertex\" VertexBind \"texcoord\" TexCoord0ConstBuffer \"UnityPerDraw\" 352Matrix 0 [glstate_matrix_mvp]BindCB \"UnityPerDraw\" 0 vs_4_0 dcl_constantbuffer cb0[4], immediateIndexed dcl_input v0.xyzw dcl_input v1.xy dcl_output_siv o0.xyzw, position dcl_output o1.xy dcl_temps 1 0: mul r0.xyzw, v0.yyyy, cb0[1].xyzw 1: mad r0.xyzw, cb0[0].xyzw, v0.xxxx, r0.xyzw 2: mad r0.xyzw, cb0[2].xyzw, v0.zzzz, r0.xyzw 3: mad o0.xyzw, cb0[3].xyzw, v0.wwww, r0.xyzw 4: mov o1.xy, v1.xyxx//将UV坐标从v1.xy传递到o1.xy 5: retUnity将UV坐标包裹在其球体周围，在球体两极处图像的顶部和底部。图像的左侧和右侧接缝连接在一起。 沿着该接缝，UV坐标值从0到1。$\\rightarrow$ 球体缝隙 添加纹理要使用纹理，必须添加另一个着色器属性。 常规纹理属性的类型是2D ，因为还有其他类型的纹理。 默认值是引用 Unity 的默认纹理之一的字符串，可以是white 、 black 或 gray 。主纹理命名约定是_MainTex。 这也使您可以使用方便的 Material.mainTexture属性以通过脚本访问它。Properties { _Tint (\"Tint\", Color) = (1, 1, 1, 1) _MainTex (\"Texture\", 2D) = \"white\" {}} “white” {} 这个花括号有什么用？ 上古时代开发的固定功能着色器曾经需要纹理设置，这些设置被放在这些括号内，但现在它们不再使用了。即使它们现在已经无用，着色编译器仍然需要它们，如果忽略它们会产生错误。选中材质，查看inspector信息 纹理_white显示 通过使用sampler2D类型为变量来访问着色器中的纹理。float4 _Tint;sampler2D _MainTex;通过在片段程序中使用tex2D函数，完成对纹理UV坐标进行采样。float4 MyFragmentProgram (Interpolators i) : SV_TARGET { return tex2D(_MainTex, i.uv);}再查看汇编后的shader代码uniform sampler2D _MainTex;in vec2 vs_TEXCOORD0;layout(location = 0) out vec4 SV_TARGET0;void main(){ SV_TARGET0 = texture(_MainTex, vs_TEXCOORD0.xy); return;}SetTexture 0 [_MainTex] 2D 0 ps_4_0 dcl_sampler s0, mode_default dcl_resource_texture2d (float,float,float,float) t0 dcl_input_ps linear v0.xy dcl_output o0.xyzw 0: sample o0.xyzw, v0.xyxx, t0.xyzw, s0 1: ret$\\rightarrow$ 带纹理球体 球体两极附近会显得非常杂乱。 为什么会这样？ 发生纹理失真是因为插值在三角形中是线性的。 Unity 的球体在两极附近只有几个三角形，其中UV坐标失真最大。所以UV坐标在三角形与三角形的顶点之间是非线性变化，但在三角形内部的顶点之间的变化是线性的。所以球体两极纹理中的直线在三角形边界处突然改变方向。 跨三角形插值 不同的网格具有不同的 UV 坐标，从而产生不同的映射。 Unity 的默认球体使用经纬度纹理映射，而网格是低分辨率立方体球体。 这足以进行测试，但您最好使用自定义球体网格以获得更好的结果。平铺和偏移为着色器添加纹理属性后，材质检查器不仅显示了纹理字段。它还显示了平铺和偏移控制。但是，更改这些 2D 向量目前没有任何效果。这些额外的纹理数据存储在材质中，也可以由着色器访问。 可以通过与关联材料同名的变量加上_ST 后缀来执行此操作。这个变量的类型必须是float4.sampler2D _MainTex;float4 _MainTex_ST;平铺向量用于缩放纹理，因此默认为$(1, 1)$。 它存储在变量的XY部分。 要使用它只需将它与UV 坐标相乘。 这可以在顶点着色器或片段着色器中完成。 在顶点着色器中这样做是有意义的，只为每个顶点而不是每个像素执行乘法。Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.uv = v.uv * _MainTex_ST.xy; return i;}偏移向量用于移动纹理，并存储在变量的ZW部分中。i.uv = v.uv * _MainTex_ST.xy + _MainTex_ST.zw;UnityCG.cginc包含一个方便的宏:TRANSFORM_TEXi.uv = TRANSFORM_TEX(v.uv, _MainTex);纹理设置 默认的纹理设置 Wrap Mode决定了在使用0-1范围之外的UV坐标进行采样时的输出。 设置为clamped时，UV被限制在 0–1 范围内。 这意味着超出边缘的像素与位于边缘的像素相同。 设置为repeat时，UV从0-1循环。这意味着超出边缘的像素与纹理另一侧的像素相同。Wrap Mode默认模式是重复纹理，这会导致它平铺。 在(2, 2)开始平铺 Mipmap和Filter当纹理的像素与它们投影到网格的像素不完全匹配时会发生什么？ 存在不匹配，必须以某种方式解决。这是由Filter Mode完成如何控制。 Point (no filter) 。这意味着当在某些 UV 坐标处对纹理进行采样时，将使用最近的像素。 这将使纹理呈现块状外观，除非像素精确映射到显示像素。 因此，它通常用于像素完美的渲染，或者需要块状样式时。 bilinear filtering双线性过滤。 当纹理在两个像素之间的某处被采样时，这两个像素被插值。 由于纹理是 2D 的，这发生在 U 轴和 V 轴上。 因此是双线性过滤。双线性过滤方法在像素密度小于显示像素密度时有效，因此当放大纹理时。结果会看起来很模糊。 当缩小纹理时，几乎不起作用。 相邻的显示像素最终会得到相距超过一个像素的样本。 这意味着将跳过部分纹理，这将导致粗糙的过渡，就像图像被锐化一样。双线性过滤问题的解决方案是在像素密度变得太高时使用较小的纹理。 显示屏上出现的纹理越小，应使用的版本越小。 这些较小的版本称为 mipmap，unity会自动为您生成。 每个连续的 mipmap 的宽度和高度都是上一层的一半。 所以当原始纹理大小为 512x512 时，mip 贴图为 256x256、128x128、64x64、32x32、16x16、8x8、4x4 和 2x2。 mipmap 是什么？ mipmap这个词是 MIP map 的缩写。 字母 MIP 代表拉丁短语 multum in parvo ，意思 是狭小空间中的众多 。 它是由 Lance Williams 在首次描述 mipmap 技术时创造的。 mipmap上有下无 那么在何时使用哪个mipmap级别，它们看起来有什么不同呢？ 先通过在高级纹理设置中启用Fadeout Mip Maps。启用一个淡入淡出范围后，inspector将显示滑块。它定义了一个mipmap范围，在该范围内 mipmap 将转换为纯灰色。 越向右滑动过渡级别越小。 mipmap过渡级别 mipmap过渡之间呈现出模糊到锐利的快速，过渡不自然。这可以通过将过滤器模式切换为Trilinear三线性。 这与双线性过滤的工作方式相同，但它是在相邻的mipmap级别之间进行插值，这使得采样成本更高，但它平滑了 mipmap 级别之间的转换。另一种有用的技术是各向异性过滤。 当把Aniso Level设置为 0 时，纹理变得更加模糊。这与mipmap 级别的选择有关。 各向异性是什么意思？ 粗略地说，当某物在不同方向上看起来相似时，它就是各向同性的。 例如，一个无特征的立方体。 如果不是这种情况，则它是各向异性的。 例如，一个三角体，因为它的纹理朝着一个方向而不是另一个方向。当纹理以某个角度投影时，由于透视关系，通常最终会发现其中一个维度比另一个维度扭曲得更多。一个很好的例子是带纹理的地面。在远处，纹理的前后维度会显得比左右维度小得多。Aniso Level选择哪个 mipmap 级别基于最差维度。 如果差异很大，那么会得到一个在一维上非常模糊的结果。各向异性过滤通过解耦维度来缓解这种情况。除了统一缩小纹理外，它还提供在任一维度上缩放不同数量的版本。 因此，您不仅有 256x256 的 mipmap，而且还有 256x128、256x64 等的 mipmap。 没有和有Aniso Level 请注意，这些额外的 mipmap 不像常规 mipmap 那样预先生成。 相反，它们是通过执行额外的纹理样本来模拟的。 因此它们不需要更多空间，但采样成本更高。 各向异性双线性过滤，过渡到灰色 各向异性过滤的深度由 Aniso Level 控制。 为 0 时，它被禁用。 为 1 时，它变为启用并提供最小的效果。 16达到最大值。 但是，这些设置会受到项目质量设置的影响。 可以通过 Edit / Project Settings / Quality 访问质量设置。 找到 各向异性纹理 项目设置 渲染质量设置 项目设置禁用各向异性纹理时，无论纹理设置如何，都不会发生各向异性过滤。当它设置为 Per Texture时，它​​完全由纹理自身设置控制。也可以设置为 Forced On ，强制把每个纹理都开启Ansio Level，但是若纹理设置Aniso Level为0，仍然不会使用各向异性过滤。" }, { "title": "Unity Transform&Matrix(翻译一)", "url": "/posts/Unity-Matrix&Transform/", "categories": "翻译, Shader", "tags": "Unity3D, Shader", "date": "2018-01-01 09:00:00 +0800", "snippet": "本篇摘要信息 matrix介绍 matrix推导 模拟transform缩放 旋转 位移功能可视空间Unity Shader是怎么知道一个像素该画在哪个位置？下面是先展示一组Cube，一步步分析下去 cube数组 操控一组3维坐标创建一组10*10*10的3维Cube数组，并作为UnityMatrices对象的成员变量，接下来显示这些Cube在空间中的位置void InitCubeArray(){ for (int i =0 , z = 0; z &lt; generalCount; z++) { for (int y = 0; y &lt; generalCount; y++) { for (int x = 0; x &lt; generalCount; x++) { cubes[i++] = CreateCubesPoint(x, y, z); } } }}Transform CreateCubesPoint(int x, int y, int z){ GameObject cube = GameObject.CreatePrimitive(PrimitiveType.Cube); cube.transform.localScale = new Vector3(0.5f, 0.5f, 0.5f); cube.transform.localPosition = CreateCoordinate(x, y, z); cube.GetComponent&lt;MeshRenderer&gt;().material.color = CreateColor(x, y, z); return cube.transform;}设置每个Cube的位置，都以(0,0,0)为原点，(10-1)*0.5为Center左右两边对称Vector3 CreateCoordinate(int x, int y, int z){ return new Vector3( x - center, y - center, z - center );}然后再用自身坐标xyz分量与center的比率初始化颜色rgb。效果如上图Color CreateColor(int x, int y, int z){ return new Color( (float)x / generalCount, (float)y / generalCount, (float)z / generalCount );}空间变换positionning, rotating, and scalingCube数组中每个元素在空间中的变换有可能会有差异，虽然每个Cube变换的细节不同，但它们都需要经过某个方法来变换到空间中的某个坐标点。为此我们可以为所有变换创建一个abstract 基类，包含一个抽象的_Applay()_成员方法，由具体的变换组件去实现这个方法。public abstract class Transformation : MonoBehaviour{ public abstract Vector3 Apply(Vector3 point);}我们给这个UnityMatrices对象添加Transformation组件，同时检索Cube数组每个对象，将其坐标传入这个组件的_Apply()_方法进行计算得到新坐标并应用，这里始终以(0，0，0)作为每个Cube对象的原点坐标，而不能依赖其实际坐标，因为会每帧实时计算并改变。最后我们用泛型列表存储这种一系列变换组件方便统一计算。private void Update(){ GetComponents&lt;Transformation&gt;(transformations); // for (int i = 0; i &lt; cubes.Length; i++) // { // cubes[i].localPosition = TransformPoint(cubes[i].localPosition); // } for (int i =0 , z = 0; z &lt; generalCount; z++) { for (int y = 0; y &lt; generalCount; y++) { for (int x = 0; x &lt; generalCount; x++) { cubes[i++].localPosition = TransformPoint(x, y, z); } } }}Vector3 TransformPoint(int x, int y, int z){ Vector3 coordinates = CreateCoordinate(x, y, z); for (int i = 0; i &lt; transformations.Count; i++) { coordinates = transformations[i].Apply(coordinates); } return coordinates; }位移现在来做第一种变换：translation位移，这很简单。首先创建一个继承自Transformation组件子类，并定义一个表示自身位置属性的变量，并实现基类的抽象方法。然后添加给Cube数组对象public class PositionTransformation : Transformation{ public Vector3 position; public override Vector3 Apply(Vector3 point) { return point + position; }}现在可以向UnityMatrices对象添加PositionTransformation组件。这允许我们在不移动UnityMatrices对象的情况下移动数组中每个对象的坐标，所有的变换都发生在cube的局部空间。 位移 缩放接下来做第二种变换：Scaling缩放。public class ScaleTransformation : Transformation{ public Vector3 scale = new Vector3(1, 1, 1); public override Vector3 Apply(Vector3 point) { point.x *= scale.x; point.y *= scale.y; point.z *= scale.z; return point; }} 缩放 这里有一个问题：当进行缩放时，缩放会改变每个Cube对象的position。这是因为我们先计算了空间坐标，然后才缩放的它。而Unity中Transform组件是先缩放后位移。所以正确的计算顺序是：先缩放后位移。旋转(二维)第三种变换：Rotation旋转。public class RotationTransform : Transformation{ public Vector3 rotation; public override Vector3 Apply(Vector3 point) { return point;//先占位 }}旋转该如何工作呢？现在先假定在2维空间下一点P，绕Z轴旋转。Unity使用了左手坐标系，正向旋转是逆时针方向，如下图： 2维空间下绕Z轴旋转 旋转一个点坐标后会发什么吗？先简单的考虑一个以原点为中心的单位圆上的一点P，设p初始位置为(1,0)，然后再以每90°增量进行一次旋转，如下图： 0°旋转到90°和180°变化 由上图可知，点p(1,0)旋转一次(90°)变为了(0,1)，再旋转一次(180°)变为了(-1,0)，再往下旋转会变为(0,-1)，最后回到原位置(1,0). 那如果用点(0,1)作为初始位置，其变换顺序(0,1)$\\rightarrow$(-1,0)$\\rightarrow$(0,-1)$\\rightarrow$(1,0)$\\rightarrow$(0,1). 因此这个点坐标始终围绕0，1，0，-1进行循环，唯一得区别是起始点位置不同。那如果以45°增量进行旋转呢?它会在XY平面对角线上产生一点，其坐标为($\\pm \\sqrt{1 \\over 2},\\pm \\sqrt{1 \\over 2}$)，这些点到原点的距离始终是一致的。而这个循环顺序也类似上面，是$0, \\sqrt{1 \\over 2}, 1, \\sqrt{1 \\over 2}, 0, −\\sqrt{1 \\over 2}, −1, −\\sqrt{1 \\over 2}$。如果继续减小增量值，我们就可以得到一个Sine曲线。 Sine 和 Cosine曲线 结合上面两张图，Sine曲线代表了Y分量，Cosine曲线代表了X分量，坐标用曲线表示就是$(\\cos z, \\sin z)$，若起始点为(1,0)则结果为$(cosz,sinz)$，逆时针旋转90°后(根据$sin(-z) = –sin z, cos(-z) = cos z$的对称性质)则结果为$(−sin z,cos z)$。因此我们可以用绕Z轴计算sine和cosine曲线，由于提供的是角度，但实际上sin及cos只能作用于弧度，所以我们需要转化它:public override Vector3 Apply(Vector3 point){ float radz = rotation.z * Mathf.Deg2Rad; float sinz = Mathf.Sin(radz); float cosz = Mathf.Cos(radz); return point;} 什么是弧度? 像度数一样，可以用作旋转的度量。使用单位圆时，弧度与圆周行进的距离相等。由于圆周的长度等于2π乘以圆的半径，因此1度等于2π/360 = π/180弧度。π 是圆的周长与其直径之间的比率。上述方法对于旋转(1,0)或(0,1)或许很好，那有米有旋转任意点的方式呢？ 这些点都是由X和Y定义的，我们可以把2维点(x,y)拆分为一个公式$xX+yY$，那么$x(1,0)+y(0,1)=(x, y)$是成立的。当旋转之后，可以用$x(cos z, sin z)+y(-sin z, cos z)$来得到经过正确旋转后的点。组合为坐标就变成了$(xcosZ−ysinZ,xsinZ+ycosZ)$.public override Vector3 Apply(Vector3 point){ float radz = rotation.z * Mathf.Deg2Rad; float sinz = Mathf.Sin(radz); float cosz = Mathf.Cos(radz); //return point; return new Vector3( point.x * cosz - point.y * sinz, point.x * sinz + point.y * cosz, point.z );}按照上述分析的缩放、旋转、位移的先后计算顺序，再在Unity内对比Transform的拖动旋转缩放的显示，二者的效果是一致的。void Start (){ //... gameObject.AddComponent&lt;ScaleTransformation&gt;(); gameObject.AddComponent&lt;RotationTransform&gt;(); gameObject.AddComponent&lt;PositionTransformation&gt;();} 最终变换效果 旋转完全体现在我们只能绕Z轴旋转，但是为了能够复刻Unity的Transform组件那样的旋转，现在就得要支持绕X轴和绕Y轴旋转。虽然分别绕这些轴旋转与绕Z轴旋转的方法相似，但是当一次同时绕多个轴旋转时这就很复杂了。目标：一次同时绕多个轴旋转，迎难而上。2D矩阵现在开始，我们要把坐标书写格式由水平式替代为垂直式。把(x,y)被改写为\\(\\begin{bmatrix} x\\\\ y\\\\\\end{bmatrix}\\)把$(xcosZ−ysinZ,xsinZ+ycosZ)$也同样被拆分为\\(\\begin{bmatrix} xcosZ-ysinZ\\\\ xsinZ+ycosZ\\\\\\end{bmatrix}\\)，再把这个表达式进一步拆分：\\(\\begin{bmatrix} cosZ&amp;-sinZ\\\\ sinZ&amp;cosZ\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\\\end{bmatrix}\\)这就是矩阵乘法，2x2矩阵的第一列值代表X轴，第二列值代表了Y轴，计算公式如下：\\(\\begin{bmatrix} Xx&amp;Yx\\\\ Xy&amp;Yy\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} a\\\\ b\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} aXx + bYx\\\\ aXy + bYy\\\\\\end{bmatrix}\\) 由于Unity是采用左手法则，在上文中单位圆上一点绕Z轴旋转的增量度不同，cos代表X轴，sin代表Y轴，在结合本文的矩阵可得\\(\\begin{bmatrix} cos\\theta&amp;-sin\\theta\\\\ sin\\theta&amp;cos\\theta\\\\\\end{bmatrix}\\)$=cos\\theta$\\(\\begin{bmatrix} 1&amp;0\\\\ 0&amp;1\\\\\\end{bmatrix}\\)$+sin\\theta$\\(\\begin{bmatrix} 0&amp;-1\\\\ 1&amp;0\\\\\\end{bmatrix}\\)数学上定义，当两个矩阵相乘时，只有在第一个矩阵的列数（column）和第二个矩阵的行数（row）相同时才有意义。结果矩阵的每项元素等于第一个矩阵行元素与第二个矩阵列元素的乘积之和\\(\\begin{bmatrix} 1&amp;2\\\\ 3&amp;4\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} a&amp;c\\\\ b&amp;d\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} 1a+2b&amp;1c+2d\\\\ 3a+4b&amp;3c+4d\\\\\\end{bmatrix}\\) A矩阵 * B矩阵 = A矩阵的行 * B矩阵的列；只有当A矩阵列数 = B矩阵行数时，矩阵相乘才有效。因此结果矩阵的行数等于第一个矩阵的行，列数等于第二个矩阵的列相同。3D矩阵到目前为止，我们有了一个2x2阶矩阵，可以用这个矩阵来绕Z轴旋转一个2D点。但我们实际上使用的是3D坐标。若试图用这个矩阵乘法\\(\\begin{bmatrix} cosZ&amp;-sinZ\\\\ sinZ&amp;cosZ\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\\\end{bmatrix}\\)就是错误的，因为这两个矩阵的行与列的个数不匹配。为确保满足矩阵相乘，我们就需要填充这个第三维Z轴，先用0填充第一个矩阵第三行：\\(\\begin{bmatrix} cosZ&amp;-sinZ&amp;0\\\\ sinZ&amp;cosZ&amp;0\\\\ 0&amp;0&amp;0\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} xcosZ-ysinZ+0z\\\\ xsinZ+ycosZ+0z\\\\ 0x+0y+0z\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} xcosZ-ysinZ\\\\ xsinZ+ycosZ\\\\ 0\\\\\\end{bmatrix}\\)得到的结果中X轴和Y轴是正确的，但是Z轴结果总是为0。为了确保绕Z旋转而不改变Z轴的值，我们先插入一个数字1在旋转矩阵的右下角位置。简化理解，这个第三列值就是代表了Z轴：\\(\\begin{bmatrix} cosZ&amp;-sinZ&amp;0\\\\ sinZ&amp;cosZ&amp;0\\\\ 0&amp;0&amp;1\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} xcosZ-ysinZ\\\\ xsinZ+ycosZ\\\\ z\\\\\\end{bmatrix}\\) 由数学上定义，任何矩阵与单位矩阵相乘都等于本身，单位矩阵如同乘法中的1\\(\\begin{bmatrix} 1&amp;0&amp;0\\\\ 0&amp;1&amp;0\\\\ 0&amp;0&amp;1\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\\\end{bmatrix}\\)绕X轴和Y轴的旋转矩阵推导根据绕Z轴旋转的方式推理可以得出绕X轴和Y轴的旋转矩阵。以绕Y轴为例，首先，X轴是以\\(\\begin{bmatrix} 1\\\\ 0\\\\ 0\\\\\\end{bmatrix}\\)开始，经过逆时针旋转90°后以\\(\\begin{bmatrix} 0\\\\ 0\\\\ -1\\\\\\end{bmatrix}\\)结束。那么经过旋转后的X轴可以表示为\\(\\begin{bmatrix} cosY\\\\ 0\\\\ -sinY\\\\\\end{bmatrix}\\)而Z轴与X轴垂直，所以Z轴就是\\(\\begin{bmatrix} sinY\\\\ 0\\\\ cosY\\\\\\end{bmatrix}\\)而Y轴始终保持不变，最后绕Y轴的旋转矩阵：\\(\\begin{bmatrix} cosY&amp;0&amp;sinY\\\\ 0&amp;1&amp;0\\\\ -sinY&amp;0&amp;cosY\\\\\\end{bmatrix}\\)同理绕X轴的旋转矩阵， X轴不变：\\(\\begin{bmatrix} 1&amp;0&amp;0\\\\ 0&amp;cosX&amp;-sinX\\\\ 0&amp;sinX&amp;cosX\\\\\\end{bmatrix}\\)那么就此可以得出三个矩阵： 绕X轴旋转矩阵\\(\\begin{bmatrix} 1&amp;0&amp;0\\\\ 0&amp;cos\\alpha&amp;sin\\alpha\\\\ 0&amp;-sin\\alpha&amp;cos\\alpha\\\\\\end{bmatrix}\\) 绕Y轴旋转矩阵\\(\\begin{bmatrix} cos\\theta&amp;0&amp;-sin\\theta\\\\ 0&amp;1&amp;0\\\\ sin\\theta&amp;0&amp;cos\\theta\\\\\\end{bmatrix}\\) 绕Z轴旋转矩阵\\(\\begin{bmatrix} cos\\theta&amp;sin\\theta&amp;0\\\\ -sin\\theta&amp;cos\\theta&amp;0\\\\ 0&amp;0&amp;1\\\\\\end{bmatrix}\\)统一的旋转矩阵通过上文我们分别得到了单独绕某个轴的旋转矩阵，现在开始我们要组合起来使用。这里的同时旋转本质上也是分步进行的，先绕Z轴旋转，然后绕Y轴，最后是绕X轴。这里有两种算法：第一种：先计算坐标点绕Z旋转，得出的结果坐标再计算绕Y轴旋转，再得出的结果坐标计算绕X轴旋转，最后得到最终的旋转坐标。第二种：把每个旋转矩阵相乘得到一个最终的新的旋转矩阵，这将同时作用与三个轴旋转。首先计算Y乘Z，这个结果矩阵的第一项的值是$cosYcosZ−0sinZ−0sinY=cosYcosZ$，最终矩阵\\(\\begin{bmatrix} cosYcosZ&amp;-cosYsinZ&amp;sinY\\\\ sinZ&amp;cosZ&amp;0\\\\ -sinYcosZ&amp;sinYsinZ&amp;cosY\\\\\\end{bmatrix}\\)最后计算X × (Y × Z)得出最终矩阵：\\(\\begin{bmatrix} cosYcosZ&amp;-cosYsinZ&amp;sinY\\\\ cosXsinZ+sinXsinYcosZ&amp;cosXcosZ-sinXsinYsinZ&amp;-sinXcosY\\\\ sinXsinZ-cosXsinYcosZ&amp;sinXcosZ+cosXsinYsinZ&amp;cosXcosY\\\\\\end{bmatrix}\\)public Vector3 rotation;//每个分量表示角度public int rotDelta; private void Update(){ rotation = new Vector3(rotDelta, rotDelta, rotDelta);} public override Vector3 Apply(Vector3 point){ float radx = rotation.x * Mathf.Deg2Rad; float rady = rotation.y * Mathf.Deg2Rad; float radz = rotation.z * Mathf.Deg2Rad; float sinx = Mathf.Sin(radx); float cosx = Mathf.Cos(radx); float siny = Mathf.Sin(rady); float cosy = Mathf.Cos(rady); float sinz = Mathf.Sin(radz); float cosz = Mathf.Cos(radz); Vector3 xRot = new Vector3( cosy * cosz, cosx * sinz + sinx * siny * cosz, sinx * sinz - cosx * siny * cosz ); Vector3 yRot = new Vector3( -cosy * sinz, cosx * cosz - sinx * siny * sinz, sinx * cosz + cosx * siny * sinz ); Vector3 zRot = new Vector3( siny, -sinx * cosy, cosx * cosy ); return xRot * point.x + yRot * point.y + zRot * point.z;}矩阵变换实现一个矩阵完成缩放、旋转、位移的计算为了实现这个目标，所以借鉴3.4旋转矩阵组合的方式，先对缩放和位移组合，即位移 x 缩放。缩放，根据单位矩阵的性质，任何矩阵与单位矩阵相乘的结果都是本身。那么对单位矩阵进行缩放即可：\\(\\begin{bmatrix} 2&amp;0&amp;0\\\\ 0&amp;3&amp;0\\\\ 0&amp;0&amp;4\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} 2x\\\\ 3y\\\\ 4z\\\\\\end{bmatrix}\\)位移，不是对三个分量完全重新计算，而是在现有的坐标之上进行偏移。因此现在不能简单的重新表示为3x3阶矩阵，而是需要额外增加一列表示偏移。\\(\\begin{bmatrix} 1&amp;0&amp;0&amp;2\\\\ 0&amp;1&amp;0&amp;3\\\\ 0&amp;0&amp;1&amp;4\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} x+2\\\\ y+3\\\\ z+4\\\\\\end{bmatrix}\\)但是，又由于矩阵乘法规定，第一个矩阵的列数等于第二个矩阵的行数才有意义。上图就是错误的。所以我们需要给坐标矩阵增加第四个元素，偏移矩阵增加一行。当它们增加的这个分量进行矩阵相乘时，其结果为1(我们先保留下这个数字1，以备后续使用).那就变成了4x4阶矩阵样式和一个4D点。\\(\\begin{bmatrix} 1&amp;0&amp;0&amp;2\\\\ 0&amp;1&amp;0&amp;3\\\\ 0&amp;0&amp;1&amp;4\\\\ 0&amp;0&amp;0&amp;0\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ 1\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} x+2\\\\ y+3\\\\ z+4\\\\ 1\\\\\\end{bmatrix}\\)根据位移矩阵，所以我们要统一用4×4的变换矩阵。缩放和旋转矩阵会额外增加一行一列，其右下角是1。所有的点都带有一个第四维坐标分量，它总是1——其次坐标。其次坐标(Homogeneous Coordinates)不知道就问： 这个坐标的第四分量坐标是个啥？ 它有啥用啊？ 我们只知道上文提到位移时有用，那么缩放、旋转有用吗？ 当它的值为0，1，-1时会发生什么呢？有这样一个东西不叫坐标而叫向量，它可以被缩放和旋转，但不能被移动。向量描述了相对于某个点的偏移，具有方向和长度属性，没有位置属性。它\\(\\begin{bmatrix}x\\\\y\\\\z\\\\1\\\\\\end{bmatrix}\\)表示为一个点，而它\\(\\begin{bmatrix}x\\\\y\\\\z\\\\0\\\\\\end{bmatrix}\\)表示为一个向量。这样区分非常有用，因为我们可以使用相同的矩阵来变换一个点的位置、法线和切线。当第四个坐标值是0或1或其他数值时会发生什么？答案是什么也不会，准确的说是没有差异。这个坐标的术语叫做其次坐标，它的意思是空间中每个点都可以用一个无穷数量坐标集和来表示。而现在普遍做法的形式是使用1作为第四个坐标值，所有其他的数字都能通过使用整个集合乘以任意数来找到\\(\\begin{bmatrix}x\\\\y\\\\z\\\\1\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix}2x\\\\2y\\\\2z\\\\2\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix}3x\\\\3y\\\\3z\\\\3\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix}wx\\\\wy\\\\wz\\\\w\\\\\\end{bmatrix}\\)=$w$\\(\\begin{bmatrix}x\\\\y\\\\z\\\\1\\\\\\end{bmatrix}\\)当我们知道了一个其次坐标时，需要转为3D坐标，只需要把第四个坐标化为1，怎么做呢？没错，就是把每个坐标除以第四个坐标，然后再舍弃第四个坐标\\(\\begin{bmatrix}x\\\\y\\\\z\\\\w\\\\\\end{bmatrix}\\)=$1 \\over w$\\(\\begin{bmatrix}x\\\\y\\\\z\\\\w\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix}x \\over w\\\\y \\over w\\\\z \\over w\\\\1\\\\\\end{bmatrix}\\)$\\rightarrow$\\(\\begin{bmatrix}x \\over w\\\\y \\over w\\\\z \\over w\\\\\\end{bmatrix}\\)所以当第四个坐标为0时是不能做上面的除法的，因此当第四个坐标值为0时，表示为向量，这就是为什么它们像方向一样。使用矩阵我们能用Unity的Matrix4x4结构体来完成矩阵乘法。从现在开始，我们将用它来代替上面的3D旋转方法。在Transformation增加一个抽象只读属性以检索变换矩阵。public abstract Matrix4x4 Matrix { get; }Transformation组件的Apply方法不再需要设为抽象，它将获取到矩阵并执行乘法运算。public Vector3 Apply (Vector3 point) { return Matrix.MultiplyPoint(point);}注意这个Matrix4x4.MultiplyPoint需要一个3D坐标参数，坐标参数假定了第四个值为1.该方法会负责把得到的其次坐标转为3D坐标，若只想计算方向向量可以使用Matrix4x4.MultiplyVector.该方法会忽略第四个坐标。public Vector3 MultiplyPoint(Vector3 v){ Vector3 vector; vector.x = (((this.m00 * v.x) + (this.m01 * v.y)) + (this.m02 * v.z)) + this.m03; vector.y = (((this.m10 * v.x) + (this.m11 * v.y)) + (this.m12 * v.z)) + this.m13; vector.z = (((this.m20 * v.x) + (this.m21 * v.y)) + (this.m22 * v.z)) + this.m23; float num = (((this.m30 * v.x) + (this.m31 * v.y)) + (this.m32 * v.z)) + this.m33;//其次坐标 num = 1f / num; vector.x *= num;//转换计算 vector.y *= num;//转换计算 vector.z *= num;//转换计算 return vector;} public Vector3 MultiplyVector(Vector3 v){ Vector3 vector; vector.x = ((this.m00 * v.x) + (this.m01 * v.y)) + (this.m02 * v.z); vector.y = ((this.m10 * v.x) + (this.m11 * v.y)) + (this.m12 * v.z); vector.z = ((this.m20 * v.x) + (this.m21 * v.y)) + (this.m22 * v.z); return vector;}具体的Transformation类现在必须将其Apply()方法更改为Matrix属性。首先是PositionTransformation组件，Matrix4x4.SetRow接口能很简易地填充这个矩阵。public override Matrix4x4 Matrix { get { Matrix4x4 matrix = new Matrix4x4(); matrix.SetRow(0, new Vector4(1f, 0f, 0f, position.x)); matrix.SetRow(1, new Vector4(0f, 1f, 0f, position.y)); matrix.SetRow(2, new Vector4(0f, 0f, 1f, position.z)); matrix.SetRow(3, new Vector4(0f, 0f, 0f, 1f)); return matrix; }}其次是ScaleTransformation.public override Matrix4x4 Matrix { get { Matrix4x4 matrix = new Matrix4x4(); matrix.SetRow(0, new Vector4(scale.x, 0f, 0f, 0f)); matrix.SetRow(1, new Vector4(0f, scale.y, 0f, 0f)); matrix.SetRow(2, new Vector4(0f, 0f, scale.z, 0f)); matrix.SetRow(3, new Vector4(0f, 0f, 0f, 1f)); return matrix; }}最后是RotationTransformation, 它设置行与列就更简单了，把之前的方法改改就能用。public override Matrix4x4 Matrix { get { float radx = rotation.x * Mathf.Deg2Rad; float rady = rotation.y * Mathf.Deg2Rad; float radz = rotation.z * Mathf.Deg2Rad; float sinx = Mathf.Sin(radx); float cosx = Mathf.Cos(radx); float siny = Mathf.Sin(rady); float cosy = Mathf.Cos(rady); float sinz = Mathf.Sin(radz); float cosz = Mathf.Cos(radz); Matrix4x4 matrix = new Matrix4x4(); matrix.SetColumn(0, new Vector4( cosy * cosz, cosx * sinz + sinx * siny * cosz, sinx * sinz - cosx * siny * cosz, 0f )); matrix.SetColumn(1, new Vector4( -cosy * sinz, cosx * cosz - sinx * siny * sinz, sinx * cosz + cosx * siny * sinz, 0f )); matrix.SetColumn(2, new Vector4( siny, -sinx * cosy, cosx * cosy, )); matrix.SetColumn(3, new Vector4(0f,0f,0f,1f)); return matrix; }}合并矩阵现在我们把上述所有变换矩阵合并为一个矩阵。 为此先在UnityMatrices类增加一个矩阵类型字段transformation。我们将在Update函数每帧更新该变量值，该步骤为先获取到第一个Transformation组件的矩阵，并依次与其他矩阵相乘，需要确保这块正确的相乘顺序。private void Update() { UpdateTransformation(); for (int i =0 , z = 0; z &lt; generalCount; z++) { //... } }void UpdateTransformation() { GetComponents&lt;Transformation&gt;(transformations); if(transformations.Count &gt; 0) { transformation = transformations[0].Matrix; for (int i = 1; i &lt; transformations.Count; i++) { transformation = transformations[i].Matrix * transformation; } }}最后不再执行Apply方法，而改用矩阵乘法代替：Vector3 TransformPoint(int x, int y, int z){ Vector3 coordinates = CreateCoordinate(x, y, z); // for (int i = 0; i &lt; transformations.Count; i++) // { // coordinates = transformations[i].Apply(coordinates); // } return transformation.MultiplyPoint(coordinates);;}这个新方法是非常有效的，因为我们之前使用的方法是分别给每个点乘一个变换矩阵。而现在我们只需要一次创建一个统一的变换矩阵作用与所有点。Unity使用类似的方案将每个对象的变换层次结构简化为单个变换矩阵。在这个例子中，我们可以使它更有效。所有的变换矩阵都有一个相同的行——[0 0 0 1]。知道了这一点，我们可以忽略这一行，跳过所有0的计算和最后的除法转换。Matrix4x4.MultiplayPoint3x4方法就是这样做的。public Vector3 MultiplyPoint3x4(Vector3 v){ Vector3 vector; vector.x = (((this.m00 * v.x) + (this.m01 * v.y)) + (this.m02 * v.z)) + this.m03; vector.y = (((this.m10 * v.x) + (this.m11 * v.y)) + (this.m12 * v.z)) + this.m13; vector.z = (((this.m20 * v.x) + (this.m21 * v.y)) + (this.m22 * v.z)) + this.m23; return vector;}这个方法有时候有用，有时候不能用。因为有时我们需要的一些变换矩阵会改变这最后一行。到目前为止只有位移变换需要第四行。所以缩放、旋转使用Matrix4x4.MultiplayPoint3x4计算速度会更快。现在就把Apply()方法改为虚方法，再由旋转、缩放组件重写，代码就不贴了。3D到2D投影矩阵到目前为止，我们能够把一个点的坐标从一个3D空间变换到另一个3D空间。但是这些点又如何展示到2D空间呢？这肯定需要一个从3D到2D的变换矩阵。那么我们开始寻找这个矩阵吧！先构造一个新的继承自Transformation的实体变换组件作用于摄像机的投影，默认值为单位矩阵。public class CameraTransformation : Transformation{ public override Matrix4x4 Matrix { get { Matrix4x4 matrix = new Matrix4x4(); matrix.SetRow(0, new Vector4(1f, 0f, 0f, 0f)); matrix.SetRow(1, new Vector4(0f, 1f, 0f, 0f)); matrix.SetRow(2, new Vector4(0f, 0f, 1f, 0f)); matrix.SetRow(3, new Vector4(0f, 0f, 0f, 1f)); return matrix; } }}正交相机Orthographic Camera从3D变换到2D空间最直接粗暴的方式是丢弃一个维度数据。就好像把3维空间压缩到2维平面，这个平面就像一个画布，用来渲染屏幕。现在我们把Z轴丢弃，试试看会发生什么\\[\\begin{bmatrix} 1&amp;0&amp;0&amp;0\\\\ 0&amp;1&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;1\\\\\\end{bmatrix}\\]把代码修改为public class CameraTransformation : Transformation{ public override Matrix4x4 Matrix { get { Matrix4x4 matrix = new Matrix4x4(); matrix.SetRow(0, new Vector4(1f, 0f, 0f, 0f)); matrix.SetRow(1, new Vector4(0f, 1f, 0f, 0f)); matrix.SetRow(2, new Vector4(0f, 0f, 0f, 0f)); matrix.SetRow(3, new Vector4(0f, 0f, 0f, 1f)); return matrix; } }}$\\rightarrow$ $\\rightarrow$ 3d转换到2d 实际上，这种粗暴的方法还蛮像那么回事，确实变成了2D了。其他的X、Y轴同理，就不演示了。这就是正交投影。不管相机如何缩放、旋转、位移，始终呈现的2D效果。移动相机的视觉效果和移动世界的相反方向是一致的，也就是3个变换组件的变量与摄像机的缩放、旋转、位移变量是互为正负关系。透视相机Perspective Camera正交相机不能模拟3D世界就很尴尬。所以我们需要一个透视相机，由于视角的原因，呈现一个原小近大的视觉。那么基于此，我们可以根据点到摄像机的距离重建这个视觉效果。以Z轴为例，把单位矩阵代表Z轴的列元素全部置0，再把单位矩阵最后一行改为[0,0,1,0]，这步改变将确保结果坐标的第四个值等于Z坐标，最后所有坐标都除以Z\\(\\begin{bmatrix} 1&amp;0&amp;0&amp;0\\\\ 0&amp;1&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;1&amp;0\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ 1\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} x\\\\ y\\\\ 0\\\\ z\\\\\\end{bmatrix}\\)$\\rightarrow$\\(\\begin{bmatrix} x \\over z\\\\ y \\over z\\\\ 0\\\\\\end{bmatrix}\\)public override Matrix4x4 Matrix { get { Matrix4x4 matrix = new Matrix4x4(); matrix.SetRow(0, new Vector4(1f, 0f, 0f, 0f)); matrix.SetRow(1, new Vector4(0f, 1f, 0f, 0f)); matrix.SetRow(2, new Vector4(0f, 0f, 0f, 0f)); matrix.SetRow(3, new Vector4(0f, 0f, 1f, 0f)); return matrix; }}与正交投影最大的不同是这些点不会直接移向到平面，而是他们会移向摄像机的位置，当然这只对位于摄像机前面的点有效，而在摄像机后面的点就不会正确的投影。先确保所有点都能位于摄像机的前方，把摄像机的Unity.Transform组件Position.Z值调好，保证所有点都先可见。$\\rightarrow$ 透视投影 设置一个点到平面的投影距离，它也会影响投影视觉效果。它就像相机的焦距，值越大视野就越小。现在我们先定义一个变量focalLength值默认为1，这能产生90°的视野。public float focalLength = 1f;当这个focalLength值越大就像相机在进行聚焦，这有效地增加了所有点的比例(想象一下相机变焦)。当我们压缩Z轴时，是不必进行缩放的\\(\\begin{bmatrix} fl&amp;0&amp;0&amp;0\\\\ 0&amp;fl&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;1&amp;0\\\\\\end{bmatrix}\\)\\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ 1\\\\\\end{bmatrix}\\)=\\(\\begin{bmatrix} xfl\\\\ yfl\\\\ 0\\\\ z\\\\\\end{bmatrix}\\)$\\rightarrow$\\(\\begin{bmatrix} xfl \\over z\\\\ yfl \\over z\\\\ 0\\\\\\end{bmatrix}\\)public override Matrix4x4 Matrix { get { Matrix4x4 matrix = new Matrix4x4(); matrix.SetRow(0, new Vector4(focalLength, 0f, 0f, 0f)); matrix.SetRow(1, new Vector4(0f, focalLength, 0f, 0f)); matrix.SetRow(2, new Vector4(0f, 0f, 0f, 0f)); matrix.SetRow(3, new Vector4(0f, 0f, 1f, 0f)); return matrix; }}现在有了一个简单的透视相机，如果要完全模拟Unity的透视相机，我们还必须处理近平面和远平面。这将需要处理投影到一个立方体而不是一个平面，因此需要保留深度信息。然后还有视野裁切方面的问题。此外，Unity的摄像头是在负Z方向上拍摄的，这需要对一些数字进行求负。矩阵不可怕。" }, { "title": "CG函数标准库", "url": "/posts/CG-function-library/", "categories": "Unity3D, Shader", "tags": "CG", "date": "2017-10-03 20:17:00 +0800", "snippet": "数学函数（Mathematical Functions）下表中列举了 Cg 标准函数库中所有的数学函数， 这些数学函数用于执行数学上常用计算，包括：三角函数、幂函数、园函数、向量和矩阵的操作函数。这些函数都被重载，以支持标量数据和不同长度的向量作为输入参数。 函数 功能 abs(x) 返回输入参数的绝对值 all(x) 如果输入参数均不为 0，则返回 ture；否则返回 flase。 &amp;&amp;运算 any(x) 输入参数只要有其中一个不为 0，则返回true。 clamp(x,a,b) 如果 x 值小于 a，则返回 a；如果 x 值大于 b，返回 b；否则返回 x degrees(x) 输入参数为弧度值(radians)， 函数将其转换为角度值(degrees) determinant(m) 计算矩阵的行列式因子 cross(A,B) 返回两个三元向量的叉积(cross product)。注意，输入参数必须是三元向量！ dot(A,B) 返回 A 和 B 的点积(dot product)。参数 A 和 B可以是标量，也可以是向量（输入参数方面，点积和叉积函数有很大不同） exp(x) 计算 ex 的值， e= 2.71828182845904523536 exp2(x) 计算 2x 的值 floor(x) 向下取整。例如 floor(float(1.3))返回的值为 1.0；但是floor(float(-1.3))返回的值为-2.0。该函数与 ceil(x)函数相对应 ceil(x) 对输入参数向上取整。 例如：ceil(float(1.3)) ，返回值为2.0 fmod(x,y) 返回 x/y 的余数。如果 y 为 0， 结果不可预料 frac(x) 返回标量或每个向量分量的小数部分 frexp(x, out exp) 将浮点数 x 分解为尾数和指数，即x = m* 2^exp， 返回 m， 并将指数存入exp 中；如果 x 为 0，则尾数和指数都返回 0 isfinite(x) 判断标量或者向量中的每个数据是否是有限数，如果是返回true；否则返回 false;无限的或者非数据(not-a-number NaN) isinf(x) 判断标量或者向量中的每个数据是否是无限，如果是返回 true；否则返回 false isnan(x) 判断标量或者向量中的每个数据是否是非数据(not-a-number NaN)，如果是返回 true；否则返回 false ldexp(x, n) 计算$x*2n$的值 lerp(a, b, f) 计算$ a+(b-a)f $ 或者$ (1-f)a+f*b $ 的值。即在下限 a 和上限 b 之间进行插值， f 表示权值。注意，如果 a 和 b 是向量，则权值f必须是标量或者等长的向量 lit(N•dot•L,N•dot•H, m) Blinn–Phong反射公式,N表示法向量，L表示入射光向量；H表示半角向量；m表示高光系数；N • L代表散射光的贡献，如果其值 &lt; 0 ，则为 0。Z 位代表镜面光的贡献，如果 N • L &lt; 0 或者N•H &lt; 0 ，则位 0；否则为 (N • L)m。函数计算环境光、散射光、镜面光的贡献，返回的 4 元向量 log(x) 计算 ln( x) 的值， x 必须大于 0 log2(x) 计算 log2(x) 的值， x 必须大于 0 log10(x) 计算 log10 (x) 的值， x 必须大于 0 max(a, b) 比较两个标量或等长向量元素，返回最大值 min(a,b) 比较两个标量或等长向量元素，返回最小值 modf(x, out ip) 将浮点数num分解成整数部分和小数部分，返回小数部分，将整数部分存入ip。不常用 mul(M, N) 计算两个矩阵相乘，如果 M 为 AxB 阶矩阵，N 为 BxC 阶矩阵，则返回AxC 阶矩阵。下面两个函数为其重载函数 mul(M, v) 计算矩阵和向量相乘 mul(v, M) 计算向量和矩阵相乘 noise(x) 噪声函数，返回值始终在 0， 1 之间；对于同样的输入，始终返回相同的值（也就是说，并不是真正意义上的随机噪声） pow(x, y) $ x^y $ radians(x) 函数将角度值转换为弧度值 round(x) 即四舍五入 sqrt(x) 求 x 的平方根， x ， x 必须大于 0 rsqrt(x) 平方根倒数$ y= \\frac{1}{\\sqrt{x}} $，x 必须大于 0 saturate(x) 如果 x 小于 0，返回 0；如果 x 大于 1，返回1；否则，返回 x sign(x) 如果 x 大于 0，返回 1；如果 x 小于 0，返回01；否则返回 0 sin(x) 输入参数为弧度，计算正弦值，返回值范围为[-1,1] cos(x) 返回弧度 x 的余弦值。返回值范围为[-1,1] tan(x) 输入参数为弧度，计算正切值 acos(x) 反余切函数，输入参数范围为[-1,1]，返回[0,π ]区间的角度值 asin(x) 反正弦函数,输入参数取值区间为[-1,1]，返回角度值范围为 atan(x) 反正切函数，返回角度值范围为 atan2(y,x) 计算$\\frac{y}{x}$ 的反正切值。实际上和 atan(x)函数功能完全一样，至少输入参数不同。 atan(x)= atan2(x, float(1)) sincos(float x,out s, out c) 该函数是同时计算 x 的 sin 值和 cos 值，其中s=sin(x)，c=cos(x)。该函数用于”同时需要计算 sin 值和 cos 值的情况”，比分别运算要快很多 sinh(x) 计算双曲正弦（hyperbolic sine）值 cosh(x) 双曲余弦（hyperbolic cosine）函数，计算 x的双曲余弦值 tanh(x) 计算双曲正切值 smoothstep(min,max, x) 值 x 位于 min、 max 区间中。如果x=min，返回 0；如果x=max，返回 1；如果 x 在两者之间，按照下列公式返回数据 step(a, x) 如果 x&lt;a，返回 0；否则，返回 1 transpose(M) M 为矩阵，计算其转置矩阵 几何函数（Geometric Functions）如下表所示，几何函数用于执行和解析几何相关的计算，例如根据入射光向量和顶点法向量，求取反射光和折射光方向向量。 Cg 语言标准函数库中有3个几何函数会经常被使用到分别是 normalize 函数，对向量进行归一化 reflect函数，计算反射光方向向量 refract 函数，计算折射光方向向量 着色程序中的向量最好进行归一化之后再使用，否则会出现难以预料的错误. reflect 函数和 refract 函数都存在以”入射光方向向量”作为输入参数，注意这两个函数中使用的入射光方向向量，是从外指向几何顶点的；平时我们在着色程序中或者在课本上都是将入射光方向向量作为从顶点出发. 函数 功能 distance( pt1, pt2) 两点之间的欧几里德距离（Euclidean distance） faceforward(N,I,Ng) 如果 Ng • I &lt; 0 ，返回 N；否则返回-N length(v) 返回一个向量的模，即 sqrt(dot(v,v)) normalize( v) 归一化向量 reflect(I, N) 根据入射光方向向量 I，和顶点法向量N，计算反射光方向向量。其中 I 和 N必须被归一化，需要非常注意的是，这个 I 是指向顶点；函数只对三元向量有效。 refract(I,N,eta) 计算折射向量， I 为入射光线， N 为法向量， eta 为折射系数；其中 I 和 N 必须被归一化， 如果 I 和 N 之间的夹角太大，则返回（0， 0， 0），也就是没有折射光线； I 是指向顶点的；函数只对三元向量有效 纹理映射函数（texture Map Functions）下表提供 Cg 标准函数库中的纹理映射函数。这些函数被 ps_2_0、 ps_2_x、arbfp1、 fp30 和fp40 等 profiles 完全支持（fully supported）。所有的这些函数返回四元向量值。 函数 功能 tex1D (sampler1D tex, float s)一维纹理查询 tex1D(sampler1D tex, float s, float dsdx, float dsdy) 使用导数值（derivatives）查询一维纹理 tex1D(sampler1D tex, float2 sz) 一维纹理查询，并进行深度值比较 tex1D(sampler1D tex, float2 sz, float dsdx,float dsdy) 使用导数值（derivatives）查询一维纹理， 并进行深度值比较 tex1Dproj(sampler1D tex, float2 sq) 一维投影纹理查询 tex1Dproj(sampler1D tex, float3 szq) 一维投影纹理查询，并比较深度值 tex2D(sampler2D tex, float2 s) 二维纹理查询—采样 tex2D(sampler2D tex, float2 s, float2 dsdx, float2 dsdy) 使用导数值（derivatives）查询二维纹理 tex2D(sampler2D tex, float3 sz) 二维纹理查询，并进行深度值比较 tex2D(sampler2D tex, float3 sz, float2 dsdx,float2 dsdy) 使用导数值（derivatives）查询二维纹理，并进行深度值比较 tex2Dproj(sampler2D tex, float3 sq) 二维投影纹理查询 tex2Dproj(sampler2D tex, float4 szq) 二维投影纹理查询，并进行深度值比较 texRECT(samplerRECT tex, float2 s) 二维非投影矩形纹理查询（OpenGL独有） texRECT (samplerRECT tex, float2 s, float2 dsdx, float2 dsdy) 二维非投影使用导数的矩形纹理查询（OpenGL独有） texRECT (samplerRECT tex, float3 sz) 二维非投影深度比较矩形纹理查询（OpenGL独有） texRECT (samplerRECT tex, float3 sz, float2 dsdx,float2 dsdy) 二维非投影深度比较并使用导数的矩形纹理查询（OpenGL独有） texRECT proj(samplerRECT tex, float3 sq) 二维投影矩形纹理查询（OpenGL独有） texRECT proj(samplerRECT tex, float3 szq) 二维投影矩形纹理查询（OpenGL独有） tex3D(sampler3D tex, float s) 三维纹理查询 tex3D(sampler3D tex, float3 s, float3 dsdx, float3 dsdy) 结合导数值（derivatives）查询三维纹理 tex3Dproj(sampler3D tex, float4 szq) 查询三维投影纹理，并进行深度值比较 texCUBE(samplerCUBE tex, float3 s) 查询立方体纹理 texCUBE(samplerCUBE tex, float3 s, float3 dsdx, float3 dsdy) 结合导数值（derivatives）查询立方体纹理 texCUBEproj(samplerCUBE tex, float4 sq) 查询投影立方体纹理 偏导函数（Derivative Functions） 函数 功能 ddx(a) 参数 a 对应一个像素位置， 返回该像素值在 X 轴上的偏导数 ddy(a) 参数 a 对应一个像素位置， 返回该像素值在 X 轴上的偏导数 调试函数（Debugging Function） 函数 功能 void debug(float4 x) 如果在编译时设置了DEBUG，片段着色程序中调用该函数可以将值x作为COLOR 语义的最终输出；否则该函数什么也不做 " }, { "title": "Markdown语法集合速查表", "url": "/posts/markdown-cheat-sheet/", "categories": "随笔, Markdown", "tags": "Markdown", "date": "2016-03-17 11:17:00 +0800", "snippet": "Thanks for visiting The Markdown Guide!This Markdown cheat sheet provides a quick overview of all the Markdown syntax elements. It can’t cover every edge case, so if you need more information about any of these elements, refer to the reference guides for basic syntax and extended syntax.Basic SyntaxThese are the elements outlined in John Gruber’s original design document. All Markdown applications support these elements.HeadingH1H2H3Boldbold textItalicitalicized textBlockquote blockquoteOrdered List First item Second item Third itemUnordered List First item Second item Third itemCodecodeHorizontal RuleLinkMarkdown GuideImage&lt;center class=\"half\"&gt; &lt;img src=\"\" width=\"15%\" /&gt; **&lt;/center&gt; *居中描述*Extended SyntaxThese elements extend the basic syntax by adding additional features. Not all Markdown applications support these elements.Table Syntax Description Header Title Paragraph Text :— 或者| 代表左对齐:–: 代表居中对齐—: 代表右对齐Fenced Code Block{ \"firstName\": \"John\", \"lastName\": \"Smith\", \"age\": 25}FootnoteHere’s a sentence with a footnote. 1Heading IDMy Great HeadingDefinition List term definitionStrikethroughThe world is flat.Task List Write the press release Update the website Contact the mediaEmojiThat is so funny! :joy:(See also Copying and Pasting Emoji)HighlightI need to highlight these ==very important words==.SubscriptH~2~OSuperscriptX^2^MathematicsThe mathematics powered by MathJax:数学公式符号两个$$包围，公式会另起一行$$ a + b = c$$例如：\\(a + b = c\\)例如：\\[{\\sum_{n=1}^\\infty} {1 \\over n^2} = \\frac{\\pi^2}{6}\\]一个$包围，公式与文字同一行When $a \\ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are例如：When $a \\ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are\\[x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}\\] 两个$$ xxx $$包围住，数学公式会换行. 一个$ xxx $包围住，数学公式不换行，如When $a \\ne 0$.&gt; 一个红色提示{: .prompt-danger } 一个红色提示- gem \"jekyll-theme-chirpy\", \"~&gt; 3.2\", \"&gt;= 3.2.1\"+ gem \"jekyll-theme-chirpy\", \"~&gt; 3.3\", \"&gt;= 3.3.0\"矩阵公式1、矩阵 数学公式放在 $$ 之间 起始标记 \\begin{matrix}，结束标记 \\end{matrix} 每一行末尾标记 \\，行间元素之间用 &amp; 分隔$$\\begin{matrix}0&amp;1&amp;1\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\\\end{matrix}$$表现如下：\\(\\begin{matrix}0&amp;1&amp;1\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\\\end{matrix}\\)2、矩阵边框 在起始、结束标记用下列词替换 matrix pmatrix：小括号边框 bmatrix：中括号边框$\\begin{bmatrix}0&amp;1&amp;11&amp;1&amp;01&amp;0&amp;1\\end{bmatrix}$ Bmatrix：大括号边框 vmatrix：单竖线边框 Vmatrix：双竖线边框表现如下：\\(\\begin{pmatrix}0&amp;1&amp;1\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\\\end{pmatrix}\\)\\[\\begin{bmatrix}0&amp;1&amp;1\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\\\end{bmatrix}\\]\\[\\begin{Bmatrix}0&amp;1&amp;1\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\\\end{Bmatrix}\\]\\[\\begin{vmatrix}0&amp;1&amp;1\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\\\end{vmatrix}\\]\\[\\begin{Vmatrix}0&amp;1&amp;1\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\\\end{Vmatrix}\\]3、省略元素 横省略号：\\cdots 竖省略号：\\vdots 斜省略号：\\ddots$$\\begin{bmatrix}{a_{11}}&amp;{a_{12}}&amp;{\\cdots}&amp;{a_{1n}}\\\\{a_{21}}&amp;{a_{22}}&amp;{\\cdots}&amp;{a_{2n}}\\\\{\\vdots}&amp;{\\vdots}&amp;{\\ddots}&amp;{\\vdots}\\\\{a_{m1}}&amp;{a_{m2}}&amp;{\\cdots}&amp;{a_{mn}}\\\\\\end{bmatrix}$$表现如下：\\(\\begin{bmatrix}{a_{11}}&amp;{a_{12}}&amp;{\\cdots}&amp;{a_{1n}}\\\\{a_{21}}&amp;{a_{22}}&amp;{\\cdots}&amp;{a_{2n}}\\\\{\\vdots}&amp;{\\vdots}&amp;{\\ddots}&amp;{\\vdots}\\\\{a_{m1}}&amp;{a_{m2}}&amp;{\\cdots}&amp;{a_{mn}}\\\\\\end{bmatrix}\\)4、阵列 需要array环境：起始、结束处以{array}声明 对齐方式：在{array}后以{}逐行统一声明 左对齐：l；居中：c；右对齐：r 竖直线：在声明对齐方式时，插入 建立竖直线 插入水平线：\\hline$$\\begin{array}{c|lll}{↓}&amp;{a}&amp;{b}&amp;{c}\\\\\\hline{R_1}&amp;{c}&amp;{b}&amp;{a}\\\\{R_2}&amp;{b}&amp;{c}&amp;{c}\\\\\\end{array}$$表现如下：\\[\\begin{array}{c|lll}{↓}&amp;{a}&amp;{b}&amp;{c}\\\\\\hline{R_1}&amp;{c}&amp;{b}&amp;{a}\\\\{R_2}&amp;{b}&amp;{c}&amp;{c}\\\\\\end{array}\\]5、方程组 需要cases环境：起始、结束处以{cases}声明 $$\\begin{cases}a_1x+b_1y+c_1z=d_1\\\\a_2x+b_2y+c_2z=d_2\\\\a_3x+b_3y+c_3z=d_3\\\\\\end{cases}$$ \\(\\begin{cases}a_1x+b_1y+c_1z=d_1\\\\a_2x+b_2y+c_2z=d_2\\\\a_3x+b_3y+c_3z=d_3\\\\\\end{cases}\\) 30^\\circ\t30∘” role=”presentation”&gt;30∘30∘\t\\bot\t⊥” role=”presentation”&gt;⊥⊥\t\\angle A\t∠A” role=”presentation”&gt;∠A∠A\\sin\tsin” role=”presentation”&gt;sinsin\t\\cos\tcos” role=”presentation”&gt;coscos\t\\tan\ttan” role=”presentation”&gt;tantan\\csc\tcsc” r箭头 短箭头形状 MarkDown $\\uparrow$ $\\uparrow$ $\\Uparrow$ $\\Uparrow$ $\\downarrow$ $\\downarrow$ $\\Downarrow$ $\\Downarrow$ $\\leftarrow$ $\\leftarrow$ $\\Leftarrow$ $\\Leftarrow$ $\\rightarrow$ $\\rightarrow$ $\\Rightarrow$ $\\Rightarrow$ $\\updownarrow$ $\\updownarrow$ $\\Updownarrow$ $\\Updownarrow$ $\\leftrightarrow$ $\\leftrightarrow$ $\\Leftrightarrow$ $\\Leftrightarrow$ 长箭头形状 MarkDown $\\longleftarrow$ $\\longleftarrow$ $\\Longleftarrow$ $\\Longleftarrow$ $\\longrightarrow$ $\\longrightarrow$ $\\Longrightarrow$ $\\Longrightarrow$ $\\longleftrightarrow$ $\\longleftrightarrow$ $\\Longleftrightarrow$ $\\Longleftrightarrow$ 给字体加颜色&lt;font color=red&gt;想变成红色的内容&lt;/font&gt;想变成红色的内容加边框&lt;kbd&gt;2 3&lt;/kbd&gt;2 3创建脚注格式类似这样 2。查看代码 &nbsp;这里写需要被折叠的代码&nbsp; This is the footnote. &#8617; 菜鸟教程 – 学的不仅是技术，更是梦想！！！ &#8617; " } ]
